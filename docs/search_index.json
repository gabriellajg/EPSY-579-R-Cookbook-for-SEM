[["index.html", "EPSY:579 R Cookbook for SEM Chapter 1 Course", " EPSY:579 R Cookbook for SEM Gabriella Jiang QUERIES, University of Illinois at Urbana-Champaign Chapter 1 Course Structural Equation Modeling (SEM) is a general class of multivariate techniques that models relationships between latent variables and observed variables (“measurement models”) and relationships among latent variables (“structural models”) simultaneously. Students will learn the theoretical background of SEM as well as the techniques using programming language R. Topics covered in this class include mediation/moderation model; confirmatory factor analysis; model fit evaluation; multi-group SEM; latent growth modeling; MTMM model; and SEM with categorical variables. 4 graduate hours. No professional credit. Prerequisite: EPSY 580 and EPSY 581; or Equivalents. This site is supposed to serve as a repository for R codes used in lab sessions of this course in Spring 2022. *Disclaimer: Opinions are my own and not the views of my employer "],["r-exercises.html", "Chapter 2 R Exercises", " Chapter 2 R Exercises "],["week4_1-lavaan-lab-1-path-analysis-model.html", "Chapter 3 Week4_1: Lavaan Lab 1 Path Analysis Model 3.1 Reading-In and Working With Realistic Datasets In R 3.2 Sample Covariance Matrices using the cov() function 3.3 Installing Packages 3.4 Loading Packages (Libraries) That You Have Installed 3.5 Using Lavaan For Path Models 3.6 Plotting SEM model 3.7 Exercise: How would you fit the model in Saunders et al. (2016)?", " Chapter 3 Week4_1: Lavaan Lab 1 Path Analysis Model In this lab, we will learn how to: install a package called lavaan in R perform path analysis using the lavaan package 3.1 Reading-In and Working With Realistic Datasets In R 3.1.0.1 To begin, we will read the file that we will use for our SEM lab (eatingDisorderSimData.csv). Try running this function, as written: file.choose() Using the GUI (graphical user interface) window that pops up, select the file eatingDisorderSimData.csv This should produce a file path like this (note: below is a Mac version): /Your/File/Path/eatingDisorderSimData.csv You can copy this path into the read.csv and put it in the file = argument of the function: read.csv() is a function for reading in .csv files. Assign the name labData to the dataset in R using &lt;- labData &lt;- read.csv(file = &quot;/Users/gejiang/Box Sync/MacSync/Teaching/590SEM/Spring 2022/Week 4/R/eatingDisorderSimData.csv&quot;, header = TRUE) Important Argument: header = if header = TRUE, indicates that your dataset has column names that are to be read in separate from the data. if header = FALSE, indicates that your dataset does NOT have column names, and therefore the first row of the dataset should be read as data. 3.1.0.2 Or you could NEST the file.choose() function inside the read.csv function labData &lt;- read.csv(file = file.choose(), header = T) Because file.choose() returns the file path, putting this inside the read.csv function is the same as writing the path inside the function! 3.1.0.3 Pros and Cons of writing the full file path vs. using read.csv(file = file.choose(), header = T) If you write down the full file path and put it in the function, then the next time you run this R script you can easily read in your data without searching through your directories and folders. However, if you move your file to a different folder in the future, you’ll need to change the directory path in your R script. file.choose() is very easy and user-friendly. Using this method allows you to find your datafile even if you’ve moved it to a different folder. However, it is slightly more effortful to go in and select your folder each time. 3.1.0.4 Gabriella recommends: Set your working directory to the directory that contains the dataset, and simply load your data by typing the name of the .csv file: setwd(&quot;~/Box Sync/MacSync/Teaching/590SEM/Spring 2022/Week 4/R&quot;) labData &lt;- read.csv(file = &quot;eatingDisorderSimData.csv&quot;, header = T, sep = &quot;,&quot;) This serves to save all your future analyses in your working directory. read.csv() is related to a broader function called read.table. The read.table function has a sep = argument sep = If sep = “,” this indicates a comma-separated (.csv) file If sep = ” ” this indicates a tab-delimited (“white space” delimited) file, such as a .txt 3.1.0.5 Finally, point and click always works… library(readr) eatingDisorderSimData &lt;- read_csv(&quot;eatingDisorderSimData.csv&quot;) View(eatingDisorderSimData) 3.2 Sample Covariance Matrices using the cov() function 3.2.0.1 Quick review: str(labData) #structure ## &#39;data.frame&#39;: 1339 obs. of 7 variables: ## $ BMI : num 0.377 0.302 -1.098 -1.13 -2.797 ... ## $ SelfEsteem : num 0.0685 -0.3059 1.4755 -0.1329 1.3538 ... ## $ Accu : num 1.782 0.491 -0.682 2.224 0.892 ... ## $ DietSE : num -0.0544 -2.3957 0.168 1.1851 0.5131 ... ## $ Restrictive: num -0.525 2.067 0.364 -1.656 0.743 ... ## $ Bulimia : num 0.432 0.196 -1.434 -0.675 -0.858 ... ## $ Risk : num 0.508 0.91 -0.777 -0.554 -0.314 ... head(labData) #first few lines ## BMI SelfEsteem Accu DietSE Restrictive Bulimia Risk ## 1 0.3769721 0.0685226 1.7822103 -0.05436952 -0.5251424 0.4322272 0.50794715 ## 2 0.3015484 -0.3058876 0.4909857 -2.39569010 2.0671867 0.1959765 0.90996098 ## 3 -1.0980232 1.4754543 -0.6819827 0.16801384 0.3638750 -1.4337656 -0.77678045 ## 4 -1.1304059 -0.1329290 2.2235223 1.18505959 -1.6557519 -0.6748446 -0.55411733 ## 5 -2.7965343 1.3537804 0.8922687 0.51311551 0.7431860 -0.8575733 -0.31385631 ## 6 0.7205735 -1.9361462 -1.0307704 0.79749119 -1.8609143 0.3290163 0.08012833 colnames(labData) #column names ## [1] &quot;BMI&quot; &quot;SelfEsteem&quot; &quot;Accu&quot; &quot;DietSE&quot; &quot;Restrictive&quot; &quot;Bulimia&quot; &quot;Risk&quot; How many observations are in this dataset? Number of observations = number of rows, with 1 person per row nrow(labData) #1339 ## [1] 1339 let’s save this number as n n &lt;- nrow(labData) Let’s look at the sample covariance matrix of these variables using the cov() function: cov(labData) ## BMI SelfEsteem Accu DietSE Restrictive Bulimia Risk ## BMI 1.07399214 -0.1380786 -0.02076620 -0.10688665 -0.13056197 0.16177119 0.07938074 ## SelfEsteem -0.13807862 1.0021547 0.03501750 0.10557111 -0.11991676 -0.31717764 -0.22864713 ## Accu -0.02076620 0.0350175 0.97176431 -0.02069863 -0.09050653 -0.09549788 -0.10327073 ## DietSE -0.10688665 0.1055711 -0.02069863 0.96607293 -0.15678475 -0.21922044 0.07119772 ## Restrictive -0.13056197 -0.1199168 -0.09050653 -0.15678475 1.01695732 0.58684522 0.78960193 ## Bulimia 0.16177119 -0.3171776 -0.09549788 -0.21922044 0.58684522 1.03637890 0.87337921 ## Risk 0.07938074 -0.2286471 -0.10327073 0.07119772 0.78960193 0.87337921 1.05356717 let’s save this sample cov as capital S: S = cov(labData) If we wanted, we could look at a subset of the dataset, e.g.,: cov(labData[,c(&quot;BMI&quot;, &quot;SelfEsteem&quot;, &quot;Accu&quot;)]) ## BMI SelfEsteem Accu ## BMI 1.0739921 -0.1380786 -0.0207662 ## SelfEsteem -0.1380786 1.0021547 0.0350175 ## Accu -0.0207662 0.0350175 0.9717643 This is often useful if our analysis will only contain certain variables. If only two variables: cov(labData$BMI, labData$SelfEsteem) ## [1] -0.1380786 If only one variable (variance): cov(labData$BMI, labData$BMI) ## [1] 1.073992 3.3 Installing Packages We will mostly be using the lavaan package to perform SEM analyses, so let’s use the install.packages() function to install it first install.packages(&quot;lavaan&quot;) lavaan stands for LAtent VAriable ANalysis using R. lavaan website: http://lavaan.ugent.be Check out the tutorials and examples! 3.4 Loading Packages (Libraries) That You Have Installed AFTER YOU’VE INSTALLED A PACKAGE ONE TIME, YOU DON’T HAVE TO EVER INSTALL IT AGAIN, UNLESS YOU DELETE AND REINSTALL R FOR SOME REASON. HOWEVER, NOW THAT THESE FUNCTIONS ARE INSTALLED IN R ON YOUR MACHINE, YOU MUST LOAD THE LIBRARY EVERY TIME YOU OPEN R AND WISH TO USE IT. To do this, use the library() function: library(lavaan) This is lavaan 0.6-9 lavaan is FREE software! Please report any bugs. Don’t worry about the “BETA” warning, this package is awesome! This may seem like a pain, but roll with it. The good news is that once you do it, you have access to a whole library of SEM functions. If you boot up R and receive error msgs like “could not find function”sem”” IT IS PROBABLY BECAUSE YOU HAVEN’T LOADED THE lavaan PACKAGE. Check out the help page of a particular function, say sem(): help(sem) ## Help on topic &#39;sem&#39; was found in the following packages: ## ## Package Library ## lavaan /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library ## sem /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library ## ## ## Using the first match ... ?sem ## Help on topic &#39;sem&#39; was found in the following packages: ## ## Package Library ## lavaan /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library ## sem /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/library ## ## ## Using the first match ... 3.5 Using Lavaan For Path Models Every analysis in lavaan has three main parts. Part I: Writing the Model Syntax Part II: Analyzing the Model Using Your Dataset Part III: Examining the results. 3.5.1 PART I: Follow the set of equations we wrote in class: Self-Efficacy = BMI + Self-Esteem + Disturbance Bulimic Symptoms = BMI + Self-Esteem + Self-Efficacy + Disturbance Restrictive Symptoms = BMI + Self-Esteem + Self-Efficacy + Disturbance Overall Risk = BMI + Self-Esteem + Self-Efficacy + Acculturation + Disturbance Let’s write some model syntax: ex1PathSyntax &lt;- &quot; #opening a quote # Tilda ~ : Regression # M ~ X regression (X predicts M) # Each line corresponds to an equation # Disturbance is automatically included for each regression # (i.e. no extra term needed) DietSE ~ BMI + SelfEsteem #DietSE is predicted by BMI and SelfEsteem Bulimia ~ DietSE + BMI + SelfEsteem Restrictive ~ DietSE + BMI + SelfEsteem Risk ~ DietSE + BMI + SelfEsteem + Accu &quot; Things to note here: We are calling our saved model syntax object ex1PathSyntax We assign it using &lt;- as usual Then we open a quotation ” Then we write each part of the model on separate lines. Then we close the quotation ” The variables names need to match those in the dataset (case matters!) Add comments inside the model syntax using hashtag 3.5.2 PART II Let’s run our model! To run this model, we will start by using the sem() function. Sensible defaults for estimating CFA models like assumptions of linear regression, so we don’t actually have to write some constraints into the model above Alternatively, one can use lavaan() function [with the fewest default settings] or cfa() function [with similar defaults as sem() function] To use lavaan(), you have to specify all 22 parameters in the model. 3.5.2.1 ex1fit You can run the sem() function using two different sources of data: The raw dataset, using: sem(model = modelSyntax, data = yourDataset) example: ex1fit &lt;- sem(model = ex1PathSyntax, data = labData) If you encounter errors like: Error in if ((!is.matrix(model)) | ncol(model) != 3) stop(“model argument must be a 3-column matrix”) : argument is of length zero IT IS PROBABLY BECAUSE YOU HAVEN’T LOADED THE lavaan PACKAGE. To make sure you are using the sem() function from the lavaan package, add PackageName:: before a function: ex1fit &lt;- lavaan::sem(model = ex1PathSyntax, data = labData) Then we can obtain complete results using the summary() function: summary(ex1fit) ## lavaan 0.6-10 ended normally after 36 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 19 ## ## Number of observations 1339 ## ## Model Test User Model: ## ## Test statistic 16.429 ## Degrees of freedom 3 ## P-value (Chi-square) 0.001 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## DietSE ~ ## BMI -0.088 0.026 -3.381 0.001 ## SelfEsteem 0.093 0.027 3.480 0.001 ## Bulimia ~ ## DietSE -0.185 0.026 -6.994 0.000 ## BMI 0.096 0.025 3.796 0.000 ## SelfEsteem -0.284 0.026 -10.871 0.000 ## Restrictive ~ ## DietSE -0.166 0.027 -6.039 0.000 ## BMI -0.154 0.026 -5.892 0.000 ## SelfEsteem -0.123 0.027 -4.561 0.000 ## Risk ~ ## DietSE 0.105 0.028 3.766 0.000 ## BMI 0.055 0.027 2.057 0.040 ## SelfEsteem -0.232 0.028 -8.425 0.000 ## Accu 0.007 0.009 0.783 0.434 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .Bulimia ~~ ## .Restrictive 0.536 0.029 18.389 0.000 ## .Risk 0.814 0.034 23.983 0.000 ## .Restrictive ~~ ## .Risk 0.785 0.034 22.996 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .DietSE 0.946 0.037 25.875 0.000 ## .Bulimia 0.890 0.034 25.875 0.000 ## .Restrictive 0.955 0.037 25.875 0.000 ## .Risk 0.989 0.038 25.875 0.000 The covariance matrix, using: sem(model = modelSyntax, sample.cov = yourCovarianceMatrix, sample.nobs = numberOfObservationsInYourDataset) This is to illustrate that WITH COMPLETE DATA, you can run SEM analyses using only covariances as input and obtain the same results as with raw data! This positions SEM for meta-analysis and replication studies. example: ex1fit_S &lt;- lavaan::sem(model = ex1PathSyntax, sample.cov = S, sample.nobs = n) summary(ex1fit_S) ## lavaan 0.6-10 ended normally after 36 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 19 ## ## Number of observations 1339 ## ## Model Test User Model: ## ## Test statistic 16.429 ## Degrees of freedom 3 ## P-value (Chi-square) 0.001 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## DietSE ~ ## BMI -0.088 0.026 -3.381 0.001 ## SelfEsteem 0.093 0.027 3.480 0.001 ## Bulimia ~ ## DietSE -0.185 0.026 -6.994 0.000 ## BMI 0.096 0.025 3.796 0.000 ## SelfEsteem -0.284 0.026 -10.871 0.000 ## Restrictive ~ ## DietSE -0.166 0.027 -6.039 0.000 ## BMI -0.154 0.026 -5.892 0.000 ## SelfEsteem -0.123 0.027 -4.561 0.000 ## Risk ~ ## DietSE 0.105 0.028 3.766 0.000 ## BMI 0.055 0.027 2.057 0.040 ## SelfEsteem -0.232 0.028 -8.425 0.000 ## Accu 0.007 0.009 0.783 0.434 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .Bulimia ~~ ## .Restrictive 0.536 0.029 18.389 0.000 ## .Risk 0.814 0.034 23.983 0.000 ## .Restrictive ~~ ## .Risk 0.785 0.034 22.996 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .DietSE 0.946 0.037 25.875 0.000 ## .Bulimia 0.890 0.034 25.875 0.000 ## .Restrictive 0.955 0.037 25.875 0.000 ## .Risk 0.989 0.038 25.875 0.000 The . before a variable name refers to its disturbance. e.g., .Bulimia refers to the disturbance of Bulimia, not Bulimia itself You should get exactly the same output in ex1fit and ex1fit_S. Wait, Gabriella, the df is not 6…. This is because sem() by default assumes that disturbances of endogenous variables covary among themselves (which, in our model, are not correlated at all!) The estimates of disturbance covariances are presented under “Covariances” in the output: Covariances: Estimate Std.Err z-value P(&gt;|z|) .Bulimia ~~ .Restrictive 0.536 0.029 18.389 0.000 .Risk 0.814 0.034 23.983 0.000 .Restrictive ~~ .Risk 0.785 0.034 22.996 0.000 3.5.2.2 ex1PathSyntax_noCov To change those defaults, one needs to explicitly fix those disturbance covariances at 0 (this is a strong assumption, I know…): http://lavaan.ugent.be/tutorial/syntax2.html ex1PathSyntax_noCov &lt;- &quot; #opening a quote # ~~ indicates a two-headed arrow (variance or covariance) # 0* in front of the 2nd variable fixes the covariance at 0 DietSE ~ BMI + SelfEsteem #DietSE is predicted by BMI and SelfEsteem Bulimia ~ DietSE + BMI + SelfEsteem Restrictive ~ DietSE + BMI + SelfEsteem Risk ~ DietSE + BMI + SelfEsteem + Accu #Disturbance covariances (fixed at 0): DietSE ~~ 0*Bulimia DietSE ~~ 0*Restrictive DietSE ~~ 0*Risk Bulimia ~~ 0*Restrictive Bulimia ~~ 0*Risk Restrictive ~~ 0*Risk # These lines above say that there is no covariance among the disturbances of all endogenous variables &quot; ex1fit_noCov &lt;- lavaan::sem(model = ex1PathSyntax_noCov, data = labData) summary(ex1fit_noCov) ## lavaan 0.6-10 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 16 ## ## Number of observations 1339 ## ## Model Test User Model: ## ## Test statistic 3536.813 ## Degrees of freedom 6 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## DietSE ~ ## BMI -0.088 0.026 -3.381 0.001 ## SelfEsteem 0.093 0.027 3.480 0.001 ## Bulimia ~ ## DietSE -0.185 0.026 -6.994 0.000 ## BMI 0.096 0.025 3.796 0.000 ## SelfEsteem -0.284 0.026 -10.871 0.000 ## Restrictive ~ ## DietSE -0.166 0.027 -6.039 0.000 ## BMI -0.154 0.026 -5.892 0.000 ## SelfEsteem -0.123 0.027 -4.561 0.000 ## Risk ~ ## DietSE 0.102 0.028 3.686 0.000 ## BMI 0.053 0.026 2.000 0.045 ## SelfEsteem -0.228 0.027 -8.332 0.000 ## Accu -0.095 0.027 -3.449 0.001 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .DietSE ~~ ## .Bulimia 0.000 ## .Restrictive 0.000 ## .Risk 0.000 ## .Bulimia ~~ ## .Restrictive 0.000 ## .Risk 0.000 ## .Restrictive ~~ ## .Risk 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .DietSE 0.946 0.037 25.875 0.000 ## .Bulimia 0.890 0.034 25.875 0.000 ## .Restrictive 0.955 0.037 25.875 0.000 ## .Risk 0.979 0.038 25.875 0.000 df = 6 and Covariances: Estimate Std.Err z-value P(&gt;|z|) .DietSE ~~ .Bulimia 0.000 .Restrictive 0.000 .Risk 0.000 .Bulimia ~~ .Restrictive 0.000 .Risk 0.000 .Restrictive ~~ .Risk 0.000 Wait, where are the variances and covariances of exogenous variables? They are not included in the output because they are estimated PERFECTLY 3.5.2.3 ex1fit_noCov_freeX fixed.x=FALSE asks for the variances/covariances/means of the exogenous variables to be freely estimated instead of being fixed at the values found from the sample This usually makes no difference from ex1fit_noCov, except that it prints more lines ex1fit_noCov_freeX &lt;- lavaan::sem(model = ex1PathSyntax_noCov, data = labData, fixed.x = FALSE) summary(ex1fit_noCov_freeX) ## lavaan 0.6-10 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 22 ## ## Number of observations 1339 ## ## Model Test User Model: ## ## Test statistic 3536.813 ## Degrees of freedom 6 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## DietSE ~ ## BMI -0.088 0.026 -3.381 0.001 ## SelfEsteem 0.093 0.027 3.480 0.001 ## Bulimia ~ ## DietSE -0.185 0.026 -6.994 0.000 ## BMI 0.096 0.025 3.796 0.000 ## SelfEsteem -0.284 0.026 -10.871 0.000 ## Restrictive ~ ## DietSE -0.166 0.027 -6.039 0.000 ## BMI -0.154 0.026 -5.892 0.000 ## SelfEsteem -0.123 0.027 -4.561 0.000 ## Risk ~ ## DietSE 0.102 0.028 3.686 0.000 ## BMI 0.053 0.026 2.000 0.045 ## SelfEsteem -0.228 0.027 -8.332 0.000 ## Accu -0.095 0.027 -3.449 0.001 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .DietSE ~~ ## .Bulimia 0.000 ## .Restrictive 0.000 ## .Risk 0.000 ## .Bulimia ~~ ## .Restrictive 0.000 ## .Risk 0.000 ## .Restrictive ~~ ## .Risk 0.000 ## BMI ~~ ## SelfEsteem -0.138 0.029 -4.828 0.000 ## Accu -0.021 0.028 -0.744 0.457 ## SelfEsteem ~~ ## Accu 0.035 0.027 1.298 0.194 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .DietSE 0.946 0.037 25.875 0.000 ## .Bulimia 0.890 0.034 25.875 0.000 ## .Restrictive 0.955 0.037 25.875 0.000 ## .Risk 0.979 0.038 25.875 0.000 ## BMI 1.073 0.041 25.875 0.000 ## SelfEsteem 1.001 0.039 25.875 0.000 ## Accu 0.971 0.038 25.875 0.000 3.5.2.4 ex1fit_noCov_lavaan As a bonus, here is how you would write the model syntax if you use lavaan() instead of sem()… ex1PathSyntax_lavaan &lt;- &quot; #opening a quote # ~~ indicates a two-headed arrow (variance or covariance) #regression coefficients (12) DietSE ~ BMI + SelfEsteem Bulimia ~ DietSE + BMI + SelfEsteem Restrictive ~ DietSE + BMI + SelfEsteem Risk ~ DietSE + BMI + SelfEsteem + Accu #variances of exogenous variables (3) BMI ~~ BMI SelfEsteem ~~ SelfEsteem Accu ~~ Accu #disturbance variances (4) DietSE ~~ DietSE Bulimia ~~ Bulimia Restrictive ~~ Restrictive Risk ~~ Risk #covariances among exogenous variables (3) BMI ~~ SelfEsteem BMI ~~ Accu SelfEsteem ~~ Accu #total: 22 parameters &quot; ex1fit_noCov_lavaan &lt;- lavaan(model = ex1PathSyntax_lavaan, data = labData) summary(ex1fit_noCov_lavaan) ## lavaan 0.6-10 ended normally after 11 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 22 ## ## Number of observations 1339 ## ## Model Test User Model: ## ## Test statistic 3536.813 ## Degrees of freedom 6 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## DietSE ~ ## BMI -0.088 0.026 -3.381 0.001 ## SelfEsteem 0.093 0.027 3.480 0.001 ## Bulimia ~ ## DietSE -0.185 0.026 -6.994 0.000 ## BMI 0.096 0.025 3.796 0.000 ## SelfEsteem -0.284 0.026 -10.871 0.000 ## Restrictive ~ ## DietSE -0.166 0.027 -6.039 0.000 ## BMI -0.154 0.026 -5.892 0.000 ## SelfEsteem -0.123 0.027 -4.561 0.000 ## Risk ~ ## DietSE 0.102 0.028 3.686 0.000 ## BMI 0.053 0.026 2.000 0.045 ## SelfEsteem -0.228 0.027 -8.332 0.000 ## Accu -0.095 0.027 -3.449 0.001 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## BMI ~~ ## SelfEsteem -0.138 0.029 -4.828 0.000 ## Accu -0.021 0.028 -0.744 0.457 ## SelfEsteem ~~ ## Accu 0.035 0.027 1.298 0.194 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## BMI 1.073 0.041 25.875 0.000 ## SelfEsteem 1.001 0.039 25.875 0.000 ## Accu 0.971 0.038 25.875 0.000 ## .DietSE 0.946 0.037 25.875 0.000 ## .Bulimia 0.890 0.034 25.875 0.000 ## .Restrictive 0.955 0.037 25.875 0.000 ## .Risk 0.979 0.038 25.875 0.000 which yields the same output as ex1fit_noCov_freeX. 3.5.3 Sigma Matrices Let’s have a look at the model-implied covarinace matrix from our final model ex1fit_noCov_freeX and save it as Sigma: fitted(ex1fit_noCov_freeX) ## $cov ## DietSE Bulimi Rstrct Risk BMI SlfEst Accu ## DietSE 0.965 ## Bulimia -0.219 1.036 ## Restrictive -0.157 0.051 1.016 ## Risk 0.069 0.060 0.005 1.052 ## BMI -0.107 0.162 -0.130 0.079 1.073 ## SelfEsteem 0.105 -0.317 -0.120 -0.228 -0.138 1.001 ## Accu 0.005 -0.013 -0.002 -0.101 -0.021 0.035 0.971 Sigma &lt;- fitted(ex1fit_noCov_freeX)$cov How close is Sigma to S? Rearrange the rows and columns of Sigma (important!) and take the difference diff = Sigma[colnames(S), colnames(S)] - S round(diff, 3) ## BMI SelfEsteem Accu DietSE Restrictive Bulimia Risk ## BMI -0.001 0.000 0.000 0.000 0.000 0.000 0.000 ## SelfEsteem 0.000 -0.001 0.000 0.000 0.000 0.000 0.000 ## Accu 0.000 0.000 -0.001 0.026 0.089 0.083 0.003 ## DietSE 0.000 0.000 0.026 -0.001 0.000 0.000 -0.002 ## Restrictive 0.000 0.000 0.089 0.000 -0.001 -0.536 -0.785 ## Bulimia 0.000 0.000 0.083 0.000 -0.536 -0.001 -0.814 ## Risk 0.000 0.000 0.003 -0.002 -0.785 -0.814 -0.001 How about the default model that include disturbance covariances? Sigma0 &lt;- fitted(ex1fit)$cov diff0 = Sigma0[colnames(S), colnames(S)] - S round(diff0, 3) ## BMI SelfEsteem Accu DietSE Restrictive Bulimia Risk ## BMI -0.001 0.000 0.000 0.000 0.000 0.000 0.000 ## SelfEsteem 0.000 -0.001 0.000 0.000 0.000 0.000 0.000 ## Accu 0.000 0.000 -0.001 0.026 0.089 0.083 0.101 ## DietSE 0.000 0.000 0.026 -0.001 0.000 0.000 0.000 ## Restrictive 0.000 0.000 0.089 0.000 -0.001 0.000 0.000 ## Bulimia 0.000 0.000 0.083 0.000 0.000 -0.001 0.000 ## Risk 0.000 0.000 0.101 0.000 0.000 0.000 0.001 3.5.3.1 Gabriella’s Practical Tips: To begin with, constraint the disturbance covariances to be 0 ; Keep the model if the model fits the data well; Relax the constraints the disturbance covariances if the initial model did not fit well. 3.5.4 PART III: Summarizing Our Analysis: There are some useful options we can ask for with summary(): summary(ex1fit_noCov_freeX, fit.measures = T) #include model fit measures summary(ex1fit_noCov_freeX, standardized = T) #This includes standardized estimates. std.all contains usual regression standardization. summary(ex1fit_noCov_freeX, ci = T) #Include confidence intervals # Add them all! If we JUST want the parameter estimates: parameterEstimates(ex1fit_noCov_freeX) ## lhs op rhs est se z pvalue ci.lower ci.upper ## 1 DietSE ~ BMI -0.088 0.026 -3.381 0.001 -0.138 -0.037 ## 2 DietSE ~ SelfEsteem 0.093 0.027 3.480 0.001 0.041 0.146 ## 3 Bulimia ~ DietSE -0.185 0.026 -6.994 0.000 -0.237 -0.133 ## 4 Bulimia ~ BMI 0.096 0.025 3.796 0.000 0.046 0.145 ## 5 Bulimia ~ SelfEsteem -0.284 0.026 -10.871 0.000 -0.335 -0.233 ## 6 Restrictive ~ DietSE -0.166 0.027 -6.039 0.000 -0.220 -0.112 ## 7 Restrictive ~ BMI -0.154 0.026 -5.892 0.000 -0.205 -0.103 ## 8 Restrictive ~ SelfEsteem -0.123 0.027 -4.561 0.000 -0.176 -0.070 ## 9 Risk ~ DietSE 0.102 0.028 3.686 0.000 0.048 0.157 ## 10 Risk ~ BMI 0.053 0.026 2.000 0.045 0.001 0.105 ## 11 Risk ~ SelfEsteem -0.228 0.027 -8.332 0.000 -0.282 -0.175 ## 12 Risk ~ Accu -0.095 0.027 -3.449 0.001 -0.149 -0.041 ## 13 DietSE ~~ Bulimia 0.000 0.000 NA NA 0.000 0.000 ## 14 DietSE ~~ Restrictive 0.000 0.000 NA NA 0.000 0.000 ## 15 DietSE ~~ Risk 0.000 0.000 NA NA 0.000 0.000 ## 16 Bulimia ~~ Restrictive 0.000 0.000 NA NA 0.000 0.000 ## 17 Bulimia ~~ Risk 0.000 0.000 NA NA 0.000 0.000 ## 18 Restrictive ~~ Risk 0.000 0.000 NA NA 0.000 0.000 ## 19 DietSE ~~ DietSE 0.946 0.037 25.875 0.000 0.874 1.018 ## 20 Bulimia ~~ Bulimia 0.890 0.034 25.875 0.000 0.822 0.957 ## 21 Restrictive ~~ Restrictive 0.955 0.037 25.875 0.000 0.883 1.028 ## 22 Risk ~~ Risk 0.979 0.038 25.875 0.000 0.905 1.054 ## 23 BMI ~~ BMI 1.073 0.041 25.875 0.000 0.992 1.154 ## 24 BMI ~~ SelfEsteem -0.138 0.029 -4.828 0.000 -0.194 -0.082 ## 25 BMI ~~ Accu -0.021 0.028 -0.744 0.457 -0.075 0.034 ## 26 SelfEsteem ~~ SelfEsteem 1.001 0.039 25.875 0.000 0.926 1.077 ## 27 SelfEsteem ~~ Accu 0.035 0.027 1.298 0.194 -0.018 0.088 ## 28 Accu ~~ Accu 0.971 0.038 25.875 0.000 0.897 1.045 parameterEstimates(ex1fit_noCov_freeX, standardized = T) #include standardized solution.... ## lhs op rhs est se z pvalue ci.lower ci.upper std.lv std.all std.nox ## 1 DietSE ~ BMI -0.088 0.026 -3.381 0.001 -0.138 -0.037 -0.088 -0.092 -0.089 ## 2 DietSE ~ SelfEsteem 0.093 0.027 3.480 0.001 0.041 0.146 0.093 0.095 0.095 ## 3 Bulimia ~ DietSE -0.185 0.026 -6.994 0.000 -0.237 -0.133 -0.185 -0.179 -0.179 ## 4 Bulimia ~ BMI 0.096 0.025 3.796 0.000 0.046 0.145 0.096 0.097 0.094 ## 5 Bulimia ~ SelfEsteem -0.284 0.026 -10.871 0.000 -0.335 -0.233 -0.284 -0.279 -0.279 ## 6 Restrictive ~ DietSE -0.166 0.027 -6.039 0.000 -0.220 -0.112 -0.166 -0.162 -0.162 ## 7 Restrictive ~ BMI -0.154 0.026 -5.892 0.000 -0.205 -0.103 -0.154 -0.158 -0.153 ## 8 Restrictive ~ SelfEsteem -0.123 0.027 -4.561 0.000 -0.176 -0.070 -0.123 -0.122 -0.122 ## 9 Risk ~ DietSE 0.102 0.028 3.686 0.000 0.048 0.157 0.102 0.098 0.098 ## 10 Risk ~ BMI 0.053 0.026 2.000 0.045 0.001 0.105 0.053 0.053 0.052 ## 11 Risk ~ SelfEsteem -0.228 0.027 -8.332 0.000 -0.282 -0.175 -0.228 -0.223 -0.223 ## 12 Risk ~ Accu -0.095 0.027 -3.449 0.001 -0.149 -0.041 -0.095 -0.091 -0.092 ## 13 DietSE ~~ Bulimia 0.000 0.000 NA NA 0.000 0.000 0.000 0.000 0.000 ## 14 DietSE ~~ Restrictive 0.000 0.000 NA NA 0.000 0.000 0.000 0.000 0.000 ## 15 DietSE ~~ Risk 0.000 0.000 NA NA 0.000 0.000 0.000 0.000 0.000 ## 16 Bulimia ~~ Restrictive 0.000 0.000 NA NA 0.000 0.000 0.000 0.000 0.000 ## 17 Bulimia ~~ Risk 0.000 0.000 NA NA 0.000 0.000 0.000 0.000 0.000 ## 18 Restrictive ~~ Risk 0.000 0.000 NA NA 0.000 0.000 0.000 0.000 0.000 ## 19 DietSE ~~ DietSE 0.946 0.037 25.875 0.000 0.874 1.018 0.946 0.980 0.980 ## 20 Bulimia ~~ Bulimia 0.890 0.034 25.875 0.000 0.822 0.957 0.890 0.859 0.859 ## 21 Restrictive ~~ Restrictive 0.955 0.037 25.875 0.000 0.883 1.028 0.955 0.940 0.940 ## 22 Risk ~~ Risk 0.979 0.038 25.875 0.000 0.905 1.054 0.979 0.931 0.931 ## 23 BMI ~~ BMI 1.073 0.041 25.875 0.000 0.992 1.154 1.073 1.000 1.073 ## 24 BMI ~~ SelfEsteem -0.138 0.029 -4.828 0.000 -0.194 -0.082 -0.138 -0.133 -0.138 ## 25 BMI ~~ Accu -0.021 0.028 -0.744 0.457 -0.075 0.034 -0.021 -0.020 -0.021 ## 26 SelfEsteem ~~ SelfEsteem 1.001 0.039 25.875 0.000 0.926 1.077 1.001 1.000 1.001 ## 27 SelfEsteem ~~ Accu 0.035 0.027 1.298 0.194 -0.018 0.088 0.035 0.035 0.035 ## 28 Accu ~~ Accu 0.971 0.038 25.875 0.000 0.897 1.045 0.971 1.000 0.971 For standardized solutions, there is also this function: standardizedSolution(ex1fit_noCov_freeX, type = &quot;std.all&quot;) ## lhs op rhs est.std se z pvalue ci.lower ci.upper ## 1 DietSE ~ BMI -0.092 0.027 -3.395 0.001 -0.146 -0.039 ## 2 DietSE ~ SelfEsteem 0.095 0.027 3.496 0.000 0.042 0.148 ## 3 Bulimia ~ DietSE -0.179 0.025 -7.093 0.000 -0.228 -0.129 ## 4 Bulimia ~ BMI 0.097 0.026 3.811 0.000 0.047 0.148 ## 5 Bulimia ~ SelfEsteem -0.279 0.025 -11.284 0.000 -0.328 -0.231 ## 6 Restrictive ~ DietSE -0.162 0.026 -6.115 0.000 -0.213 -0.110 ## 7 Restrictive ~ BMI -0.158 0.027 -5.962 0.000 -0.210 -0.106 ## 8 Restrictive ~ SelfEsteem -0.122 0.027 -4.593 0.000 -0.175 -0.070 ## 9 Risk ~ DietSE 0.098 0.027 3.702 0.000 0.046 0.150 ## 10 Risk ~ BMI 0.053 0.027 2.003 0.045 0.001 0.106 ## 11 Risk ~ SelfEsteem -0.223 0.026 -8.536 0.000 -0.274 -0.172 ## 12 Risk ~ Accu -0.091 0.026 -3.462 0.001 -0.143 -0.039 ## 13 DietSE ~~ Bulimia 0.000 0.000 NA NA 0.000 0.000 ## 14 DietSE ~~ Restrictive 0.000 0.000 NA NA 0.000 0.000 ## 15 DietSE ~~ Risk 0.000 0.000 NA NA 0.000 0.000 ## 16 Bulimia ~~ Restrictive 0.000 0.000 NA NA 0.000 0.000 ## 17 Bulimia ~~ Risk 0.000 0.000 NA NA 0.000 0.000 ## 18 Restrictive ~~ Risk 0.000 0.000 NA NA 0.000 0.000 ## 19 DietSE ~~ DietSE 0.980 0.008 129.769 0.000 0.965 0.995 ## 20 Bulimia ~~ Bulimia 0.859 0.018 48.727 0.000 0.824 0.894 ## 21 Restrictive ~~ Restrictive 0.940 0.013 74.768 0.000 0.915 0.965 ## 22 Risk ~~ Risk 0.931 0.013 69.534 0.000 0.904 0.957 ## 23 BMI ~~ BMI 1.000 0.000 NA NA 1.000 1.000 ## 24 BMI ~~ SelfEsteem -0.133 0.027 -4.958 0.000 -0.186 -0.080 ## 25 BMI ~~ Accu -0.020 0.027 -0.744 0.457 -0.074 0.033 ## 26 SelfEsteem ~~ SelfEsteem 1.000 0.000 NA NA 1.000 1.000 ## 27 SelfEsteem ~~ Accu 0.035 0.027 1.300 0.194 -0.018 0.089 ## 28 Accu ~~ Accu 1.000 0.000 NA NA 1.000 1.000 How does it work? ?standardizedSolution 3.6 Plotting SEM model # install.packages(&quot;semPlot&quot;) library(semPlot) # Plot! semPaths(ex1fit_noCov_freeX) # estimates instead of paths only semPaths(ex1fit_noCov_freeX, what=&#39;est&#39;, edge.label.cex=1.25, curvePivot = TRUE, fade=FALSE) # standardized solutions semPaths(ex1fit_noCov_freeX, what=&#39;std&#39;, edge.label.cex=1.25, curvePivot = TRUE, fade=FALSE) semPaths(ex1fit_noCov_freeX, what=&#39;est&#39;, rotation = 2, # default rotation = 1 with four options edge.label.cex=1.25, curvePivot = TRUE, fade=FALSE) 3.6.1 customize it your way semPaths(ex1fit_noCov_freeX, whatLabels=&quot;est&quot;, # plot model not parm ests rotation = 2, # default rotation = 1 with four options asize = 5, # arrows&#39; size esize = 2, # width of paths&#39; lines / curves edge.label.cex = 0.8, # font size of regr&#39;n coeffs sizeMan = 10, # font size of manifest variable names nCharNodes = 0, nCharEdges = 0, # don&#39;t limit variable name lengths fade = FALSE, # don&#39;t weight path width to reflect strength curvePivot = TRUE, # make straight edges instead of round ones curve = 2, # pull covariances&#39; curves out a little style = &quot;lisrel&quot;, # no variances vs. # &quot;ram&quot;&#39;s 2-headed for variances color = &quot;green&quot;, # color of variables edge.color = &quot;black&quot;, # color of edges/paths layout = &quot;tree2&quot;, # tree, spring, circle, circle2 residuals = TRUE) # residuals variances included in the path diagram semPaths(ex1fit_noCov_freeX, what=&#39;est&#39;, rotation = 2, # default rotation = 1 with four options curve = 2, # pull covariances&#39; curves out a little nCharNodes = 0, nCharEdges = 0, # don&#39;t limit variable name lengths sizeMan = 8, # font size of manifest variable names style = &quot;lisrel&quot;, # single-headed arrows vs. # &quot;ram&quot;&#39;s 2-headed for variances edge.label.cex=1.2, curvePivot = TRUE, fade=FALSE) 3.7 Exercise: How would you fit the model in Saunders et al. (2016)? "],["week4_2-lavaan-lab-2-mediation-and-indirect-effects.html", "Chapter 4 Week4_2: Lavaan Lab 2 Mediation and Indirect Effects 4.1 Reading-In and Working With Realistic Datasets In R 4.2 Using Lavaan For Mediation Models - Preacher &amp; Hayes’s 4.3 PART I: # Follow the two equations of M (DietSE) &amp; Y (Bulimia) 4.4 PART II Let’s run our model! 4.5 PART III: Summarizing Our Analysis: 4.6 PART IV: Bootstrap confidence intervals 4.7 In-Class Exercise: Use Lavaan to estimate and interpret the following model 4.8 Exercise: Eating Disorder Mediation Analysis", " Chapter 4 Week4_2: Lavaan Lab 2 Mediation and Indirect Effects In this lab, we will learn how to: perform a simple mediation analysis using Preacher &amp; Hayes (2004) + Bootstrap test mediation effects in the eating disorder path model 4.1 Reading-In and Working With Realistic Datasets In R If your data (eatingDisorderSimData.csv) is stored in you current working directory, then simply load your data by typing the name of the .csv file: labData &lt;- read.csv(file = &quot;eatingDisorderSimData.csv&quot;, header = T, sep = &quot;,&quot;) 4.2 Using Lavaan For Mediation Models - Preacher &amp; Hayes’s Load the package: library(lavaan) Part I: Writing the Model Syntax Part II: Analyzing the Model Using Your Dataset Part III: Examining the results. 4.3 PART I: # Follow the two equations of M (DietSE) &amp; Y (Bulimia) Diet Self-Efficacy = BMI + Disturbance Bulimic Symptoms = BMI + Diet Self-Efficacy + Disturbance Let’s write some model syntax: ex1MediationSyntax &lt;- &quot; #opening a quote #Regressions DietSE ~ BMI #M ~ X regression (a path) Bulimia ~ BMI + DietSE #Y ~ X + M regression (c prime and b) &quot; No need to fix disturbance covariances in simple mediation as none was estimated 4.4 PART II Let’s run our model! let fixed.x=FALSE to print more lines ex1fit_freeX &lt;- lavaan::sem(model = ex1MediationSyntax, data = labData, fixed.x = FALSE) summary(ex1fit_freeX) ## lavaan 0.6-10 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 1339 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## DietSE ~ ## BMI -0.100 0.026 -3.861 0.000 ## Bulimia ~ ## BMI 0.129 0.026 4.960 0.000 ## DietSE -0.213 0.028 -7.725 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .DietSE 0.955 0.037 25.875 0.000 ## .Bulimia 0.968 0.037 25.875 0.000 ## BMI 1.073 0.041 25.875 0.000 note that there are six parameter estimates and df = 0. But the output does not include the mediation effect a*b? 4.4.1 Label the mediation effect Let’s learn how to label parameters great tutorial example: http://lavaan.ugent.be/tutorial/mediation.html To label a parameter, include the coefficient label and an asterisk * before the variable to be labelled. E.g., y ~ b1x + b2m This would give x the label b1 and m the label b2 in the y regression. ex2MediationSyntax &lt;- &quot; #opening a quote #Regressions DietSE ~ a*BMI #Label the a coefficient in the M regression. Bulimia ~ cPrime*BMI + b*DietSE #Label the direct effect (cPrime) of X and direct effect of M (b) in the Y regression. &quot; What does this do? ex2fit &lt;- lavaan::sem(model = ex2MediationSyntax, data = labData, fixed.x=FALSE) summary(ex2fit) ## lavaan 0.6-10 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 1339 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## DietSE ~ ## BMI (a) -0.100 0.026 -3.861 0.000 ## Bulimia ~ ## BMI (cPrm) 0.129 0.026 4.960 0.000 ## DietSE (b) -0.213 0.028 -7.725 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .DietSE 0.955 0.037 25.875 0.000 ## .Bulimia 0.968 0.037 25.875 0.000 ## BMI 1.073 0.041 25.875 0.000 The regression coefficients have labels now! 4.4.2 Define a new term for the mediation effect a*b …using the labels we just created in ex2MediationSyntax The := operator in lavaan defines new terms to be tested: (name of a new term) := operator ex3MediationSyntax &lt;- &quot; #opening a quote #Regressions DietSE ~ a*BMI #Label the a coefficient in the M regression. Bulimia ~ cPrime*BMI + b*DietSE #Label the direct effect (cPrime) of X and direct effect of M (b) in the Y regression. #Define New Parameters ab := a*b #the product term is computed as a*b c := cPrime + ab #having defined ab, we can use this here. &quot; ex3fit &lt;- lavaan::sem(model = ex3MediationSyntax, data = labData, fixed.x=FALSE) summary(ex3fit) ## lavaan 0.6-10 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 1339 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## DietSE ~ ## BMI (a) -0.100 0.026 -3.861 0.000 ## Bulimia ~ ## BMI (cPrm) 0.129 0.026 4.960 0.000 ## DietSE (b) -0.213 0.028 -7.725 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .DietSE 0.955 0.037 25.875 0.000 ## .Bulimia 0.968 0.037 25.875 0.000 ## BMI 1.073 0.041 25.875 0.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## ab 0.021 0.006 3.454 0.001 ## c 0.151 0.027 5.678 0.000 Now there are two significance tests of the indirect effect ab and the total effect c! Question: why didn’t the #parameters change? Note: defining a new term is NOT equivalent to adding a new parameter! You can create as many terms as your want without changing the #parameters and the df 4.5 PART III: Summarizing Our Analysis: We can request standardized coefficients very easily by adding a statement to the summary command. summary(ex3fit, standardized = TRUE) #This includes standardized estimates. std.all contains usual regression standardization. ## lavaan 0.6-10 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 1339 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## DietSE ~ ## BMI (a) -0.100 0.026 -3.861 0.000 -0.100 -0.105 ## Bulimia ~ ## BMI (cPrm) 0.129 0.026 4.960 0.000 0.129 0.132 ## DietSE (b) -0.213 0.028 -7.725 0.000 -0.213 -0.205 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .DietSE 0.955 0.037 25.875 0.000 0.955 0.989 ## .Bulimia 0.968 0.037 25.875 0.000 0.968 0.935 ## BMI 1.073 0.041 25.875 0.000 1.073 1.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ab 0.021 0.006 3.454 0.001 0.021 0.022 ## c 0.151 0.027 5.678 0.000 0.151 0.153 summary(ex3fit, ci = T) #Include confidence intervals ## lavaan 0.6-10 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 1339 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## DietSE ~ ## BMI (a) -0.100 0.026 -3.861 0.000 -0.150 -0.049 ## Bulimia ~ ## BMI (cPrm) 0.129 0.026 4.960 0.000 0.078 0.181 ## DietSE (b) -0.213 0.028 -7.725 0.000 -0.267 -0.159 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .DietSE 0.955 0.037 25.875 0.000 0.882 1.027 ## .Bulimia 0.968 0.037 25.875 0.000 0.895 1.041 ## BMI 1.073 0.041 25.875 0.000 0.992 1.154 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## ab 0.021 0.006 3.454 0.001 0.009 0.033 ## c 0.151 0.027 5.678 0.000 0.099 0.203 or both! summary(ex3fit, standardized = TRUE, ci = T) ## lavaan 0.6-10 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 1339 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper Std.lv Std.all ## DietSE ~ ## BMI (a) -0.100 0.026 -3.861 0.000 -0.150 -0.049 -0.100 -0.105 ## Bulimia ~ ## BMI (cPrm) 0.129 0.026 4.960 0.000 0.078 0.181 0.129 0.132 ## DietSE (b) -0.213 0.028 -7.725 0.000 -0.267 -0.159 -0.213 -0.205 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper Std.lv Std.all ## .DietSE 0.955 0.037 25.875 0.000 0.882 1.027 0.955 0.989 ## .Bulimia 0.968 0.037 25.875 0.000 0.895 1.041 0.968 0.935 ## BMI 1.073 0.041 25.875 0.000 0.992 1.154 1.073 1.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper Std.lv Std.all ## ab 0.021 0.006 3.454 0.001 0.009 0.033 0.021 0.022 ## c 0.151 0.027 5.678 0.000 0.099 0.203 0.151 0.153 Important: the default significance tests of defined parameters in lavaan is Sobel’s test. 4.6 PART IV: Bootstrap confidence intervals 4.6.1 The default one is boot.ci.type = “perc” You can request bootstrap standard errors in sem() using se = “bootstrap” and bootstrap = 1000 set.seed(2022) ex3Boot &lt;- lavaan::sem(model = ex3MediationSyntax, data = labData, se = &quot;bootstrap&quot;, bootstrap = 1000, fixed.x=FALSE) This requires the full dataset - need more than the covariance matrix. se = “bootstrap” requests bootstrap standard errors. bootstrap = 1000 requests 1000 bootstrap samples. Request bootstrap CI: summary(ex3Boot, ci = TRUE) ## lavaan 0.6-10 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 1339 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Bootstrap ## Number of requested bootstrap draws 1000 ## Number of successful bootstrap draws 1000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## DietSE ~ ## BMI (a) -0.100 0.026 -3.779 0.000 -0.152 -0.048 ## Bulimia ~ ## BMI (cPrm) 0.129 0.025 5.099 0.000 0.080 0.177 ## DietSE (b) -0.213 0.027 -7.820 0.000 -0.269 -0.159 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .DietSE 0.955 0.035 26.909 0.000 0.884 1.031 ## .Bulimia 0.968 0.038 25.378 0.000 0.890 1.041 ## BMI 1.073 0.044 24.482 0.000 0.991 1.165 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## ab 0.021 0.006 3.319 0.001 0.010 0.035 ## c 0.151 0.026 5.814 0.000 0.099 0.200 Now we have bootstrap standard error and percentile confidence interval for ab! 4.6.2 BC (bias-corrected) confidence interval What about other types of bootstrap confidence intervals? You can request a BC (bias-corrected) by adding an argument boot.ci.type = “bca.simple” to parameterEstimates(): parameterEstimates(ex3Boot, level = 0.95, boot.ci.type=&quot;bca.simple&quot;) ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 DietSE ~ BMI a -0.100 0.026 -3.779 0.000 -0.154 -0.048 ## 2 Bulimia ~ BMI cPrime 0.129 0.025 5.099 0.000 0.076 0.176 ## 3 Bulimia ~ DietSE b -0.213 0.027 -7.820 0.000 -0.264 -0.157 ## 4 DietSE ~~ DietSE 0.955 0.035 26.909 0.000 0.888 1.035 ## 5 Bulimia ~~ Bulimia 0.968 0.038 25.378 0.000 0.898 1.045 ## 6 BMI ~~ BMI 1.073 0.044 24.482 0.000 0.990 1.165 ## 7 ab := a*b ab 0.021 0.006 3.319 0.001 0.011 0.036 ## 8 c := cPrime+ab c 0.151 0.026 5.814 0.000 0.097 0.198 which returns a 95% BC confidence interval. This approach will yield similar results to the PROCESS Macro in SPSS with bias-corrected standard errors. 4.7 In-Class Exercise: Use Lavaan to estimate and interpret the following model ex4MediationSyntax &lt;- &quot; #Regressions DietSE ~ a*SelfEsteem Risk ~ cPrime*SelfEsteem + b*DietSE #Define New Parameters ab := a*b #the product term is computed as a*b c := cPrime + ab #having defined ab, we can use this here. &quot; ex4fit &lt;- lavaan::sem(model = ex4MediationSyntax, data = labData, fixed.x=FALSE) summary(ex4fit, ci = T) ## lavaan 0.6-10 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 1339 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## DietSE ~ ## SlfEstm (a) 0.105 0.027 3.949 0.000 0.053 0.158 ## Risk ~ ## SlfEstm (cPrm) -0.239 0.027 -8.728 0.000 -0.292 -0.185 ## DietSE (b) 0.100 0.028 3.583 0.000 0.045 0.154 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .DietSE 0.954 0.037 25.875 0.000 0.882 1.027 ## .Risk 0.991 0.038 25.875 0.000 0.916 1.066 ## SelfEsteem 1.001 0.039 25.875 0.000 0.926 1.077 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## ab 0.011 0.004 2.653 0.008 0.003 0.018 ## c -0.228 0.027 -8.352 0.000 -0.282 -0.175 Bootstrap confidence intervals: set.seed(2022) ex4Boot &lt;- lavaan::sem(model = ex4MediationSyntax, data = labData, se = &quot;bootstrap&quot;, bootstrap = 1000, fixed.x=FALSE) parameterEstimates(ex4Boot, level = 0.95, boot.ci.type=&quot;bca.simple&quot;) ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 DietSE ~ SelfEsteem a 0.105 0.024 4.364 0.000 0.055 0.151 ## 2 Risk ~ SelfEsteem cPrime -0.239 0.026 -9.162 0.000 -0.291 -0.189 ## 3 Risk ~ DietSE b 0.100 0.028 3.609 0.000 0.049 0.156 ## 4 DietSE ~~ DietSE 0.954 0.035 26.887 0.000 0.888 1.032 ## 5 Risk ~~ Risk 0.991 0.038 25.844 0.000 0.919 1.072 ## 6 SelfEsteem ~~ SelfEsteem 1.001 0.040 25.164 0.000 0.928 1.083 ## 7 ab := a*b ab 0.011 0.004 2.752 0.006 0.004 0.020 ## 8 c := cPrime+ab c -0.228 0.026 -8.693 0.000 -0.281 -0.176 4.8 Exercise: Eating Disorder Mediation Analysis Give it a try before peaking the answers! Hints: Label the regression coefficients: b1 - b12; Fix all disturbance covariances at 0; Define mediation effects and total effects for each of the six mediation models using the labels; Request bootstrap standard errors using se = “bootstrap”; Print and interpret the mediation effects; (Optional) Identify and interpret the inconsistent mediation effects. I’ll get you started: 4.8.1 Step 1: Labeling and defining the parameters 4.8.2 Step 2: Fix all disturbance covariances at 0 ex5PathSyntax_noCov &lt;- &quot; #opening a quote DietSE ~ b1*BMI + b5*SelfEsteem #DietSE is predicted by BMI and SelfEsteem Bulimia ~ b10*DietSE + b2*BMI + b6*SelfEsteem Restrictive ~ b11*DietSE + b3*BMI + b7*SelfEsteem Risk ~ b12*DietSE + b4*BMI + b8*SelfEsteem + b9*Accu #Disturbance covariances (fixed at 0): DietSE ~~ 0*Bulimia # ~~ indicates a two-headed arrow (variance or covariance) DietSE ~~ 0*Restrictive # 0* in front of the 2nd variable fixes the covariance at 0 DietSE ~~ 0*Risk # These lines say that all endogenous variables have no correlated disturbance variances Bulimia ~~ 0*Restrictive Bulimia ~~ 0*Risk Restrictive ~~ 0*Risk &quot; 4.8.3 Step 3: Define new terms for mediation effects Recall: Define New Parameters ab := ab #the product term is computed as ab ex5MediationSyntax &lt;- &quot; DietSE ~ b1*BMI + b5*SelfEsteem #DietSE is predicted by BMI and SelfEsteem Bulimia ~ b10*DietSE + b2*BMI + b6*SelfEsteem Restrictive ~ b11*DietSE + b3*BMI + b7*SelfEsteem Risk ~ b12*DietSE + b4*BMI + b8*SelfEsteem + b9*Accu #Disturbance covariances (fixed at 0): DietSE ~~ 0*Bulimia # ~~ indicates a two-headed arrow (variance or covariance) DietSE ~~ 0*Restrictive # 0* in front of the 2nd variable fixes the covariance at 0 DietSE ~~ 0*Risk # These lines say that all endogenous variables have no correlated disturbance variances Bulimia ~~ 0*Restrictive Bulimia ~~ 0*Risk Restrictive ~~ 0*Risk #Define New Parameters med1 := b1*b10 total1 := b2 + med1 med2 := b1*b11 total2 := b3 + med2 med3 := b1*b12 total3 := b4 + med3 med4 := b5*b10 total4 := b6 + med4 med5 := b5*b11 total5 := b7 + med5 med6 := b5*b12 total6 := b8 + med6 &quot; ex5fit &lt;- lavaan::sem(model = ex5MediationSyntax, data = labData, fixed.x=FALSE) summary(ex5fit, ci = T) 4.8.4 Step 4: Bootstrap confidence intervals: 4.8.5 Step 5: Print and interpret the mediation effects; set.seed(2022) ex5Boot &lt;- lavaan::sem(model = ex5MediationSyntax, data = labData, se = &quot;bootstrap&quot;, bootstrap = 1000, fixed.x=FALSE) parameterEstimates(ex5Boot, level = 0.95, boot.ci.type=&quot;bca.simple&quot;) ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 DietSE ~ BMI b1 -0.088 0.026 -3.336 0.001 -0.141 -0.036 ## 2 DietSE ~ SelfEsteem b5 0.093 0.024 3.862 0.000 0.044 0.139 ## 3 Bulimia ~ DietSE b10 -0.185 0.026 -7.001 0.000 -0.237 -0.128 ## 4 Bulimia ~ BMI b2 0.096 0.025 3.844 0.000 0.047 0.142 ## 5 Bulimia ~ SelfEsteem b6 -0.284 0.025 -11.284 0.000 -0.338 -0.237 ## 6 Restrictive ~ DietSE b11 -0.166 0.027 -6.192 0.000 -0.217 -0.109 ## 7 Restrictive ~ BMI b3 -0.154 0.025 -6.173 0.000 -0.203 -0.107 ## 8 Restrictive ~ SelfEsteem b7 -0.123 0.026 -4.668 0.000 -0.174 -0.071 ## 9 Risk ~ DietSE b12 0.102 0.027 3.737 0.000 0.050 0.157 ## 10 Risk ~ BMI b4 0.053 0.026 2.037 0.042 0.003 0.104 ## 11 Risk ~ SelfEsteem b8 -0.228 0.026 -8.654 0.000 -0.281 -0.178 ## 12 Risk ~ Accu b9 -0.095 0.028 -3.399 0.001 -0.151 -0.041 ## 13 DietSE ~~ Bulimia 0.000 0.000 NA NA 0.000 0.000 ## 14 DietSE ~~ Restrictive 0.000 0.000 NA NA 0.000 0.000 ## 15 DietSE ~~ Risk 0.000 0.000 NA NA 0.000 0.000 ## 16 Bulimia ~~ Restrictive 0.000 0.000 NA NA 0.000 0.000 ## 17 Bulimia ~~ Risk 0.000 0.000 NA NA 0.000 0.000 ## 18 Restrictive ~~ Risk 0.000 0.000 NA NA 0.000 0.000 ## 19 DietSE ~~ DietSE 0.946 0.035 27.052 0.000 0.880 1.024 ## 20 Bulimia ~~ Bulimia 0.890 0.036 24.842 0.000 0.826 0.963 ## 21 Restrictive ~~ Restrictive 0.955 0.036 26.606 0.000 0.892 1.033 ## 22 Risk ~~ Risk 0.979 0.038 25.718 0.000 0.912 1.062 ## 23 BMI ~~ BMI 1.073 0.044 24.482 0.000 0.990 1.165 ## 24 BMI ~~ SelfEsteem -0.138 0.029 -4.802 0.000 -0.197 -0.083 ## 25 BMI ~~ Accu -0.021 0.027 -0.755 0.450 -0.079 0.030 ## 26 SelfEsteem ~~ SelfEsteem 1.001 0.040 25.164 0.000 0.928 1.083 ## 27 SelfEsteem ~~ Accu 0.035 0.027 1.304 0.192 -0.016 0.090 ## 28 Accu ~~ Accu 0.971 0.037 26.425 0.000 0.905 1.052 ## 29 med1 := b1*b10 med1 0.016 0.006 2.925 0.003 0.007 0.029 ## 30 total1 := b2+med1 total1 0.112 0.025 4.439 0.000 0.060 0.159 ## 31 med2 := b1*b11 med2 0.015 0.005 2.825 0.005 0.006 0.026 ## 32 total2 := b3+med2 total2 -0.139 0.025 -5.495 0.000 -0.192 -0.091 ## 33 med3 := b1*b12 med3 -0.009 0.004 -2.536 0.011 -0.018 -0.003 ## 34 total3 := b4+med3 total3 0.044 0.026 1.681 0.093 -0.005 0.096 ## 35 med4 := b5*b10 med4 -0.017 0.005 -3.399 0.001 -0.029 -0.009 ## 36 total4 := b6+med4 total4 -0.301 0.026 -11.652 0.000 -0.356 -0.253 ## 37 med5 := b5*b11 med5 -0.015 0.005 -3.238 0.001 -0.026 -0.007 ## 38 total5 := b7+med5 total5 -0.139 0.027 -5.154 0.000 -0.191 -0.085 ## 39 med6 := b5*b12 med6 0.010 0.004 2.641 0.008 0.004 0.018 ## 40 total6 := b8+med6 total6 -0.219 0.027 -8.227 0.000 -0.273 -0.168 4.8.6 Plot it! library(semPlot) semPaths(ex5Boot, what=&#39;est&#39;, rotation = 2, # default rotation = 1 with four options curve = 2, # pull covariances&#39; curves out a little nCharNodes = 0, nCharEdges = 0, # don&#39;t limit variable name lengths sizeMan = 8, # font size of manifest variable names style = &quot;lisrel&quot;, # single-headed arrows vs. # &quot;ram&quot;&#39;s 2-headed for variances edge.label.cex=1.2, curvePivot = TRUE, fade=FALSE) "],["week5_2-lavaan-lab-3-moderation-and-conditional-effects.html", "Chapter 5 Week5_2: Lavaan Lab 3 Moderation and Conditional Effects 5.1 Reading-In Datasets 5.2 Interactions in Regression Using lm() 5.3 Interactions in Lavaan 5.4 Visual inspection of interactions 5.5 Centering Continuous Moderator 5.6 Interactions in Lavaan (Continuous Moderator) 5.7 Simple Slopes Analysis 5.8 Visual inspection of interactions (lm approach) 5.9 JOHNSON-NEYMAN INTERVAL 5.10 Exercise: How Framing Affects Justifications for Giving or Withholding Aid to Disaster Victims", " Chapter 5 Week5_2: Lavaan Lab 3 Moderation and Conditional Effects In this lab, we will learn how to: how to perform moderation using regression and sem test the moderation effects of binary and continuous moderators visualize moderation effects. 5.1 Reading-In Datasets Let’s read this dataset in. Change the file path to whatever directory where you saved the file! cbtData &lt;- read.csv(file = &quot;dataInClass.csv&quot;, header = T) Let’s examine this dataset: head(cbtData) ## ID CBT CBTDummy NeedCog NegThoughts Depression NeedCogCont ## 1 1 CBT Treatment 1 0 -4.1453029 -5.802172 0.0182802 ## 2 2 Information Only 0 1 2.1775218 5.496665 1.4238703 ## 3 3 CBT Treatment 1 0 -1.5551349 -1.950566 -1.0151726 ## 4 4 Information Only 0 0 0.1679286 2.655801 -0.8547152 ## 5 5 Information Only 0 1 2.5103192 6.855488 0.6759705 ## 6 6 CBT Treatment 1 0 -3.1626670 -2.968198 -0.9123426 str(cbtData) ## &#39;data.frame&#39;: 1000 obs. of 7 variables: ## $ ID : int 1 2 3 4 5 6 7 8 9 10 ... ## $ CBT : chr &quot;CBT Treatment&quot; &quot;Information Only&quot; &quot;CBT Treatment&quot; &quot;Information Only&quot; ... ## $ CBTDummy : int 1 0 1 0 0 1 1 1 1 0 ... ## $ NeedCog : int 0 1 0 0 1 0 0 0 0 0 ... ## $ NegThoughts: num -4.145 2.178 -1.555 0.168 2.51 ... ## $ Depression : num -5.8 5.5 -1.95 2.66 6.86 ... ## $ NeedCogCont: num 0.0183 1.4239 -1.0152 -0.8547 0.676 ... colSums(is.na(cbtData)) ## ID CBT CBTDummy NeedCog NegThoughts Depression NeedCogCont ## 0 0 0 0 0 0 0 Notice that the first two columns are not model variables col 1 is a case ID variable. col 2 is a factor variable indicating CBT vs. Info-Only treatment. Besides, col 5 is a variable that measures negative thoughts. col 7 is a continuous measure of NeedCog. In the first part of this demo, we will work with three variables: CBTDummy, NeedCog, and Depression Let’s look at the covariance matrix of the three variables Multiple ways to accomplish this: cov(cbtData[,-c(1,2,5,7)]) ## CBTDummy NeedCog Depression ## CBTDummy 0.250250250 -0.008508509 -2.3372519 ## NeedCog -0.008508509 0.213732733 0.4371738 ## Depression -2.337251860 0.437173798 31.9301427 cov(cbtData[,c(3,4,6)]) ## CBTDummy NeedCog Depression ## CBTDummy 0.250250250 -0.008508509 -2.3372519 ## NeedCog -0.008508509 0.213732733 0.4371738 ## Depression -2.337251860 0.437173798 31.9301427 cov(cbtData[,c(&quot;CBTDummy&quot;, &quot;NeedCog&quot;, &quot;Depression&quot;)]) ## CBTDummy NeedCog Depression ## CBTDummy 0.250250250 -0.008508509 -2.3372519 ## NeedCog -0.008508509 0.213732733 0.4371738 ## Depression -2.337251860 0.437173798 31.9301427 let’s round this to two decimals round(cov(cbtData[,c(&quot;CBTDummy&quot;, &quot;NeedCog&quot;, &quot;Depression&quot;)]), digits = 2) ## CBTDummy NeedCog Depression ## CBTDummy 0.25 -0.01 -2.34 ## NeedCog -0.01 0.21 0.44 ## Depression -2.34 0.44 31.93 What about the means? round(apply(cbtData[,c(&quot;CBTDummy&quot;, &quot;NeedCog&quot;, &quot;Depression&quot;)], 2, mean), 2) ## CBTDummy NeedCog Depression ## 0.50 0.31 -1.59 Although they are not centered, we will proceed because CBTDummy and NeedCog are both binary. 5.2 Interactions in Regression Using lm() In regression course we learned the lm() function, which stands for linear model. To include an interaction in regression, simply use an : to create a product in the formula: interactionModel &lt;- lm(formula = Depression ~ CBTDummy + NeedCog + CBTDummy:NeedCog, data = cbtData) NOTE: R is very helpful, in that if you just put an asterisk *, it includes all lower-order terms! interactionModel &lt;- lm(formula = Depression ~ CBTDummy*NeedCog, data = cbtData) Let’s look at this interaction model: summary(interactionModel) ## ## Call: ## lm(formula = Depression ~ CBTDummy * NeedCog, data = cbtData) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.7785 -1.4280 0.0662 1.6252 6.8283 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.2684 0.1345 9.428 &lt;2e-16 *** ## CBTDummy -6.8119 0.1880 -36.240 &lt;2e-16 *** ## NeedCog 5.5586 0.2356 23.590 &lt;2e-16 *** ## CBTDummy:NeedCog -8.0093 0.3384 -23.666 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.47 on 996 degrees of freedom ## Multiple R-squared: 0.8095, Adjusted R-squared: 0.809 ## F-statistic: 1411 on 3 and 996 DF, p-value: &lt; 2.2e-16 Let’s interpret this … (In class) 5.3 Interactions in Lavaan Now let us write the same model using lavaan. Load the package: library(lavaan) 5.3.1 IMPORTANT NOTE Because lavaan uses the * for assigning coefficient labels, this cannot be used to create interaction terms. Instead, we have to create the product term in the dataset first, before running our model. This is easy to do. General Format: existingDataFrame$variableName &lt;- vectorToBeAssignedAsNewVariable cbtData$CBTxNeedCog &lt;- cbtData$CBTDummy * cbtData$NeedCog You can name the product term arbitrarily: cbtData$fourth &lt;- cbtData$CBTDummy * cbtData$NeedCog Let’s look at cbtData again: head(cbtData) ## ID CBT CBTDummy NeedCog NegThoughts Depression NeedCogCont CBTxNeedCog ## 1 1 CBT Treatment 1 0 -4.1453029 -5.802172 0.0182802 0 ## 2 2 Information Only 0 1 2.1775218 5.496665 1.4238703 0 ## 3 3 CBT Treatment 1 0 -1.5551349 -1.950566 -1.0151726 0 ## 4 4 Information Only 0 0 0.1679286 2.655801 -0.8547152 0 ## 5 5 Information Only 0 1 2.5103192 6.855488 0.6759705 0 ## 6 6 CBT Treatment 1 0 -3.1626670 -2.968198 -0.9123426 0 Now you have a new variable called CBTxNeedCog at the end. 5.3.2 Follow the equation of Y (Depression): Depression = CBTDummy + NeedCog + CBTDummy*NeedCog + Disturbance Let’s write some model syntax (with the labels): interactionSyntax &lt;- &quot; #Regression with interaction #with labels Depression ~ b1*CBTDummy + b2*NeedCog + b3*CBTxNeedCog &quot; let fixed.x=FALSE to print more lines: inter_fit1 &lt;- sem(model = interactionSyntax, data = cbtData, fixed.x = FALSE) If you’d like lavaan to print means and intercepts, we need to ask sem() to include the meanstructure: inter_fit1 &lt;- sem(model = interactionSyntax, data = cbtData, fixed.x =FALSE, meanstructure = TRUE) summary(inter_fit1) ## lavaan 0.6-10 ended normally after 33 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 14 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## Depression ~ ## CBTDummy (b1) -6.812 0.188 -36.312 0.000 ## NeedCog (b2) 5.559 0.235 23.637 0.000 ## CBTxNedCg (b3) -8.009 0.338 -23.713 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## CBTDummy ~~ ## NeedCog -0.009 0.007 -1.163 0.245 ## CBTxNeedCog 0.073 0.006 12.083 0.000 ## NeedCog ~~ ## CBTxNeedCog 0.101 0.006 16.630 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .Depression 1.268 0.134 9.446 0.000 ## CBTDummy 0.500 0.016 31.623 0.000 ## NeedCog 0.309 0.015 21.147 0.000 ## CBTxNeedCog 0.146 0.011 13.075 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .Depression 6.076 0.272 22.361 0.000 ## CBTDummy 0.250 0.011 22.361 0.000 ## NeedCog 0.214 0.010 22.361 0.000 ## CBTxNeedCog 0.125 0.006 22.361 0.000 How does this compare to our regression model? summary(interactionModel) ## ## Call: ## lm(formula = Depression ~ CBTDummy * NeedCog, data = cbtData) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.7785 -1.4280 0.0662 1.6252 6.8283 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.2684 0.1345 9.428 &lt;2e-16 *** ## CBTDummy -6.8119 0.1880 -36.240 &lt;2e-16 *** ## NeedCog 5.5586 0.2356 23.590 &lt;2e-16 *** ## CBTDummy:NeedCog -8.0093 0.3384 -23.666 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.47 on 996 degrees of freedom ## Multiple R-squared: 0.8095, Adjusted R-squared: 0.809 ## F-statistic: 1411 on 3 and 996 DF, p-value: &lt; 2.2e-16 Same…but sem is more verbose. 5.4 Visual inspection of interactions One way to plot the interactions is to use the interact_plot() function on the lm() object. Install and load the package interactions first: library(interactions) interact_plot(interactionModel, pred = &quot;CBTDummy&quot;, modx = &quot;NeedCog&quot;) 5.5 Centering Continuous Moderator Now let’s work with the continuous measure of NeedCog directly: mean(cbtData$NeedCogCont) ## [1] 0.005925852 sd(cbtData$NeedCogCont) ## [1] 0.9974319 NeedCogCont has been standardized already, which is helpful. If not, we use scale() function to center a continuous variable Usage: scale(x, center = TRUE, scale = TRUE) If you just need to center a variable, you disable scale=FALSE centeredNeedCog &lt;- scale(cbtData$NeedCogCont, center = TRUE, scale = FALSE) hist(centeredNeedCog) For now, we will leave these variables as is in our dataset. But the scale() function is good to know. 5.6 Interactions in Lavaan (Continuous Moderator) Just like for binary NeedCog moderator, we have to manually create a product term in the dataset first before running our model. This is easy to do: cbtData$CBTxNeedCogCont &lt;- cbtData$CBTDummy * cbtData$NeedCogCont Let’s look at cbtData again: head(cbtData) ## ID CBT CBTDummy NeedCog NegThoughts Depression NeedCogCont CBTxNeedCog CBTxNeedCogCont ## 1 1 CBT Treatment 1 0 -4.1453029 -5.802172 0.0182802 0 0.0182802 ## 2 2 Information Only 0 1 2.1775218 5.496665 1.4238703 0 0.0000000 ## 3 3 CBT Treatment 1 0 -1.5551349 -1.950566 -1.0151726 0 -1.0151726 ## 4 4 Information Only 0 0 0.1679286 2.655801 -0.8547152 0 0.0000000 ## 5 5 Information Only 0 1 2.5103192 6.855488 0.6759705 0 0.0000000 ## 6 6 CBT Treatment 1 0 -3.1626670 -2.968198 -0.9123426 0 -0.9123426 Time to write some lavaan model syntax (with labels): interactionSyntax2 &lt;- &quot; #Regression Depression ~ b1*CBTDummy + b2*NeedCogCont + b3*CBTxNeedCogCont &quot; Let’s ask sem() to include the meanstructure: inter_fit2 &lt;- sem(model = interactionSyntax2, data = cbtData, fixed.x =FALSE, meanstructure = TRUE) summary(inter_fit2) ## lavaan 0.6-10 ended normally after 32 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 14 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## Depression ~ ## CBTDummy (b1) -9.245 0.113 -82.096 0.000 ## NeedCgCnt (b2) 3.305 0.079 42.064 0.000 ## CBTxNdCgC (b3) -4.967 0.113 -43.943 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## CBTDummy ~~ ## NeedCogCont -0.020 0.016 -1.249 0.212 ## CBTxNeedCogCnt -0.008 0.011 -0.764 0.445 ## NeedCogCont ~~ ## CBTxNeedCogCnt 0.480 0.027 18.055 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .Depression 2.931 0.080 36.794 0.000 ## CBTDummy 0.500 0.016 31.623 0.000 ## NeedCogCont 0.006 0.032 0.188 0.851 ## CBTxNeedCogCnt -0.017 0.022 -0.764 0.445 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .Depression 3.166 0.142 22.361 0.000 ## CBTDummy 0.250 0.011 22.361 0.000 ## NeedCogCont 0.994 0.044 22.361 0.000 ## CBTxNeedCogCnt 0.480 0.021 22.361 0.000 5.7 Simple Slopes Analysis pick-a-point (Rogosa, 1980) and plot the simple slopes of X at designated levels of Z: mean(cbtData$NeedCogCont) #0 ## [1] 0.005925852 sd(cbtData$NeedCogCont) # almost 1 ## [1] 0.9974319 mean(cbtData$NeedCogCont) - sd(cbtData$NeedCogCont) # 1sd below the mean ## [1] -0.991506 mean(cbtData$NeedCogCont) + sd(cbtData$NeedCogCont) # 1sd above the mean ## [1] 1.003358 interactionSyntax3 &lt;- &quot; #Regression Depression ~ b1*CBTDummy + b2*NeedCogCont + b3*CBTxNeedCogCont #regression coefficient labels #Simple Slopes SSHigh := b1+b3*1 #Since sd(NeedCogCont) = approximately 1, this is +1 SD SSMod := b1+b3*0 #at the mean of (centered) NeedCogCont SSLow := b1+b3*(-1) #Low Simple Slope is at -1 (1 SD below since SD = 1) &quot; inter_fit3 &lt;- sem(model = interactionSyntax3, data = cbtData, fixed.x =FALSE, meanstructure = TRUE) summary(inter_fit3) ## lavaan 0.6-10 ended normally after 32 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 14 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## Depression ~ ## CBTDummy (b1) -9.245 0.113 -82.096 0.000 ## NeedCgCnt (b2) 3.305 0.079 42.064 0.000 ## CBTxNdCgC (b3) -4.967 0.113 -43.943 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## CBTDummy ~~ ## NeedCogCont -0.020 0.016 -1.249 0.212 ## CBTxNeedCogCnt -0.008 0.011 -0.764 0.445 ## NeedCogCont ~~ ## CBTxNeedCogCnt 0.480 0.027 18.055 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .Depression 2.931 0.080 36.794 0.000 ## CBTDummy 0.500 0.016 31.623 0.000 ## NeedCogCont 0.006 0.032 0.188 0.851 ## CBTxNeedCogCnt -0.017 0.022 -0.764 0.445 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .Depression 3.166 0.142 22.361 0.000 ## CBTDummy 0.250 0.011 22.361 0.000 ## NeedCogCont 0.994 0.044 22.361 0.000 ## CBTxNeedCogCnt 0.480 0.021 22.361 0.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## SSHigh -14.212 0.159 -89.281 0.000 ## SSMod -9.245 0.113 -82.096 0.000 ## SSLow -4.279 0.160 -26.755 0.000 Now we have tests of the simple slopes at low, moderate, and high values of the moderator! Along with significance tests. 5.8 Visual inspection of interactions (lm approach) Interactions in Regression Using lm() To include ab interaction in regression, simply use an * to create a product in the formula. interactionModel2 &lt;- lm(Depression ~ CBTDummy*NeedCogCont, cbtData) summary(interactionModel2) ## ## Call: ## lm(formula = Depression ~ CBTDummy * NeedCogCont, data = cbtData) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7282 -1.1381 0.0649 1.2229 4.4762 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.93065 0.07981 36.72 &lt;2e-16 *** ## CBTDummy -9.24546 0.11284 -81.93 &lt;2e-16 *** ## NeedCogCont 3.30525 0.07873 41.98 &lt;2e-16 *** ## CBTDummy:NeedCogCont -4.96674 0.11325 -43.85 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.783 on 996 degrees of freedom ## Multiple R-squared: 0.9008, Adjusted R-squared: 0.9005 ## F-statistic: 3013 on 3 and 996 DF, p-value: &lt; 2.2e-16 pick-a-point (Rogosa, 1980) and plot the simple slopes of X at designated levels of Z: library(interactions) interact_plot(interactionModel2, pred = &quot;CBTDummy&quot;, modx = &quot;NeedCogCont&quot;) 5.9 JOHNSON-NEYMAN INTERVAL interactions::johnson_neyman(interactionModel2, pred = &quot;CBTDummy&quot;, modx = &quot;NeedCogCont&quot;, alpha = 0.05) ## JOHNSON-NEYMAN INTERVAL ## ## When NeedCogCont is OUTSIDE the interval [-1.96, -1.77], the slope of CBTDummy is p &lt; .05. ## ## Note: The range of observed values of NeedCogCont is [-2.83, 3.31] 5.10 Exercise: How Framing Affects Justifications for Giving or Withholding Aid to Disaster Victims For this exercise, we will use a real dataset in a study by Chapman and Lickel (2016). This study was interested in examining the relation between Climate Change and Disasters: How Framing Affects Justifications for Giving or Withholding Aid to Disaster Victims? Researchers hypothesizes that Framing a natural disaster as the product of climate change impacts attitudes toward disaster victims and humanitarian relief. The predictor is X/Frame: Participants read a story about a humanitarian crisis caused by a drought in Africa. X = 1: Half of the participants were told that the drought was caused by climate change (the climate change condition) X = 0: The other half were not told anything about the specific cause of the drought and thus had no reason to believe it wasn’t the result of natural causes (the natural causes condition). The outcome is Y/Donate: the participants’ willingness to donate to the victims was assessed using a set of questions. Responses were made on a set of 7-point scales, with higher scores reflecting a greater willingness to donate to the victims The moderator is W/Skeptic: The belief whether climate change is a real phenomenon was also measured. The moderation model looks at whether the attribution frame manipulation (X) might have had a different effect on people’s willingness to donate (Y) depending on their climate change skepticism (M) 5.10.1 Data Prep The following example data are from Chapman and Lickel (2016) Also example data in Chapter 12 of Hayes (2017) Simply load the .rda into R: load(&quot;disaster.rda&quot;) head(disaster) ## id frame donate justify skeptic ## 1 1 1 5.6 2.95 1.8 ## 2 2 1 4.2 2.85 5.2 ## 3 3 1 4.2 3.00 3.2 ## 4 4 1 4.6 3.30 1.0 ## 5 5 1 3.0 5.00 7.6 ## 6 6 0 5.0 3.20 4.2 str(disaster) ## &#39;data.frame&#39;: 211 obs. of 5 variables: ## $ id : num 1 2 3 4 5 6 7 8 9 10 ... ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.0&quot; ## ..- attr(*, &quot;display_width&quot;)= int 6 ## $ frame : num 1 1 1 1 1 0 0 1 0 0 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Experimental condition&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.2&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:2] 0 1 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;naturally caused disaster&quot; &quot;climate change caused disaster&quot; ## $ donate : num 5.6 4.2 4.2 4.6 3 5 4.8 6 4.2 4.4 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Positive attitudes toward donating&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.2&quot; ## ..- attr(*, &quot;display_width&quot;)= int 9 ## $ justify: num 2.95 2.85 3 3.3 5 3.2 2.9 1.4 3.25 3.55 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Negative justifications&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.2&quot; ## ..- attr(*, &quot;display_width&quot;)= int 10 ## $ skeptic: num 1.8 5.2 3.2 1 7.6 4.2 4.2 1.2 1.8 8.8 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Climate change skepticism&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.2&quot; If you are able to install package processR, you can also view its help page: install.packages(&quot;processR&quot;) library(processR) data(disaster) # take a look at the dataset: ?disaster You probably have to go to https://www.xquartz.org/ to download and install X11, which is a server required by many R packages, including processR. Now, disaster is a data.frame with 211 obs. of 5 variables: id frame: Experimental condition. 0 = naturally caused disaster, 1 = climate change caused disaster donate: Positive attitudes toward donating justify: Negative justifications skeptic: Climate change skepticism 5.10.2 Moderation with a binary moderator Let me first manually create a binary moderator based on the continuous version of skeptic: disaster$skeptic_b &lt;- ifelse(disaster$skeptic&lt;3, 0, 1) # low and high levels of skeptism of climate change table(disaster$skeptic_b) ## ## 0 1 ## 112 99 Next, can you test the moderation effect of skeptic_b on the path from frame to donate? (you can use either lm or lavaan) Please interpret the coefficients in the model above and visualize the interaction using interact_plot(). "],["week6_1-lavaan-lab-4-mediated-moderation-moderated-mediation.html", "Chapter 6 Week6_1: Lavaan Lab 4 Mediated Moderation &amp; Moderated Mediation 6.1 PART 1: Mediated Moderation (Indirect Conditional effect) 6.2 PART 2: Moderated Mediation (Conditional Indirect effect)", " Chapter 6 Week6_1: Lavaan Lab 4 Mediated Moderation &amp; Moderated Mediation In this lab, we will learn how to: Estimate the mediated moderation model Estimate the moderated mediation model Bootstrap the effects Conduct simple slope analyses 6.1 PART 1: Mediated Moderation (Indirect Conditional effect) 6.1.1 Step 1: Read-in Data Imagine that we extended our CBT study by adding a mediator: the average number of daily negative thoughts reported at the end of six weeks. The hypothesis we will test is that NegThoughts mediates the CBT*NeedCog -&gt; Depression path Let’s read this dataset in: cbtData &lt;- read.csv(file = &quot;dataInClass.csv&quot;, header = T, sep = &#39;,&#39;) This time we work with the continuous version of the moderator: NeedCogCont. Let’s examine their means and standard deviations: apply(cbtData[,-c(1,2)], 2, mean) ## CBTDummy NeedCog NegThoughts Depression NeedCogCont ## 0.500000000 0.309000000 -1.944249996 -1.589356290 0.005925852 apply(cbtData[,-c(1,2)], 2, sd) ## CBTDummy NeedCog NegThoughts Depression NeedCogCont ## 0.5002502 0.4623124 3.1877174 5.6506763 0.9974319 Why dropping the first two variables? The first two variables ID and CBT are not numerical and have no means. NeedCogCont has been standardized already, which is helpful. If not, don’t forget to use scale() function to center the continuous variables. 6.1.2 Step 2: Create the interaction term for Moderation Analysis To test the moderation effect, we have to manually create a product term in the dataset before running our model: cbtData$CBTxNeedCogCont &lt;- cbtData$CBTDummy * cbtData$NeedCogCont Let’s look at cbtData again: head(cbtData) ## ID CBT CBTDummy NeedCog NegThoughts Depression NeedCogCont CBTxNeedCogCont ## 1 1 CBT Treatment 1 0 -4.1453029 -5.802172 0.0182802 0.0182802 ## 2 2 Information Only 0 1 2.1775218 5.496665 1.4238703 0.0000000 ## 3 3 CBT Treatment 1 0 -1.5551349 -1.950566 -1.0151726 -1.0151726 ## 4 4 Information Only 0 0 0.1679286 2.655801 -0.8547152 0.0000000 ## 5 5 Information Only 0 1 2.5103192 6.855488 0.6759705 0.0000000 ## 6 6 CBT Treatment 1 0 -3.1626670 -2.968198 -0.9123426 -0.9123426 6.1.3 Step 3: Write the syntax and Fit the model load the package: library(lavaan) Follow the two equations to write the model syntax: ex1MedModerationBasic &lt;- &quot; # label the coefficients: NegThoughts ~ a_m1*CBTxNeedCogCont + a_m2*NeedCogCont + a_m3*CBTDummy Depression ~ b1*CBTDummy + b2*NeedCogCont + b3*CBTxNeedCogCont + bM*NegThoughts #Define New Parameter Using := #Mediated Moderation effect MedMod_ab := a_m1*bM TotalMod := MedMod_ab + b3 &quot; Since we included the intercept term, we need to ask sem() to include the meanstructure: ex1fit &lt;- sem(model = ex1MedModerationBasic, data = cbtData, fixed.x = FALSE, meanstructure = TRUE) summary(ex1fit, ci = T) ## lavaan 0.6-10 ended normally after 38 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 20 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## NegThoughts ~ ## CBTxNCC (a_m1) -3.071 0.064 -48.104 0.000 -3.196 -2.945 ## NdCgCnt (a_m2) 2.000 0.044 45.075 0.000 1.913 2.087 ## CBTDmmy (a_m3) -5.060 0.064 -79.562 0.000 -5.185 -4.936 ## Depression ~ ## CBTDmmy (b1) -1.240 0.137 -9.055 0.000 -1.509 -0.972 ## NdCgCnt (b2) 0.141 0.061 2.292 0.022 0.020 0.261 ## CBTxNCC (b3) -0.109 0.092 -1.180 0.238 -0.290 0.072 ## NgThght (bM) 1.582 0.025 62.899 0.000 1.533 1.631 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## CBTxNeedCogCont ~~ ## NeedCogCont 0.480 0.027 18.055 0.000 0.428 0.532 ## CBTDummy -0.008 0.011 -0.764 0.445 -0.030 0.013 ## NeedCogCont ~~ ## CBTDummy -0.020 0.016 -1.249 0.212 -0.051 0.011 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .NegThoughts 0.523 0.045 11.618 0.000 0.434 0.611 ## .Depression 2.104 0.038 55.199 0.000 2.029 2.179 ## CBTxNeedCogCnt -0.017 0.022 -0.764 0.445 -0.060 0.026 ## NeedCogCont 0.006 0.032 0.188 0.851 -0.056 0.068 ## CBTDummy 0.500 0.016 31.623 0.000 0.469 0.531 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .NegThoughts 1.010 0.045 22.361 0.000 0.921 1.098 ## .Depression 0.639 0.029 22.361 0.000 0.583 0.695 ## CBTxNeedCogCnt 0.480 0.021 22.361 0.000 0.438 0.522 ## NeedCogCont 0.994 0.044 22.361 0.000 0.907 1.081 ## CBTDummy 0.250 0.011 22.361 0.000 0.228 0.272 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## MedMod_ab -4.858 0.127 -38.211 0.000 -5.107 -4.609 ## TotalMod -4.967 0.113 -43.943 0.000 -5.188 -4.745 Are we done? 6.1.4 Step 4: Bootstrap Version We need to request Bootstrap because this involves testing a mediation effect MedMod_ab. Remember to set a seed: set.seed(2022) ex1Boot &lt;- sem(model = ex1MedModerationBasic, data = cbtData, fixed.x = FALSE, meanstructure = TRUE, se = &quot;bootstrap&quot;, bootstrap = 1000) This requires the full dataset - need more than the covariance matrix. se = “bootstrap” requests bootstrap standard errors. bootstrap = 1000 requests 1000 bootstrap samples. Request BC confidence interval: parameterEstimates(ex1Boot, level = 0.95, boot.ci.type=&quot;bca.simple&quot;, standardized = TRUE) ## lhs op rhs label est se z pvalue ci.lower ci.upper std.lv std.all std.nox ## 1 NegThoughts ~ CBTxNeedCogCont a_m1 -3.071 0.067 -45.675 0.000 -3.195 -2.922 -3.071 -0.668 -0.964 ## 2 NegThoughts ~ NeedCogCont a_m2 2.000 0.048 42.047 0.000 1.902 2.093 2.000 0.626 0.628 ## 3 NegThoughts ~ CBTDummy a_m3 -5.060 0.064 -78.764 0.000 -5.183 -4.930 -5.060 -0.794 -1.588 ## 4 Depression ~ CBTDummy b1 -1.240 0.133 -9.337 0.000 -1.493 -0.979 -1.240 -0.110 -0.220 ## 5 Depression ~ NeedCogCont b2 0.141 0.059 2.379 0.017 0.020 0.256 0.141 0.025 0.025 ## 6 Depression ~ CBTxNeedCogCont b3 -0.109 0.088 -1.232 0.218 -0.270 0.071 -0.109 -0.013 -0.019 ## 7 Depression ~ NegThoughts bM 1.582 0.024 65.833 0.000 1.539 1.632 1.582 0.892 0.892 ## 8 NegThoughts ~~ NegThoughts 1.010 0.042 23.885 0.000 0.937 1.109 1.010 0.099 0.099 ## 9 Depression ~~ Depression 0.639 0.030 21.077 0.000 0.585 0.708 0.639 0.020 0.020 ## 10 CBTxNeedCogCont ~~ CBTxNeedCogCont 0.480 0.034 14.292 0.000 0.411 0.545 0.480 1.000 0.480 ## 11 CBTxNeedCogCont ~~ NeedCogCont 0.480 0.034 14.314 0.000 0.412 0.545 0.480 0.695 0.480 ## 12 CBTxNeedCogCont ~~ CBTDummy -0.008 0.011 -0.781 0.435 -0.030 0.013 -0.008 -0.024 -0.008 ## 13 NeedCogCont ~~ NeedCogCont 0.994 0.044 22.749 0.000 0.909 1.078 0.994 1.000 0.994 ## 14 NeedCogCont ~~ CBTDummy -0.020 0.016 -1.258 0.208 -0.051 0.013 -0.020 -0.040 -0.020 ## 15 CBTDummy ~~ CBTDummy 0.250 0.000 619.383 0.000 0.250 0.250 0.250 1.000 0.250 ## 16 NegThoughts ~1 0.523 0.046 11.401 0.000 0.436 0.616 0.523 0.164 0.164 ## 17 Depression ~1 2.104 0.038 54.754 0.000 2.029 2.177 2.104 0.373 0.373 ## 18 CBTxNeedCogCont ~1 -0.017 0.021 -0.781 0.435 -0.059 0.026 -0.017 -0.024 -0.017 ## 19 NeedCogCont ~1 0.006 0.031 0.190 0.849 -0.062 0.063 0.006 0.006 0.006 ## 20 CBTDummy ~1 0.500 0.016 30.669 0.000 0.466 0.528 0.500 1.000 0.500 ## 21 MedMod_ab := a_m1*bM MedMod_ab -4.858 0.125 -38.863 0.000 -5.089 -4.602 -4.858 -0.596 -0.860 ## 22 TotalMod := MedMod_ab+b3 TotalMod -4.967 0.118 -42.112 0.000 -5.195 -4.720 -4.967 -0.609 -0.879 Warning message: In norm.inter(t, adj.alpha) : extreme order statistics used as endpoints https://rcompanion.org/handbook/E_04.html The BCa (bias corrected, accelerated) is often cited as the best for theoretical reasons. The percentile method is also cited as typically good. However, if you get the “extreme order statistics used as endpoints” warning message, use a different test. For small data sets, the interval from BCa may be wider than for some other methods. 6.1.5 Step 5: Effect size measures Measure 1: Completely Standardized Indirect Effect (CSIE) beta_a_m1 = -0.668 beta_bM = 0.892 es1 = beta_a_m1*beta_bM es1 ## [1] -0.595856 According to Cohen, .01-.09 is small, .10-.25 is medium, and .25 + is large This is a large mediation effect Measure 2: Use unstandardized parameter estimates: TotalMod = -4.967 MedMod_ab = -4.858 prop = MedMod_ab/TotalMod #97.8% prop ## [1] 0.9780552 b3 = -0.109 # pvalue=0.218 # nonsig Mediated% = indirect effect / total effect = ab / c Consistent mediation Complete mediation as the remaining direct effect is nonsig and prop &gt; 80% 6.2 PART 2: Moderated Mediation (Conditional Indirect effect) In this lab, we’ll test this first-stage moderated mediation model in which NeedCog moderates the CBT -&gt; NegThoughts path 6.2.1 Step 1: Product Term We already have the product term in the dataset: cbtData$CBTxNeedCogCont &lt;- cbtData$CBTDummy * cbtData$NeedCogCont If NeedCog moderates the NegThoughts -&gt; Depression path, then we center NegThoughts and create a product term between centered NegThoughts*NeedCogCont (making sense?) 6.2.2 Step 2: Write the syntax and Fit the model ex2ModMediationBasic &lt;- &quot; NegThoughts ~ a1*CBTDummy + a2*NeedCogCont + a3*CBTxNeedCogCont Depression ~ b*NegThoughts + cprime*CBTDummy &quot; We’ll need to define the Index of Moderated Mediation in the syntax: ex2ModMediation &lt;- &quot; #Regressions NegThoughts ~ a1*CBTDummy + a2*NeedCogCont + a3*CBTxNeedCogCont Depression ~ b*NegThoughts + cprime*CBTDummy #Index of Moderated Mediation IndexOfModMed := a3*b &quot; 6.2.3 Step 3: Bootstrap Version Since this model involves tests of indirect effects let’s jump to the bootstrap test: set.seed(2022) ex2Boot &lt;- sem(model = ex2ModMediation, data = cbtData, fixed.x = FALSE, meanstructure = TRUE, se = &quot;bootstrap&quot;, bootstrap = 1000) You can further request a BC (bias-corrected) by adding an argument boot.ci.type = “bca.simple” to parameterEstimates(): parameterEstimates(ex2Boot, level = 0.95, boot.ci.type=&quot;bca.simple&quot;, standardized = TRUE) ## lhs op rhs label est se z pvalue ci.lower ci.upper std.lv std.all std.nox ## 1 NegThoughts ~ CBTDummy a1 -5.060 0.064 -78.764 0.000 -5.183 -4.930 -5.060 -0.794 -1.588 ## 2 NegThoughts ~ NeedCogCont a2 2.000 0.048 42.047 0.000 1.902 2.093 2.000 0.626 0.628 ## 3 NegThoughts ~ CBTxNeedCogCont a3 -3.071 0.067 -45.675 0.000 -3.195 -2.922 -3.071 -0.668 -0.964 ## 4 Depression ~ NegThoughts b 1.617 0.013 124.544 0.000 1.592 1.642 1.617 0.912 0.912 ## 5 Depression ~ CBTDummy cprime -1.066 0.084 -12.758 0.000 -1.216 -0.899 -1.066 -0.094 -0.189 ## 6 NegThoughts ~~ NegThoughts 1.010 0.042 23.885 0.000 0.937 1.109 1.010 0.099 0.099 ## 7 Depression ~~ Depression 0.645 0.031 21.047 0.000 0.592 0.714 0.645 0.020 0.020 ## 8 CBTDummy ~~ CBTDummy 0.250 0.000 619.383 0.000 0.250 0.250 0.250 1.000 0.250 ## 9 CBTDummy ~~ NeedCogCont -0.020 0.016 -1.258 0.208 -0.051 0.013 -0.020 -0.040 -0.020 ## 10 CBTDummy ~~ CBTxNeedCogCont -0.008 0.011 -0.781 0.435 -0.030 0.013 -0.008 -0.024 -0.008 ## 11 NeedCogCont ~~ NeedCogCont 0.994 0.044 22.749 0.000 0.909 1.078 0.994 1.000 0.994 ## 12 NeedCogCont ~~ CBTxNeedCogCont 0.480 0.034 14.314 0.000 0.412 0.545 0.480 0.695 0.480 ## 13 CBTxNeedCogCont ~~ CBTxNeedCogCont 0.480 0.034 14.292 0.000 0.411 0.545 0.480 1.000 0.480 ## 14 NegThoughts ~1 0.523 0.046 11.401 0.000 0.436 0.616 0.523 0.164 0.164 ## 15 Depression ~1 2.089 0.037 56.930 0.000 2.021 2.160 2.089 0.370 0.370 ## 16 CBTDummy ~1 0.500 0.016 30.669 0.000 0.468 0.531 0.500 1.000 0.500 ## 17 NeedCogCont ~1 0.006 0.031 0.190 0.849 -0.062 0.063 0.006 0.006 0.006 ## 18 CBTxNeedCogCont ~1 -0.017 0.021 -0.781 0.435 -0.059 0.026 -0.017 -0.024 -0.017 ## 19 IndexOfModMed := a3*b IndexOfModMed -4.967 0.114 -43.431 0.000 -5.180 -4.727 -4.967 -0.609 -0.879 Defined Parameters: Estimate Std.Err ci.lower ci.upper std.all IndexOfModMed -4.967 0.114 -5.180 -4.727 -0.609 NeedCogCont significantly moderates CBT -&gt; NegThoughts -&gt; Depression indirect effect through moderating the first stage of the indirect effect Since we expect the effect of CBT on Depression to be negative (CBT reduces Depression) And IndexOfModMed is also negative We’ll say NeedCogCont strengthens the indirect effect of CBT on Depression through NegThoughts The higher need for cognition, the stronger the indirect effect, and the more effect mediated by NegThoughts 6.2.4 Step 4: Simple Slopes As a follow-up analysis to a significant moderation effect, we conduct simple slope anlaysis: Let’s use pick-a-point (Rogosa, 1980) and plot the indirect effects at designated levels of NeedCogCont: mean(cbtData$NeedCogCont) #0 ## [1] 0.005925852 sd(cbtData$NeedCogCont) # 1 ## [1] 0.9974319 Three representative levels: mean(cbtData$NeedCogCont) - sd(cbtData$NeedCogCont) # -1 ## [1] -0.991506 mean(cbtData$NeedCogCont) #0 ## [1] 0.005925852 mean(cbtData$NeedCogCont) + sd(cbtData$NeedCogCont) # 1 ## [1] 1.003358 let’s define the Conditional Indirect Effects in the syntax: ex3ModMediation &lt;- &quot; #Regressions NegThoughts ~ a1*CBTDummy + a2*NeedCogCont + a3*CBTxNeedCogCont Depression ~ b*NegThoughts + cprime*CBTDummy #Index of Moderated Mediation IndexOfModMed := a3*b #Simple Slopes aSSLow := a1+a3*(-1) aSSMean := a1+a3*0 aSSHigh := a1+a3*1 #Conditional Indirect Effects abLow := aSSLow*b abMean := aSSMean*b abHigh := aSSHigh*b &quot; set.seed(2022) ex3Boot &lt;- sem(model = ex3ModMediation, data = cbtData, se = &quot;bootstrap&quot;, bootstrap = 1000, fixed.x=FALSE, meanstructure = TRUE) parameterEstimates(ex3Boot, level = 0.95, boot.ci.type=&quot;bca.simple&quot;, standardized = TRUE) ## lhs op rhs label est se z pvalue ci.lower ci.upper std.lv std.all std.nox ## 1 NegThoughts ~ CBTDummy a1 -5.060 0.064 -78.764 0.000 -5.183 -4.930 -5.060 -0.794 -1.588 ## 2 NegThoughts ~ NeedCogCont a2 2.000 0.048 42.047 0.000 1.902 2.093 2.000 0.626 0.628 ## 3 NegThoughts ~ CBTxNeedCogCont a3 -3.071 0.067 -45.675 0.000 -3.195 -2.922 -3.071 -0.668 -0.964 ## 4 Depression ~ NegThoughts b 1.617 0.013 124.544 0.000 1.592 1.642 1.617 0.912 0.912 ## 5 Depression ~ CBTDummy cprime -1.066 0.084 -12.758 0.000 -1.216 -0.899 -1.066 -0.094 -0.189 ## 6 NegThoughts ~~ NegThoughts 1.010 0.042 23.885 0.000 0.937 1.109 1.010 0.099 0.099 ## 7 Depression ~~ Depression 0.645 0.031 21.047 0.000 0.592 0.714 0.645 0.020 0.020 ## 8 CBTDummy ~~ CBTDummy 0.250 0.000 619.383 0.000 0.250 0.250 0.250 1.000 0.250 ## 9 CBTDummy ~~ NeedCogCont -0.020 0.016 -1.258 0.208 -0.051 0.013 -0.020 -0.040 -0.020 ## 10 CBTDummy ~~ CBTxNeedCogCont -0.008 0.011 -0.781 0.435 -0.030 0.013 -0.008 -0.024 -0.008 ## 11 NeedCogCont ~~ NeedCogCont 0.994 0.044 22.749 0.000 0.909 1.078 0.994 1.000 0.994 ## 12 NeedCogCont ~~ CBTxNeedCogCont 0.480 0.034 14.314 0.000 0.412 0.545 0.480 0.695 0.480 ## 13 CBTxNeedCogCont ~~ CBTxNeedCogCont 0.480 0.034 14.292 0.000 0.411 0.545 0.480 1.000 0.480 ## 14 NegThoughts ~1 0.523 0.046 11.401 0.000 0.436 0.616 0.523 0.164 0.164 ## 15 Depression ~1 2.089 0.037 56.930 0.000 2.021 2.160 2.089 0.370 0.370 ## 16 CBTDummy ~1 0.500 0.016 30.669 0.000 0.468 0.531 0.500 1.000 0.500 ## 17 NeedCogCont ~1 0.006 0.031 0.190 0.849 -0.062 0.063 0.006 0.006 0.006 ## 18 CBTxNeedCogCont ~1 -0.017 0.021 -0.781 0.435 -0.059 0.026 -0.017 -0.024 -0.017 ## 19 IndexOfModMed := a3*b IndexOfModMed -4.967 0.114 -43.431 0.000 -5.180 -4.727 -4.967 -0.609 -0.879 ## 20 aSSLow := a1+a3*(-1) aSSLow -1.990 0.092 -21.679 0.000 -2.176 -1.813 -1.990 -0.126 -0.624 ## 21 aSSMean := a1+a3*0 aSSMean -5.060 0.064 -78.724 0.000 -5.183 -4.930 -5.060 -0.794 -1.588 ## 22 aSSHigh := a1+a3*1 aSSHigh -8.131 0.094 -86.243 0.000 -8.313 -7.942 -8.131 -1.462 -2.552 ## 23 abLow := aSSLow*b abLow -3.218 0.151 -21.338 0.000 -3.532 -2.941 -3.218 -0.115 -0.570 ## 24 abMean := aSSMean*b abMean -8.185 0.121 -67.625 0.000 -8.433 -7.944 -8.185 -0.725 -1.449 ## 25 abHigh := aSSHigh*b abHigh -13.152 0.181 -72.721 0.000 -13.513 -12.779 -13.152 -1.334 -2.329 Defined Parameters: Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper std.all IndexOfModMed -4.967 0.114 -43.431 0.000 -5.180 -4.727 -0.609 aSSLow -1.990 0.092 -21.679 0.000 -2.176 -1.813 -0.126 aSSMean -5.060 0.064 -78.724 0.000 -5.183 -4.930 -0.794 aSSHigh -8.131 0.094 -86.243 0.000 -8.313 -7.942 -1.462 abLow -3.218 0.151 -21.338 0.000 -3.532 -2.941 -0.115 abMean -8.185 0.121 -67.625 0.000 -8.433 -7.944 -0.725 abHigh -13.152 0.181 -72.721 0.000 -13.513 -12.779 -1.334 b 1.617 0.013 124.544 0.000 1.592 1.642 0.912 cprime -1.066 0.084 -12.758 0.000 -1.216 -0.899 -0.094 What does a1 tell you? What does a2 tell you? What does a3 tell you? What does IndexOfModMed tell you? What does aSSLow tell you? What does aSSMean tell you? What does aSSHigh tell you? What does b tell you? What does abLow tell you? What does abMean tell you? What does abHigh tell you? What does cprime tell you? the simple slopes of CBT -&gt; NegThoughts (a path) are all negative at three levels of the moderator the indirect effects of CBT -&gt; NegThoughts -&gt; Depression (ab) are all negative at three levels of the moderator 6.2.5 Step 5 JOHNSON-NEYMAN INTERVAL Although johnson_neyman() does not work on lavaan fitted object (yet), one can use a try-and-error approach to figure out the region of significance: First, obtain the minimum and maximum of the moderator NeedCogCont: min(cbtData$NeedCogCont) # -2.83 ## [1] -2.829197 max(cbtData$NeedCogCont) # 3.31 ## [1] 3.310095 ex4_JN &lt;- &quot; #Regressions NegThoughts ~ a1*CBTDummy + a2*NeedCogCont + a3*CBTxNeedCogCont Depression ~ b*NegThoughts + cprime*CBTDummy #Index of Moderated Mediation IndexOfModMed := a3*b #Simple Slopes aSSMin := a1+a3*(-2.83) aSSMin1 := a1+a3*(-1.75) aSSMin2 := a1+a3*(-1.74) aSSMin3 := a1+a3*(-1.58) aSSMin4 := a1+a3*(-1.57) aSSLow := a1+a3*(-1) aSSMean := a1+a3*0 aSSHigh := a1+a3*1 aSSMax := a1+a3*(3.31) #Conditional Indirect Effects abMin := (a1+a3*(-2.83))*b abMin1 := (a1+a3*(-1.75))*b # cutoff1 abMin2 := (a1+a3*(-1.74))*b abMin3 := (a1+a3*(-1.58))*b abMin4 := (a1+a3*(-1.57))*b # cutoff2 abLow := aSSLow*b abMean := aSSMean*b abHigh := aSSHigh*b abMax := (a1+a3*(3.31))*b &quot; set.seed(2022) ex4Boot &lt;- sem(model = ex4_JN, data = cbtData, se = &quot;bootstrap&quot;, bootstrap = 1000, fixed.x=FALSE, meanstructure = TRUE) parameterEstimates(ex4Boot, level = 0.95, boot.ci.type=&quot;bca.simple&quot;) ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 NegThoughts ~ CBTDummy a1 -5.060 0.064 -78.764 0.000 -5.183 -4.930 ## 2 NegThoughts ~ NeedCogCont a2 2.000 0.048 42.047 0.000 1.902 2.093 ## 3 NegThoughts ~ CBTxNeedCogCont a3 -3.071 0.067 -45.675 0.000 -3.195 -2.922 ## 4 Depression ~ NegThoughts b 1.617 0.013 124.544 0.000 1.592 1.642 ## 5 Depression ~ CBTDummy cprime -1.066 0.084 -12.758 0.000 -1.216 -0.899 ## 6 NegThoughts ~~ NegThoughts 1.010 0.042 23.885 0.000 0.937 1.109 ## 7 Depression ~~ Depression 0.645 0.031 21.047 0.000 0.592 0.714 ## 8 CBTDummy ~~ CBTDummy 0.250 0.000 619.383 0.000 0.250 0.250 ## 9 CBTDummy ~~ NeedCogCont -0.020 0.016 -1.258 0.208 -0.051 0.013 ## 10 CBTDummy ~~ CBTxNeedCogCont -0.008 0.011 -0.781 0.435 -0.030 0.013 ## 11 NeedCogCont ~~ NeedCogCont 0.994 0.044 22.749 0.000 0.909 1.078 ## 12 NeedCogCont ~~ CBTxNeedCogCont 0.480 0.034 14.314 0.000 0.412 0.545 ## 13 CBTxNeedCogCont ~~ CBTxNeedCogCont 0.480 0.034 14.292 0.000 0.411 0.545 ## 14 NegThoughts ~1 0.523 0.046 11.401 0.000 0.436 0.616 ## 15 Depression ~1 2.089 0.037 56.930 0.000 2.021 2.160 ## 16 CBTDummy ~1 0.500 0.016 30.669 0.000 0.468 0.531 ## 17 NeedCogCont ~1 0.006 0.031 0.190 0.849 -0.062 0.063 ## 18 CBTxNeedCogCont ~1 -0.017 0.021 -0.781 0.435 -0.059 0.026 ## 19 IndexOfModMed := a3*b IndexOfModMed -4.967 0.114 -43.431 0.000 -5.180 -4.727 ## 20 aSSMin := a1+a3*(-2.83) aSSMin 3.630 0.199 18.215 0.000 3.204 4.011 ## 21 aSSMin1 := a1+a3*(-1.75) aSSMin1 0.313 0.133 2.363 0.018 0.029 0.562 ## 22 aSSMin2 := a1+a3*(-1.74) aSSMin2 0.283 0.132 2.141 0.032 0.000 0.530 ## 23 aSSMin3 := a1+a3*(-1.58) aSSMin3 -0.209 0.123 -1.701 0.089 -0.471 0.018 ## 24 aSSMin4 := a1+a3*(-1.57) aSSMin4 -0.239 0.122 -1.960 0.050 -0.500 -0.013 ## 25 aSSLow := a1+a3*(-1) aSSLow -1.990 0.092 -21.679 0.000 -2.176 -1.813 ## 26 aSSMean := a1+a3*0 aSSMean -5.060 0.064 -78.724 0.000 -5.183 -4.930 ## 27 aSSHigh := a1+a3*1 aSSHigh -8.131 0.094 -86.243 0.000 -8.313 -7.942 ## 28 aSSMax := a1+a3*(3.31) aSSMax -15.224 0.233 -65.232 0.000 -15.658 -14.751 ## 29 abMin := (a1+a3*(-2.83))*b abMin 5.871 0.324 18.092 0.000 5.185 6.487 ## 30 abMin1 := (a1+a3*(-1.75))*b abMin1 0.507 0.214 2.363 0.018 0.047 0.904 ## 31 abMin2 := (a1+a3*(-1.74))*b abMin2 0.457 0.213 2.141 0.032 -0.001 0.853 ## 32 abMin3 := (a1+a3*(-1.58))*b abMin3 -0.338 0.199 -1.700 0.089 -0.759 0.030 ## 33 abMin4 := (a1+a3*(-1.57))*b abMin4 -0.387 0.198 -1.959 0.050 -0.807 -0.020 ## 34 abLow := aSSLow*b abLow -3.218 0.151 -21.338 0.000 -3.532 -2.941 ## 35 abMean := aSSMean*b abMean -8.185 0.121 -67.625 0.000 -8.433 -7.944 ## 36 abHigh := aSSHigh*b abHigh -13.152 0.181 -72.721 0.000 -13.513 -12.779 ## 37 abMax := (a1+a3*(3.31))*b abMax -24.624 0.418 -58.964 0.000 -25.391 -23.738 So our regions of significance are: \\([-2.83, -1.75]\\): In which the ab are positive and significant. Participants with this level of NeegCog experienced elevated levels of depression due to CBT because CBT induces more negative thoughts among them. \\([-1.75, -1.57]\\): In which the ab are NOT significant. \\([-1.57, 3.31]\\): In which the ab are negative and significant. Participants with this level of NeegCog experienced reduced levels of depression due to CBT because CBT reduced negative thoughts for them. "],["week6_2-r-lab-on-disaster-dataset-chapman-and-lickel-2016.html", "Chapter 7 Week6_2: R Lab on Disaster Dataset (Chapman and Lickel, 2016) 7.1 Data Prep 7.2 Model 1: Simple Linear Regression Model 7.3 Model 2: Simple Mediation Model 7.4 Model 3: Simple Moderation Model 7.5 Model 4a: Moderated Mediation Model - Path a only 7.6 Model 4b: Moderated Mediation Model - Path b only 7.7 Model 4c: Moderation &amp; Mediation Model - Path cprime only 7.8 Model 4d: Moderated Mediation Model - Path a and cprime 7.9 Model 4e: Moderated Mediation Model - Path b and cprime 7.10 Model 4f: Moderated Mediation Model - Path a and b 7.11 Model 4g: Moderated Mediation Model - Path a, b, and cprime 7.12 Conclusions", " Chapter 7 Week6_2: R Lab on Disaster Dataset (Chapman and Lickel, 2016) 7.1 Data Prep The following example data are from Chapman and Lickel (2016) Also example data in Chapter 12 of Hayes (2017) Simply load the .rda into R: load(&quot;disaster.rda&quot;) head(disaster) ## id frame donate justify skeptic ## 1 1 1 5.6 2.95 1.8 ## 2 2 1 4.2 2.85 5.2 ## 3 3 1 4.2 3.00 3.2 ## 4 4 1 4.6 3.30 1.0 ## 5 5 1 3.0 5.00 7.6 ## 6 6 0 5.0 3.20 4.2 str(disaster) ## &#39;data.frame&#39;: 211 obs. of 5 variables: ## $ id : num 1 2 3 4 5 6 7 8 9 10 ... ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.0&quot; ## ..- attr(*, &quot;display_width&quot;)= int 6 ## $ frame : num 1 1 1 1 1 0 0 1 0 0 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Experimental condition&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.2&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:2] 0 1 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;naturally caused disaster&quot; &quot;climate change caused disaster&quot; ## $ donate : num 5.6 4.2 4.2 4.6 3 5 4.8 6 4.2 4.4 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Positive attitudes toward donating&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.2&quot; ## ..- attr(*, &quot;display_width&quot;)= int 9 ## $ justify: num 2.95 2.85 3 3.3 5 3.2 2.9 1.4 3.25 3.55 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Negative justifications&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.2&quot; ## ..- attr(*, &quot;display_width&quot;)= int 10 ## $ skeptic: num 1.8 5.2 3.2 1 7.6 4.2 4.2 1.2 1.8 8.8 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Climate change skepticism&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.2&quot; If you are able to install package processR, you can also view its help page: install.packages(&quot;processR&quot;) # If error message persists, change the repository to CRAN: install.packages(&quot;processR&quot;, repos=&quot;https://cran.rstudio.com/&quot;) library(processR) data(disaster) # take a look at the dataset: ?disaster library(processR) You probably have to go to https://www.xquartz.org/ to download and install X11, which is a server required by many R packages, including processR. Disaster is a data.frame with 211 obs. of 5 variables: id frame: Experimental condition. 0 = naturally caused disaster, 1 = climate change caused disaster donate: Positive attitudes toward donating justify: Negative justifications skeptic: Climate change skepticism 7.1.1 Scatterplot Matrix Before we build linear models, we should plot the relationship between pairs of variables: library(PerformanceAnalytics) chart.Correlation(disaster[,-1]) 7.1.2 p-value or bootstrapped confidence interval? For models that involve mediation effects, we prefer to use bootstrap confidence intervals over p-values for evaluating the significance of parameter estimates. That is, in the parameter table generated by parameterEstimates() function: A coefficient is considered significant when the interval [ci.lower, ci.upper] does not include zero; A coefficient is considered insignificant when the interval [ci.lower, ci.upper] includes zero. In most cases, bootstrap confidence intervals and p-values yield the same conclusions regarding the significance of parameter estimates. If not, bootstrap confidence intervals are used to make the final call. In this document, all bootstrap confidence intervals and p-values yield the same conclusions regarding significances, so I’ll only refer to p-values for the readability of the analyses. 7.2 Model 1: Simple Linear Regression Model With processR, you can draw concept diagram and statistical diagram of mediation and moderation models quite easily. For example: labels=list(X=&quot;frame&quot;,M=&quot;justify&quot;,Y=&quot;donate&quot;,W=&quot;skeptic&quot;) par(mfrow = c(2,1), mar=c(0,0,0,0), oma=c(0,0,0,0)) pmacroModel(0,labels=labels) statisticalDiagram(0, labels=labels) return the diagrams of a simple linear regression model. For Model 1, let’s run a simple linear regression using lm() to estimate the total effect of frame on willingness to donate: lm1 = lm(donate ~ frame , data = disaster) summary(lm1)[[4]] ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.56363636 0.1258337 36.2672028 3.955220e-92 ## frame 0.08388839 0.1818769 0.4612372 6.451081e-01 Note that the [[4]] was added after summary(lm1) to request the coefficient table only. The total effect is c = 0.084 (p = 0.645), not significant. However, we learned in this class that absence of association between X and Y does NOT mean that X isn’t affecting Y (remember inconsistent mediation?). So let’s move on… 7.3 Model 2: Simple Mediation Model Q: If a disaster is framed as the result of a climate change (instead of a natural disaster), do you think it’s justified to withhold aid to the victims, and thus become less willing to donate? par(mfrow = c(1,2), mar=c(0,0,0,0), oma=c(0,0,0,0)) pmacroModel(4,labels=labels) statisticalDiagram(4, labels=labels) Load the lavaan package: library(lavaan) and test the mediation effect (ab) using bootstrap: lm2.syntax &lt;- &#39; donate ~ b*justify + cprime*frame justify ~ a*frame # Define new parameters #The := operator in lavaan defines new parameters. ab:= a*b c:= a*b + cprime &#39; set.seed(2022) lm2.bfit = sem(lm2.syntax, data = disaster, se = &quot;bootstrap&quot;, bootstrap = 1000, fixed.x=FALSE, meanstructure = TRUE) summary(lm2.bfit, ci = T) ## lavaan 0.6-10 ended normally after 19 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 9 ## ## Number of observations 211 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Bootstrap ## Number of requested bootstrap draws 1000 ## Number of successful bootstrap draws 1000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## donate ~ ## justify (b) -0.953 0.065 -14.641 0.000 -1.078 -0.823 ## frame (cprm) 0.212 0.139 1.526 0.127 -0.065 0.492 ## justify ~ ## frame (a) 0.134 0.131 1.023 0.306 -0.133 0.393 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .donate 7.235 0.199 36.380 0.000 6.810 7.595 ## .justify 2.802 0.083 33.785 0.000 2.644 2.967 ## frame 0.479 0.036 13.458 0.000 0.403 0.545 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .donate 0.948 0.118 8.061 0.000 0.712 1.171 ## .justify 0.856 0.097 8.866 0.000 0.683 1.071 ## frame 0.250 0.003 97.289 0.000 0.241 0.250 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## ab -0.128 0.125 -1.022 0.307 -0.378 0.125 ## c 0.084 0.188 0.447 0.655 -0.253 0.476 parameterEstimates(lm2.bfit, boot.ci.type = &quot;bca.simple&quot;, standardized = T) ## lhs op rhs label est se z pvalue ci.lower ci.upper std.lv std.all std.nox ## 1 donate ~ justify b -0.953 0.065 -14.641 0.000 -1.083 -0.828 -0.953 -0.673 -0.673 ## 2 donate ~ frame cprime 0.212 0.139 1.526 0.127 -0.062 0.503 0.212 0.081 0.161 ## 3 justify ~ frame a 0.134 0.131 1.023 0.306 -0.147 0.380 0.134 0.072 0.145 ## 4 donate ~~ donate 0.948 0.118 8.061 0.000 0.735 1.214 0.948 0.549 0.549 ## 5 justify ~~ justify 0.856 0.097 8.866 0.000 0.699 1.096 0.856 0.995 0.995 ## 6 frame ~~ frame 0.250 0.003 97.289 0.000 0.244 0.250 0.250 1.000 0.250 ## 7 donate ~1 7.235 0.199 36.380 0.000 6.808 7.590 7.235 5.506 5.506 ## 8 justify ~1 2.802 0.083 33.785 0.000 2.649 2.976 2.802 3.022 3.022 ## 9 frame ~1 0.479 0.036 13.458 0.000 0.390 0.536 0.479 0.958 0.479 ## 10 ab := a*b ab -0.128 0.125 -1.022 0.307 -0.362 0.144 -0.128 -0.049 -0.097 ## 11 c := a*b+cprime c 0.084 0.188 0.447 0.655 -0.233 0.524 0.084 0.032 0.064 From the coefficient table, we can see: a path: a = 0.134 (p = 0.306) b path: b = -0.953 (p = 0.000) indirect effect: ab = -0.128 (p = 0.307) direct effect: cprime = 0.212 (p = 0.127) total effect: c = ab + cprime = 0.084 (p = 0.655) Except for b path, all effects above are not significant. This tells us: The framing of the disaster did not significantly change people’s beliefs about whether providing aid to the victims is justified (a path) Justification for withholding aid did make participants less willing to donate (b path) However, the indirect effect ab was not significant, meaning justification for withholding did not explain the relationship between frame and willingness to donate Let’s switch to moderation model: 7.4 Model 3: Simple Moderation Model Skepticism of climate change cannot be changed by the frame, so skeptic is a moderator, not a mediator, it does not stand in the middle of the pathway par(mfrow = c(1,2), mar=c(0,0,0,0), oma=c(0,0,0,0)) pmacroModel(1,labels=labels) statisticalDiagram(1, labels=labels) Since the moderator skeptic is a continuous measure, we need to mean center it first: disaster$skep_raw = disaster$skeptic disaster$skeptic = scale(disaster$skep_raw, center = TRUE, scale = FALSE) disaster$skep_sd = scale(disaster$skep_raw, center = TRUE, scale = TRUE) disaster$skepxframe = disaster$skeptic * disaster$frame .. and manually create an interaction term by multiplying skeptic and frame: disaster$skepxframe = disaster$skeptic * disaster$frame Let’s examine their means and standard deviations: round(apply(disaster, 2, mean), 2) ## id frame donate justify skeptic skep_raw skep_sd skepxframe ## 106.00 0.48 4.60 2.87 0.00 3.38 0.00 0.02 round(apply(disaster, 2, sd), 2) ## id frame donate justify skeptic skep_raw skep_sd skepxframe ## 61.05 0.50 1.32 0.93 2.03 2.03 1.00 1.40 Let’s write the syntax for the simple moderation model: lm3.syntax &lt;- &#39; #Regression with interaction donate ~ b1*skeptic + b2*frame + b3*skepxframe &#39; lm3.fit = sem(lm3.syntax, data = disaster, fixed.x=FALSE, meanstructure = TRUE) parameterEstimates(lm3.fit, ci = T) ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 donate ~ skeptic b1 -0.140 0.057 -2.433 0.015 -0.252 -0.027 ## 2 donate ~ frame b2 0.103 0.169 0.609 0.542 -0.228 0.433 ## 3 donate ~ skepxframe b3 -0.171 0.083 -2.054 0.040 -0.334 -0.008 ## 4 donate ~~ donate 1.495 0.146 10.271 0.000 1.210 1.780 ## 5 skeptic ~~ skeptic 4.113 0.400 10.271 0.000 3.328 4.897 ## 6 skeptic ~~ frame 0.021 0.070 0.294 0.769 -0.116 0.157 ## 7 skeptic ~~ skepxframe 1.957 0.237 8.249 0.000 1.492 2.423 ## 8 frame ~~ frame 0.250 0.024 10.271 0.000 0.202 0.297 ## 9 frame ~~ skepxframe 0.011 0.048 0.222 0.824 -0.084 0.105 ## 10 skepxframe ~~ skepxframe 1.957 0.191 10.271 0.000 1.584 2.331 ## 11 donate ~1 4.558 0.117 39.092 0.000 4.330 4.787 ## 12 skeptic ~1 0.000 0.140 0.000 1.000 -0.274 0.274 ## 13 frame ~1 0.479 0.034 13.919 0.000 0.411 0.546 ## 14 skepxframe ~1 0.021 0.096 0.213 0.831 -0.168 0.209 b3 = -0.171, p = 0.040 So this moderator, skeptic, is a significant moderator of the frame-donate path, that is, skepticism of climate change could change the effect of framing on willingness to donate. 7.4.1 JOHNSON-NEYMAN INTERVAL interactionModel2 &lt;- lm(donate ~ skeptic*frame, disaster) summary(interactionModel2) ## ## Call: ## lm(formula = donate ~ skeptic * frame, data = disaster) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.8341 -0.7077 0.1659 0.9101 2.6682 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.55815 0.11772 38.719 &lt;2e-16 *** ## skeptic -0.13953 0.05790 -2.410 0.0168 * ## frame 0.10266 0.17016 0.603 0.5469 ## skeptic:frame -0.17071 0.08393 -2.034 0.0432 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.234 on 207 degrees of freedom ## Multiple R-squared: 0.1343, Adjusted R-squared: 0.1218 ## F-statistic: 10.71 on 3 and 207 DF, p-value: 1.424e-06 library(interactions) interactions::johnson_neyman(interactionModel2, pred = &quot;frame&quot;, modx = &quot;skeptic&quot;, alpha = 0.05) ## JOHNSON-NEYMAN INTERVAL ## ## When skeptic is OUTSIDE the interval [-2.59, 22.34], the slope of frame is p &lt; .05. ## ## Note: The range of observed values of skeptic is [-2.38, 5.62] As can be seen, it appears that among those low in climate change skepticism (lower than -2.59), framing the drought as caused by climate change produced a greater willingness to donate (simple slopes were significantly positive) compared to when climate change was not described as the cause. Among climate change skeptics (i.e., those high on the skepticism scale), the willingness to donate to the victims were not affected the framing of the problem (simple slopes were not significantly different from 0). Next, let’s test those Moderated Mediation Models one by one. 7.5 Model 4a: Moderated Mediation Model - Path a only par(mfrow = c(1,2), mar=c(0,0,0,0), oma=c(0,0,0,0)) pmacroModel(7,labels=labels) statisticalDiagram(7, labels=labels) Since the frame-justify (a path) is hypothesized to be moderated by skeptic, the simple slope of justify on frame is a function of skeptic, that is, a1+a3*skeptic The indirect effect through justify also depends on skeptic, calculated as b*(a1+a3*skeptic) Since skeptic is a continuous variable, we will pick three values from it. The chapter in Hayes (2017) picked the 16th, 50th, and 84th percentiles of the distribution using the quantile() function: quantile(disaster$skeptic, probs = c(0.16, 0.5, 0.84)) ## 16% 50% 84% ## -1.7779621 -0.5779621 1.8220379 which are: low: -1.78 median: -0.58 mean: 0 (why) high: 1.82 We’ll also define the index of moderated mediation to be: a3*b (refer to slides of week6_1) Let’s write the syntax for the moderated mediation model: lm4a.syntax &lt;- &#39; donate ~ b*justify + cprime*frame justify ~ a1*frame + a2*skeptic + a3*skepxframe # Define simple slopes and conditional indirect effects using := # index of moderated mediation IndMedMod:= a3*b # simple slope of justify on frame is a1+a3*skeptic aLow: = a1+a3*(-1.78) aMedian: = a1+a3*(-0.58) aMean: = a1+a3*(0) aHigh: = a1+a3*1.82 # conditional indirect effects is b*(a1+a3*skeptic) abLow: = b*aLow abMedian: = b*aMedian abMean: = b*aMean abHigh: = b*aHigh &#39; set.seed(2022) lm4a.fit = sem(lm4a.syntax, data = disaster, se = &quot;bootstrap&quot;, bootstrap = 1000, fixed.x=FALSE, meanstructure = TRUE) parameterEstimates(lm4a.fit, level = 0.95, boot.ci.type = &quot;bca.simple&quot;) ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 donate ~ justify b -0.953 0.065 -14.641 0.000 -1.083 -0.828 ## 2 donate ~ frame cprime 0.212 0.139 1.526 0.127 -0.062 0.503 ## 3 justify ~ frame a1 0.117 0.114 1.023 0.306 -0.113 0.324 ## 4 justify ~ skeptic a2 0.105 0.043 2.459 0.014 0.020 0.194 ## 5 justify ~ skepxframe a3 0.201 0.063 3.201 0.001 0.077 0.326 ## 6 donate ~~ donate 0.948 0.118 8.061 0.000 0.735 1.214 ## 7 justify ~~ justify 0.648 0.066 9.767 0.000 0.539 0.809 ## 8 frame ~~ frame 0.250 0.003 97.289 0.000 0.242 0.250 ## 9 frame ~~ skeptic 0.021 0.074 0.277 0.782 -0.125 0.168 ## 10 frame ~~ skepxframe 0.011 0.052 0.207 0.836 -0.089 0.121 ## 11 skeptic ~~ skeptic 4.113 0.498 8.257 0.000 3.211 5.198 ## 12 skeptic ~~ skepxframe 1.957 0.349 5.606 0.000 1.333 2.721 ## 13 skepxframe ~~ skepxframe 1.957 0.348 5.618 0.000 1.328 2.708 ## 14 donate ~1 7.235 0.199 36.380 0.000 6.808 7.590 ## 15 justify ~1 2.806 0.079 35.374 0.000 2.659 2.976 ## 16 frame ~1 0.479 0.036 13.458 0.000 0.396 0.540 ## 17 skeptic ~1 0.000 0.142 0.000 1.000 -0.274 0.286 ## 18 skepxframe ~1 0.021 0.099 0.206 0.837 -0.173 0.223 ## 19 IndMedMod := a3*b IndMedMod -0.192 0.062 -3.073 0.002 -0.327 -0.076 ## 20 aLow := a1+a3*(-1.78) aLow -0.241 0.154 -1.568 0.117 -0.563 0.045 ## 21 aMedian := a1+a3*(-0.58) aMedian 0.000 0.117 0.004 0.997 -0.240 0.209 ## 22 aMean := a1+a3*(0) aMean 0.117 0.115 1.023 0.307 -0.113 0.324 ## 23 aHigh := a1+a3*1.82 aHigh 0.483 0.168 2.874 0.004 0.138 0.829 ## 24 abLow := b*aLow abLow 0.230 0.149 1.546 0.122 -0.044 0.540 ## 25 abMedian := b*aMedian abMedian 0.000 0.112 -0.004 0.997 -0.201 0.236 ## 26 abMean := b*aMean abMean -0.112 0.109 -1.027 0.305 -0.306 0.103 ## 27 abHigh := b*aHigh abHigh -0.461 0.164 -2.811 0.005 -0.796 -0.130 So we have: interaction of skepxframe on justify: a3 = 0.201 (p = 0.001) The overall effect of frame on justify is significantly moderated by skeptic. That is, how the disaster is framed has a differential effect for people who differ in their climate change skepticism on their beliefs that if it was justified to withhold aid to the victims. Let’s look at the simple slopes. aLow = -0.241 (p = 0.117) aMedian = 0 (p = 0.997) aMean = 0.117 (p = 0.307) aHigh = 0.483 (p = 0.004) Moreover, the simple slope aHigh is positive and aLow is negative. That is, when told that the disaster is the result of climate change instead of natural disaster (changing frame from 0 to 1), those who doubt climate change (high on skepticism) think it’s justified to withhold the aid whereas those who do not doubt (low on skepticism) it think it’s not justified to withhold the aid. Okay! The a path is moderated. What about the mediation effect ab? IndexOfModMed = -0.192 (p = 0.002) IndexOfModMed is sig! Woo-hoo! Indirect effect is moderated, too! So what story does it tell you? abLow = 0.230 (p = 0.122) abMedian = 0.000 (p = 0.997) abMean = -0.112 (p = 0.305) abHigh = -0.461 (p = 0.005) Furthermore, abHigh is negative and abLow is positive, meaning that framing the disaster as caused by climate change leads to less donation for people who doubt climate change (high on skepticism) but it leads to more donation for people who believe it (low on skepticism). The reason for this differential effect is that those who doubt climate change tend to favor the idea of withholding the aid, thus leading to less donation. 7.6 Model 4b: Moderated Mediation Model - Path b only par(mfrow = c(1,2), mar=c(0,0,0,0), oma=c(0,0,0,0)) pmacroModel(14,labels=labels) statisticalDiagram(14, labels=labels) Since justify is also a continuous measure, we need to mean center it first: disaster$just_raw = disaster$justify disaster$justify = scale(disaster$just_raw, center = TRUE, scale = FALSE) disaster$just_sd = scale(disaster$just_raw, center = TRUE, scale = TRUE) and create an interaction term by multiplying skeptic by justify (note that this is a new interaction term!): disaster$skepxjusti = disaster$skeptic * disaster$justify Let’s examine their means and standard deviations: round(apply(disaster[,-1], 2, mean), 2) ## frame donate justify skeptic skep_raw skep_sd skepxframe just_raw just_sd skepxjusti ## 0.48 4.60 0.00 0.00 3.38 0.00 0.02 2.87 0.00 0.83 round(apply(disaster[,-1], 2, sd), 2) ## frame donate justify skeptic skep_raw skep_sd skepxframe just_raw just_sd skepxjusti ## 0.50 1.32 0.93 2.03 2.03 1.00 1.40 0.93 1.00 2.66 Since the b path is hypothesized moderated by skeptic, the simple slope of donation on justify (b path) depends on skeptic, the indirect effect through justify (ab) also depends on skeptic. We’ll define an index of moderated mediation to be: a*b3 (can you derive this?) Let’s write the syntax for the moderated mediation model: lm4b.syntax &lt;- &#39; donate ~ b1*justify + cprime*frame + b2*skeptic + b3*skepxjusti justify ~ a*frame # Define simple slopes and conditional indirect effects using := # index of moderated mediation IndMedMod:= a*b3 # simple slope of donate on justify is b1+b3*skeptic bLow: = b1+b3*(-1.78) bMedian: = b1+b3*(-0.58) bMean: = b1+b3*(0) bHigh: = b1+b3*1.82 # conditional indirect effects is a*(b1+b3*skeptic) abLow: = a*bLow abMedian: = a*bMedian abMean: = a*bMean abHigh: = a*bHigh &#39; set.seed(2022) lm4b.fit = sem(lm4b.syntax, data = disaster, se = &quot;bootstrap&quot;, bootstrap = 1000, fixed.x=FALSE, meanstructure = TRUE) parameterEstimates(lm4b.fit, level = 0.95, boot.ci.type = &quot;bca.simple&quot;) ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 donate ~ justify b1 -0.922 0.079 -11.611 0.000 -1.085 -0.772 ## 2 donate ~ frame cprime 0.204 0.140 1.455 0.146 -0.064 0.492 ## 3 donate ~ skeptic b2 -0.040 0.041 -0.968 0.333 -0.127 0.039 ## 4 donate ~ skepxjusti b3 0.008 0.026 0.327 0.743 -0.043 0.059 ## 5 justify ~ frame a 0.134 0.131 1.023 0.306 -0.147 0.380 ## 6 donate ~~ donate 0.943 0.116 8.128 0.000 0.736 1.200 ## 7 justify ~~ justify 0.856 0.097 8.866 0.000 0.699 1.096 ## 8 frame ~~ frame 0.250 0.003 97.289 0.000 0.244 0.250 ## 9 frame ~~ skeptic 0.021 0.074 0.277 0.782 -0.125 0.168 ## 10 frame ~~ skepxjusti 0.204 0.091 2.233 0.026 0.045 0.414 ## 11 skeptic ~~ skeptic 4.113 0.498 8.257 0.000 3.211 5.198 ## 12 skeptic ~~ skepxjusti 2.067 0.867 2.383 0.017 0.691 4.089 ## 13 skepxjusti ~~ skepxjusti 7.017 2.414 2.907 0.004 3.602 13.111 ## 14 donate ~1 4.499 0.110 40.942 0.000 4.288 4.708 ## 15 justify ~1 -0.064 0.083 -0.775 0.438 -0.218 0.109 ## 16 frame ~1 0.479 0.036 13.458 0.000 0.396 0.540 ## 17 skeptic ~1 0.000 0.142 0.000 1.000 -0.274 0.286 ## 18 skepxjusti ~1 0.828 0.179 4.632 0.000 0.526 1.218 ## 19 IndMedMod := a*b3 IndMedMod 0.001 0.005 0.243 0.808 -0.005 0.018 ## 20 bLow := b1+b3*(-1.78) bLow -0.937 0.103 -9.140 0.000 -1.138 -0.729 ## 21 bMedian := b1+b3*(-0.58) bMedian -0.927 0.085 -10.904 0.000 -1.096 -0.758 ## 22 bMean := b1+b3*(0) bMean -0.922 0.079 -11.605 0.000 -1.085 -0.772 ## 23 bHigh := b1+b3*1.82 bHigh -0.907 0.079 -11.427 0.000 -1.073 -0.752 ## 24 abLow := a*bLow abLow -0.126 0.123 -1.023 0.306 -0.369 0.130 ## 25 abMedian := a*bMedian abMedian -0.125 0.122 -1.023 0.306 -0.361 0.137 ## 26 abMean := a*bMean abMean -0.124 0.121 -1.022 0.307 -0.360 0.135 ## 27 abHigh := a*bHigh abHigh -0.122 0.120 -1.015 0.310 -0.351 0.130 So we have: b3 = 0.008 (p = 0.743) The effect of justify on donate is not moderated by skeptic. That is, justification for withholding aid always leads to less donation has a fixed effect on their willingness to donate (b path) regardless of their climate change skepticism. Let’s look at the simple slopes. bLow = -0.937 (p = 0.000) bMedian = -0.927 (p = 0.000) bMean = -0.922 (p = 0.000) bHigh = -0.907 (p = 0.000) which do not change much as their climate change skepticism change. Justification for withholding aid always leads to less donation. Skeptic is not an effective moderator. Let’s look at the indirect effects IndexOfModMed = 0.001 (p = 0.808) abLow = -0.126 (p = 0.306) abMedian = -0.125 (p = 0.306) abMean = -0.124 (p = 0.307) abHigh = -0.122 (p = 0.310) Similarly, skeptic is not a good moderator for the indirect effect given that IndexOfModMed is not significant and the indirect effects at high/low levels of skepticism do not differ much. 7.7 Model 4c: Moderation &amp; Mediation Model - Path cprime only par(mfrow = c(1,2), mar=c(0,0,0,0), oma=c(0,0,0,0)) pmacroModel(5,labels=labels) statisticalDiagram(5, labels=labels) Since there is NO indirect effect being moderated here, we’ll not define any index of moderation mediation. There is only one c3prime coefficient that quantifies the moderation effect of skeptic on frame-donation path. lm4c.syntax &lt;- &#39; donate ~ b*justify + c1prime*frame + skeptic + c3prime*skepxframe justify ~ a*frame # Define new parameters #The := operator in lavaan defines new parameters. # simple slope of donate on frame is c1prime+c3prime*skeptic cLow: = c1prime+c3prime*(-1.78) cMedian: = c1prime+c3prime*(-0.58) cMean: = c1prime+c3prime*(0) cHigh: = c1prime+c3prime*1.82 # mediation effect ab:= a*b &#39; set.seed(2022) lm4c.fit = sem(lm4c.syntax, data = disaster, se = &quot;bootstrap&quot;, bootstrap = 1000, fixed.x=FALSE, meanstructure = TRUE) parameterEstimates(lm4c.fit, level = 0.95, boot.ci.type = &quot;bca.simple&quot;) ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 donate ~ justify b -0.923 0.078 -11.781 0.000 -1.076 -0.764 ## 2 donate ~ frame c1prime 0.211 0.140 1.503 0.133 -0.064 0.496 ## 3 donate ~ skeptic -0.043 0.057 -0.742 0.458 -0.165 0.061 ## 4 donate ~ skepxframe c3prime 0.015 0.074 0.203 0.839 -0.120 0.171 ## 5 justify ~ frame a 0.134 0.131 1.023 0.306 -0.147 0.380 ## 6 donate ~~ donate 0.943 0.116 8.152 0.000 0.743 1.208 ## 7 justify ~~ justify 0.856 0.097 8.866 0.000 0.699 1.096 ## 8 frame ~~ frame 0.250 0.003 97.289 0.000 0.244 0.250 ## 9 frame ~~ skeptic 0.021 0.074 0.277 0.782 -0.125 0.168 ## 10 frame ~~ skepxframe 0.011 0.052 0.207 0.836 -0.089 0.121 ## 11 skeptic ~~ skeptic 4.113 0.498 8.257 0.000 3.211 5.198 ## 12 skeptic ~~ skepxframe 1.957 0.349 5.606 0.000 1.333 2.721 ## 13 skepxframe ~~ skepxframe 1.957 0.348 5.618 0.000 1.328 2.708 ## 14 donate ~1 4.503 0.109 41.164 0.000 4.283 4.711 ## 15 justify ~1 -0.064 0.083 -0.775 0.438 -0.218 0.109 ## 16 frame ~1 0.479 0.036 13.458 0.000 0.396 0.540 ## 17 skeptic ~1 0.000 0.142 0.000 1.000 -0.274 0.286 ## 18 skepxframe ~1 0.021 0.099 0.206 0.837 -0.173 0.223 ## 19 cLow := c1prime+c3prime*(-1.78) cLow 0.184 0.172 1.074 0.283 -0.150 0.527 ## 20 cMedian := c1prime+c3prime*(-0.58) cMedian 0.202 0.138 1.463 0.143 -0.062 0.482 ## 21 cMean := c1prime+c3prime*(0) cMean 0.211 0.140 1.503 0.133 -0.064 0.496 ## 22 cHigh := c1prime+c3prime*1.82 cHigh 0.238 0.213 1.118 0.264 -0.153 0.662 ## 23 ab := a*b ab -0.124 0.122 -1.020 0.308 -0.356 0.138 Since we have: c3prime = 0.015 (p = 0.839) Skeptic is not a moderator for this frame-donation path (cprime). The mediation effect: ab = -0.124 (p = 0.308) is not significant, just like in Model 2. 7.8 Model 4d: Moderated Mediation Model - Path a and cprime par(mfrow = c(1,2), mar=c(0,0,0,0), oma=c(0,0,0,0)) pmacroModel(8,labels=labels) statisticalDiagram(8, labels=labels) Model 4d is very similar to Model 4a, except that cprime path is also moderated. We’ll still define the index of moderated mediation to be: a3*b lm4d.syntax &lt;- &#39; donate ~ b*justify + c1prime*frame + c2prime*skeptic + c3prime*skepxframe justify ~ a1*frame + a2*skeptic + a3*skepxframe # Define simple slopes and conditional indirect effects using := # index of moderated mediation IndMedMod:= a3*b # simple slope of justify on frame is a1+a3*skeptic aLow: = a1+a3*(-1.78) aMedian: = a1+a3*(-0.58) aMean: = a1+a3*(0) aHigh: = a1+a3*1.82 # conditional indirect effects is b*(a1+a3*skeptic) abLow: = b*aLow abMedian: = b*aMedian abMean: = b*aMean abHigh: = b*aHigh &#39; set.seed(2022) lm4d.fit = sem(lm4d.syntax, data = disaster, se = &quot;bootstrap&quot;, bootstrap = 1000, fixed.x=FALSE, meanstructure = TRUE) parameterEstimates(lm4d.fit, level = 0.95, boot.ci.type = &quot;bca.simple&quot;) ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 donate ~ justify b -0.923 0.078 -11.781 0.000 -1.076 -0.764 ## 2 donate ~ frame c1prime 0.211 0.140 1.503 0.133 -0.064 0.496 ## 3 donate ~ skeptic c2prime -0.043 0.057 -0.742 0.458 -0.165 0.061 ## 4 donate ~ skepxframe c3prime 0.015 0.074 0.203 0.839 -0.120 0.171 ## 5 justify ~ frame a1 0.117 0.114 1.023 0.306 -0.113 0.324 ## 6 justify ~ skeptic a2 0.105 0.043 2.459 0.014 0.020 0.194 ## 7 justify ~ skepxframe a3 0.201 0.063 3.201 0.001 0.077 0.326 ## 8 donate ~~ donate 0.943 0.116 8.152 0.000 0.743 1.208 ## 9 justify ~~ justify 0.648 0.066 9.767 0.000 0.539 0.809 ## 10 frame ~~ frame 0.250 0.003 97.289 0.000 0.244 0.250 ## 11 frame ~~ skeptic 0.021 0.074 0.277 0.782 -0.125 0.168 ## 12 frame ~~ skepxframe 0.011 0.052 0.207 0.836 -0.089 0.121 ## 13 skeptic ~~ skeptic 4.113 0.498 8.257 0.000 3.211 5.198 ## 14 skeptic ~~ skepxframe 1.957 0.349 5.606 0.000 1.333 2.721 ## 15 skepxframe ~~ skepxframe 1.957 0.348 5.618 0.000 1.328 2.708 ## 16 donate ~1 4.503 0.109 41.164 0.000 4.283 4.711 ## 17 justify ~1 -0.060 0.079 -0.759 0.448 -0.208 0.109 ## 18 frame ~1 0.479 0.036 13.458 0.000 0.396 0.540 ## 19 skeptic ~1 0.000 0.142 0.000 1.000 -0.274 0.286 ## 20 skepxframe ~1 0.021 0.099 0.206 0.837 -0.173 0.223 ## 21 IndMedMod := a3*b IndMedMod -0.186 0.062 -2.979 0.003 -0.318 -0.071 ## 22 aLow := a1+a3*(-1.78) aLow -0.241 0.154 -1.568 0.117 -0.563 0.045 ## 23 aMedian := a1+a3*(-0.58) aMedian 0.000 0.117 0.004 0.997 -0.240 0.209 ## 24 aMean := a1+a3*(0) aMean 0.117 0.115 1.023 0.307 -0.113 0.324 ## 25 aHigh := a1+a3*1.82 aHigh 0.483 0.168 2.874 0.004 0.138 0.829 ## 26 abLow := b*aLow abLow 0.222 0.146 1.526 0.127 -0.036 0.532 ## 27 abMedian := b*aMedian abMedian 0.000 0.108 -0.004 0.997 -0.194 0.228 ## 28 abMean := b*aMean abMean -0.108 0.105 -1.026 0.305 -0.301 0.095 ## 29 abHigh := b*aHigh abHigh -0.446 0.162 -2.757 0.006 -0.785 -0.127 Here we have two interaction coefficients: a3 = 0.201 (p = 0.001) c3prime = 0.015 (p = 0.839) so that skeptic is a moderator for the frame-to-justify path (a path) but not a moderator for frame-to-donate path (cprime path). For the frame-to-justify path: aLow = -0.241 (p = 0.117) aMedian = 0 (p = 0.997) aMean = 0.117 (p = 0.307) aHigh = 0.483 (p = 0.004) which are exactly the same as those in Model 4a. The simple slope aHigh is positive and aLow is negative. Those who are high on climate change skepticism think it’s justified to withhold the aid whereas those who are low on the skepticism do not think it’s justified to withhold the aid. IndexOfModMed = -0.186 (p = 0.003) abLow = 0.222 (p = 0.127) abMedian = 0.000 (p = 0.997) abMean = -0.108 (p = 0.305) abHigh = -0.446 (p = 0.006) which are close to those in Model 4a. The indirect effect of donate on frame through justify (ab path) is moderated by skeptic (IndexOfModMed is sig!). Moreover, abHigh is negative and abLow is positive, meaning that framing the disaster as caused by climate change leads to less donation for people who doubt climate change but it leads to more donation for people who believes it because they do not think it’s justified to withhold the aid. 7.9 Model 4e: Moderated Mediation Model - Path b and cprime par(mfrow = c(1,2), mar=c(0,0,0,0), oma=c(0,0,0,0)) pmacroModel(15,labels=labels) statisticalDiagram(15, labels=labels) Model 4e is very similar to Model 4b except that cprime path is also moderated. We’ll still define the index of moderated mediation to be: a*b3 (actually, a*b2 in this diagram) Let’s write the syntax for the moderated mediation model: lm4e.syntax &lt;- &#39; donate ~ b1*justify + b2*skepxjusti + c1prime*frame + c2prime*skeptic + c3prime*skepxframe justify ~ a*frame # Define simple slopes and conditional indirect effects using := # index of moderated mediation IndMedMod:= a*b2 # simple slope of donate on justify is b1+b2*skeptic bLow: = b1+b2*(-1.78) bMedian: = b1+b2*(-0.58) bMean: = b1+b2*(0) bHigh: = b1+b2*1.82 # conditional indirect effects is a*(b1+b2*skeptic) abLow: = a*bLow abMedian: = a*bMedian abMean: = a*bMean abHigh: = a*bHigh &#39; set.seed(2022) lm4e.fit = sem(lm4e.syntax, data = disaster, se = &quot;bootstrap&quot;, bootstrap = 1000, fixed.x=FALSE, meanstructure = TRUE) parameterEstimates(lm4e.fit, level = 0.95, boot.ci.type = &quot;bca.simple&quot;) ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 donate ~ justify b1 -0.925 0.081 -11.473 0.000 -1.085 -0.762 ## 2 donate ~ skepxjusti b2 0.007 0.028 0.258 0.797 -0.047 0.063 ## 3 donate ~ frame c1prime 0.205 0.143 1.432 0.152 -0.075 0.500 ## 4 donate ~ skeptic c2prime -0.043 0.058 -0.749 0.454 -0.165 0.058 ## 5 donate ~ skepxframe c3prime 0.009 0.079 0.119 0.905 -0.139 0.171 ## 6 justify ~ frame a 0.134 0.131 1.023 0.306 -0.147 0.380 ## 7 donate ~~ donate 0.943 0.116 8.148 0.000 0.744 1.212 ## 8 justify ~~ justify 0.856 0.097 8.866 0.000 0.699 1.096 ## 9 skepxjusti ~~ skepxjusti 7.017 2.414 2.907 0.004 3.602 13.111 ## 10 skepxjusti ~~ frame 0.204 0.091 2.233 0.026 0.045 0.414 ## 11 skepxjusti ~~ skeptic 2.067 0.867 2.383 0.017 0.691 4.089 ## 12 skepxjusti ~~ skepxframe 1.828 0.784 2.332 0.020 0.600 3.681 ## 13 frame ~~ frame 0.250 0.003 97.289 0.000 0.244 0.250 ## 14 frame ~~ skeptic 0.021 0.074 0.277 0.782 -0.125 0.168 ## 15 frame ~~ skepxframe 0.011 0.052 0.207 0.836 -0.089 0.121 ## 16 skeptic ~~ skeptic 4.113 0.498 8.257 0.000 3.211 5.198 ## 17 skeptic ~~ skepxframe 1.957 0.349 5.606 0.000 1.333 2.721 ## 18 skepxframe ~~ skepxframe 1.957 0.348 5.618 0.000 1.328 2.708 ## 19 donate ~1 4.499 0.110 40.945 0.000 4.283 4.708 ## 20 justify ~1 -0.064 0.083 -0.775 0.438 -0.218 0.109 ## 21 skepxjusti ~1 0.828 0.179 4.632 0.000 0.526 1.218 ## 22 frame ~1 0.479 0.036 13.458 0.000 0.397 0.540 ## 23 skeptic ~1 0.000 0.142 0.000 1.000 -0.274 0.286 ## 24 skepxframe ~1 0.021 0.099 0.206 0.837 -0.173 0.223 ## 25 IndMedMod := a*b2 IndMedMod 0.001 0.005 0.189 0.850 -0.006 0.018 ## 26 bLow := b1+b2*(-1.78) bLow -0.937 0.103 -9.121 0.000 -1.142 -0.730 ## 27 bMedian := b1+b2*(-0.58) bMedian -0.929 0.085 -10.877 0.000 -1.096 -0.756 ## 28 bMean := b1+b2*(0) bMean -0.925 0.081 -11.467 0.000 -1.085 -0.762 ## 29 bHigh := b1+b2*1.82 bHigh -0.912 0.086 -10.622 0.000 -1.084 -0.741 ## 30 abLow := a*bLow abLow -0.126 0.123 -1.024 0.306 -0.372 0.129 ## 31 abMedian := a*bMedian abMedian -0.125 0.122 -1.022 0.307 -0.366 0.127 ## 32 abMean := a*bMean abMean -0.124 0.122 -1.021 0.307 -0.361 0.132 ## 33 abHigh := a*bHigh abHigh -0.122 0.121 -1.012 0.312 -0.349 0.133 Here we have two interaction coefficients: b2 = 0.007 (p = 0.258) c3prime = 0.009 (p = 0.905) so that skeptic is not a moderator for the b path nor for the cprime path. For the simple slopes of b path: bLow = -0.937 (p = 0.000) bMedian = -0.929 (p = 0.000) bMean = -0.925 (p = 0.000) bHigh = -0.912 (p = 0.000) which do not change much as their climate change skepticism change. Justification for withholding aid always leads to less donation. Skeptic is not an effective moderator. Let’s look at the indirect effects: IndexOfModMed = 0.001 (p = 0.850) abLow = -0.126 (p = 0.306) abMedian = -0.125 (p = 0.307) abMean = -0.124 (p = 0.307) abHigh = -0.122 (p = 0.312) Similarly, skeptic is not a good moderator for the indirect effect given that IndexOfModMed is not significant and the indirect effects at high/low levels of skepticism do not differ much. 7.10 Model 4f: Moderated Mediation Model - Path a and b par(mfrow = c(1,2), mar=c(0,0,0,0), oma=c(0,0,0,0)) pmacroModel(58,labels=labels) statisticalDiagram(58, labels=labels) When both a and b are moderated by the same moderator, we have: a = a1 + a3*skep b = b1 + b3*skep ab = (a1 + a3*skep)*(b1 + b3*skep) = a1*b1 + (a1*b3+a3*b1)*skep + a3*b3*skep^2 So the indirect effect does not depend on the moderator in a linear way. We don’t have a formal definition of index of moderated mediation in this scenario. If we are lucky, we might get both (a1*b3+a3*b1) and a3*b3 to be significant… Let’s write the syntax for the moderated mediation model: lm4f.syntax &lt;- &#39; donate ~ b1*justify + b2*skeptic + b3*skepxjusti + cprime*frame justify ~ a1*frame + a2*skeptic + a3*skepxframe # index of moderated mediation IndMedMod1:= a1*b3+a3*b1 IndMedMod2:= a3*b3 # simple slope of justify on frame is a1+a3*skeptic aLow: = a1+a3*(-1.78) aMedian: = a1+a3*(-0.58) aMean: = a1+a3*(0) aHigh: = a1+a3*1.82 # simple slope of donate on justify is b1+b3*skeptic bLow: = b1+b3*(-1.78) bMedian: = b1+b3*(-0.58) bMean: = b1+b3*(0) bHigh: = b1+b3*1.82 # conditional indirect effects is a*(b1+b3*skeptic) abLow: = aLow*bLow abMedian: = aMedian*bMedian abMean: = aMean*bMean abHigh: = aHigh*bHigh &#39; set.seed(2022) lm4f.fit = sem(lm4f.syntax, data = disaster, se = &quot;bootstrap&quot;, bootstrap = 1000, fixed.x=FALSE, meanstructure = TRUE) parameterEstimates(lm4f.fit, level = 0.95, boot.ci.type = &quot;bca.simple&quot;) ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 donate ~ justify b1 -0.922 0.079 -11.611 0.000 -1.085 -0.772 ## 2 donate ~ skeptic b2 -0.040 0.041 -0.968 0.333 -0.127 0.039 ## 3 donate ~ skepxjusti b3 0.008 0.026 0.327 0.743 -0.043 0.059 ## 4 donate ~ frame cprime 0.204 0.140 1.455 0.146 -0.064 0.492 ## 5 justify ~ frame a1 0.117 0.114 1.023 0.306 -0.113 0.324 ## 6 justify ~ skeptic a2 0.105 0.043 2.459 0.014 0.020 0.194 ## 7 justify ~ skepxframe a3 0.201 0.063 3.201 0.001 0.077 0.326 ## 8 donate ~~ donate 0.943 0.116 8.129 0.000 0.736 1.200 ## 9 justify ~~ justify 0.648 0.066 9.767 0.000 0.539 0.809 ## 10 skeptic ~~ skeptic 4.113 0.498 8.257 0.000 3.211 5.198 ## 11 skeptic ~~ skepxjusti 2.067 0.867 2.383 0.017 0.691 4.089 ## 12 skeptic ~~ frame 0.021 0.074 0.277 0.782 -0.125 0.168 ## 13 skeptic ~~ skepxframe 1.957 0.349 5.606 0.000 1.333 2.721 ## 14 skepxjusti ~~ skepxjusti 7.017 2.414 2.907 0.004 3.602 13.111 ## 15 skepxjusti ~~ frame 0.204 0.091 2.233 0.026 0.045 0.414 ## 16 skepxjusti ~~ skepxframe 1.828 0.784 2.332 0.020 0.600 3.681 ## 17 frame ~~ frame 0.250 0.003 97.289 0.000 0.244 0.250 ## 18 frame ~~ skepxframe 0.011 0.052 0.207 0.836 -0.089 0.121 ## 19 skepxframe ~~ skepxframe 1.957 0.348 5.618 0.000 1.328 2.708 ## 20 donate ~1 4.499 0.110 40.942 0.000 4.288 4.708 ## 21 justify ~1 -0.060 0.079 -0.759 0.448 -0.208 0.109 ## 22 skeptic ~1 0.000 0.142 0.000 1.000 -0.274 0.286 ## 23 skepxjusti ~1 0.828 0.179 4.632 0.000 0.526 1.218 ## 24 frame ~1 0.479 0.036 13.458 0.000 0.396 0.540 ## 25 skepxframe ~1 0.021 0.099 0.206 0.837 -0.173 0.223 ## 26 IndMedMod1 := a1*b3+a3*b1 IndMedMod1 -0.185 0.062 -2.975 0.003 -0.324 -0.071 ## 27 IndMedMod2 := a3*b3 IndMedMod2 0.002 0.005 0.330 0.741 -0.009 0.012 ## 28 aLow := a1+a3*(-1.78) aLow -0.241 0.154 -1.568 0.117 -0.563 0.045 ## 29 aMedian := a1+a3*(-0.58) aMedian 0.000 0.117 0.004 0.997 -0.240 0.209 ## 30 aMean := a1+a3*(0) aMean 0.117 0.115 1.023 0.307 -0.113 0.324 ## 31 aHigh := a1+a3*1.82 aHigh 0.483 0.168 2.874 0.004 0.138 0.829 ## 32 bLow := b1+b3*(-1.78) bLow -0.937 0.103 -9.140 0.000 -1.138 -0.729 ## 33 bMedian := b1+b3*(-0.58) bMedian -0.927 0.085 -10.904 0.000 -1.096 -0.758 ## 34 bMean := b1+b3*(0) bMean -0.922 0.079 -11.605 0.000 -1.085 -0.772 ## 35 bHigh := b1+b3*1.82 bHigh -0.907 0.079 -11.427 0.000 -1.073 -0.752 ## 36 abLow := aLow*bLow abLow 0.226 0.150 1.501 0.133 -0.035 0.578 ## 37 abMedian := aMedian*bMedian abMedian 0.000 0.109 -0.004 0.997 -0.192 0.234 ## 38 abMean := aMean*bMean abMean -0.108 0.105 -1.028 0.304 -0.304 0.096 ## 39 abHigh := aHigh*bHigh abHigh -0.438 0.161 -2.727 0.006 -0.779 -0.123 Here we have two interaction terms: a3 = 0.201 (p = 0.001) b3 = 0.008 (p = 0.743) which means that skeptic is a moderator for the a path but not a moderator for the b path. As for the index of moderated mediation: IndMedMod1 = -0.185 (p = 0.003) IndMedMod2 = 0.002 (p = 0.741) So IndMedMod1 is sig but IndMedMod2 is not (meh)… Let’s examine the indirect effects at different levels: abLow = 0.226 (p = 0.133) abMedian = 0.000 (p = 0.997) abMean = -0.108 (p = 0.304) abHigh = -0.438 (p = 0.006) which does vary as a function of skepticism. So, we still have a significant moderated mediation in this model. 7.11 Model 4g: Moderated Mediation Model - Path a, b, and cprime par(mfrow = c(1,2), mar=c(0,0,0,0), oma=c(0,0,0,0)) pmacroModel(59,labels=labels) statisticalDiagram(59, labels=labels) Let’s write the syntax for the moderated mediation model: lm4g.syntax &lt;- &#39; donate ~ b1*justify + b2*skepxjusti + c1prime*frame + c2prime*skeptic + c3prime*skepxframe justify ~ a1*frame + a2*skeptic + a3*skepxframe # index of moderated mediation IndMedMod1:= a1*b2+a3*b1 IndMedMod2:= a3*b2 # simple slope of justify on frame is a1+a3*skeptic aLow: = a1+a3*(-1.78) aMedian: = a1+a3*(-0.58) aMean: = a1+a3*(0) aHigh: = a1+a3*1.82 # simple slope of donate on justify is b1+b2*skeptic bLow: = b1+b2*(-1.78) bMedian: = b1+b2*(-0.58) bMean: = b1+b2*(0) bHigh: = b1+b2*1.82 # conditional indirect effects is a*(b1+b2*skeptic) abLow: = aLow*bLow abMedian: = aMedian*bMedian abMean: = aMean*bMean abHigh: = aHigh*bHigh &#39; set.seed(2022) lm4g.fit = sem(lm4g.syntax, data = disaster, se = &quot;bootstrap&quot;, bootstrap = 1000, fixed.x=FALSE, meanstructure = TRUE) parameterEstimates(lm4g.fit, level = 0.95, boot.ci.type = &quot;bca.simple&quot;) ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 donate ~ justify b1 -0.925 0.081 -11.473 0.000 -1.085 -0.762 ## 2 donate ~ skepxjusti b2 0.007 0.028 0.258 0.797 -0.047 0.063 ## 3 donate ~ frame c1prime 0.205 0.143 1.432 0.152 -0.075 0.500 ## 4 donate ~ skeptic c2prime -0.043 0.058 -0.749 0.454 -0.165 0.058 ## 5 donate ~ skepxframe c3prime 0.009 0.079 0.119 0.905 -0.139 0.171 ## 6 justify ~ frame a1 0.117 0.114 1.023 0.306 -0.113 0.324 ## 7 justify ~ skeptic a2 0.105 0.043 2.459 0.014 0.020 0.194 ## 8 justify ~ skepxframe a3 0.201 0.063 3.201 0.001 0.077 0.326 ## 9 donate ~~ donate 0.943 0.116 8.148 0.000 0.744 1.212 ## 10 justify ~~ justify 0.648 0.066 9.767 0.000 0.539 0.809 ## 11 skepxjusti ~~ skepxjusti 7.017 2.414 2.907 0.004 3.602 13.111 ## 12 skepxjusti ~~ frame 0.204 0.091 2.233 0.026 0.045 0.414 ## 13 skepxjusti ~~ skeptic 2.067 0.867 2.383 0.017 0.691 4.089 ## 14 skepxjusti ~~ skepxframe 1.828 0.784 2.332 0.020 0.600 3.681 ## 15 frame ~~ frame 0.250 0.003 97.289 0.000 0.245 0.250 ## 16 frame ~~ skeptic 0.021 0.074 0.277 0.782 -0.125 0.168 ## 17 frame ~~ skepxframe 0.011 0.052 0.207 0.836 -0.089 0.121 ## 18 skeptic ~~ skeptic 4.113 0.498 8.257 0.000 3.211 5.198 ## 19 skeptic ~~ skepxframe 1.957 0.349 5.606 0.000 1.333 2.721 ## 20 skepxframe ~~ skepxframe 1.957 0.348 5.618 0.000 1.328 2.708 ## 21 donate ~1 4.499 0.110 40.945 0.000 4.283 4.708 ## 22 justify ~1 -0.060 0.079 -0.759 0.448 -0.208 0.109 ## 23 skepxjusti ~1 0.828 0.179 4.632 0.000 0.526 1.218 ## 24 frame ~1 0.479 0.036 13.458 0.000 0.396 0.540 ## 25 skeptic ~1 0.000 0.142 0.000 1.000 -0.274 0.286 ## 26 skepxframe ~1 0.021 0.099 0.206 0.837 -0.173 0.223 ## 27 IndMedMod1 := a1*b2+a3*b1 IndMedMod1 -0.185 0.063 -2.948 0.003 -0.321 -0.071 ## 28 IndMedMod2 := a3*b2 IndMedMod2 0.001 0.006 0.257 0.797 -0.009 0.014 ## 29 aLow := a1+a3*(-1.78) aLow -0.241 0.154 -1.568 0.117 -0.563 0.045 ## 30 aMedian := a1+a3*(-0.58) aMedian 0.000 0.117 0.004 0.997 -0.240 0.209 ## 31 aMean := a1+a3*(0) aMean 0.117 0.115 1.023 0.307 -0.113 0.324 ## 32 aHigh := a1+a3*1.82 aHigh 0.483 0.168 2.874 0.004 0.138 0.829 ## 33 bLow := b1+b2*(-1.78) bLow -0.937 0.103 -9.121 0.000 -1.142 -0.730 ## 34 bMedian := b1+b2*(-0.58) bMedian -0.929 0.085 -10.877 0.000 -1.096 -0.756 ## 35 bMean := b1+b2*(0) bMean -0.925 0.081 -11.467 0.000 -1.085 -0.762 ## 36 bHigh := b1+b2*1.82 bHigh -0.912 0.086 -10.622 0.000 -1.084 -0.741 ## 37 abLow := aLow*bLow abLow 0.226 0.151 1.500 0.134 -0.036 0.580 ## 38 abMedian := aMedian*bMedian abMedian 0.000 0.109 -0.004 0.997 -0.194 0.233 ## 39 abMean := aMean*bMean abMean -0.108 0.105 -1.027 0.304 -0.306 0.094 ## 40 abHigh := aHigh*bHigh abHigh -0.441 0.164 -2.685 0.007 -0.783 -0.118 Here we have interaction terms: a3 = 0.201 (p = 0.001) b2 = 0.007 (p = 0.797) c3prime = 0.009 (p = 0.905) which means that skeptic is a moderator for the a path but not a moderator for the b path or the cprime path. As for the index of moderated mediation: IndMedMod1 = -0.185 (p = 0.003) IndMedMod2 = 0.001 (p = 0.797) So IndMedMod1 is sig but IndMedMod2 is not (again)… Let’s examine the indirect effects at different levels: abLow = 0.226 (p = 0.134) abMedian = 0.000 (p = 0.997) abMean = -0.108 (p = 0.304) abHigh = -0.441 (p = 0.007) which does vary as a function of skepticism. So, we still have a significant moderated mediation in this model. 7.12 Conclusions Although the total effect of frame on donation is not significant to begin with (in Model 1), it should not discourage you from looking for mediators and moderators on any of the paths. In the simple mediation model (model 2), only b path is significantly negative, meaning that justification for withholding aid always leads to less donation regardless of the skepticism towards climate change. Although a path was not significant, again, it should not discourage you from looking for mediators and moderators on that a path. Including a moderator skeptic for a path and testing the moderated mediation models in Model 4a-Model 4g showed that skeptic is only a moderator for the a path, meaning that those who are high on climate change skepticism think it’s justified to withhold the aid whereas those who are low on the skepticism think it’s not justified to withhold the aid. Comparing Model 4a/4d/4f/4g (which all involve moderate a path), all the simple slopes and indirect effects of a path are more or less the same, and I recommend reported model 4g. Our final conclusion is: a path was moderated by skepticism, b path was not moderated by skepticism but b path itself is significant, cprime was not moderated by skepticism. The indirect path ab was also moderated by skepticism. In particular, framing the disaster as caused by climate change (X) leads to less donation (Y) for people who doubt climate change (W_high) but it leads to more donation (Y) for people who believes it (W_low) because they do not think it’s justified to withhold the aid (M). Ignoring this moderator leads to an insignificant mediation effect in Model 2. According to Hayes (2017, p. 439): “Climate change skeptics seem to feel that victims of a climate change induced disaster (compared to one not attributed to climate change) don’t deserve assistance, and this belief may translate into a reduced willingness to personally donate to the victims. This is a negative indirect effect. But among believers in climate change, the opposite effect is observed, with a climate change induced disaster leading believers to see the victims as more worthy of assistance than if the disaster wasn’t caused by climate change, and this is related to a greater willingness to donate. This is a positive indirect effect. Ignoring the contingency of the indirect effect by failing to include moderation by climate change skepticism in the mediation model obscures the conditional nature of the mechanism at work.” "],["week7_1-lavaan-lab-5-one-factor-cfa-model.html", "Chapter 8 Week7_1: Lavaan Lab 5 One-factor CFA Model 8.1 Data Prep 8.2 PART I: One-Factor CFA, Fixed Loading 8.3 PART II: One-Factor CFA, Fixed Factor Variance 8.4 Exercise: One-factor CFA Model", " Chapter 8 Week7_1: Lavaan Lab 5 One-factor CFA Model In this lab, we will learn how to: Identify the One-factor CFA Model Scale the One-factor CFA Model Estimate the One-factor CFA Model Interpret the One-factor CFA Model 8.1 Data Prep We will use cfaInClassData.csv in this lab. This is a simulated dataset based on Todd Little’s positive affect example. The hypothesis is that a latent variable ‘positive affect’ is measured by three indicators (glad, cheerful, and happy). Let’s read this dataset in: cfaData&lt;- read.csv(&quot;cfaInclassData.csv&quot;, header = T) and examine the dataset: head(cfaData) ## ID glad cheerful happy satisfied content comfortable ## 1 1 0.13521092 0.5413297 -0.1041445 -0.5777446 0.8645383 0.02935020 ## 2 2 -0.29116043 0.2434081 0.6671535 2.0763730 -0.7382832 1.05439183 ## 3 3 0.71975913 0.2218277 0.4722337 2.1685984 -0.2727574 0.09053090 ## 4 4 0.44432030 0.9295414 0.8574083 -1.0575363 -1.3841364 -0.07940091 ## 5 5 2.84476524 3.1710123 3.5145040 1.5725274 2.3406754 1.59866763 ## 6 6 -0.03317526 -0.8434011 -0.1485924 -0.5469343 -1.5750953 -0.69629828 str(cfaData) ## &#39;data.frame&#39;: 1000 obs. of 7 variables: ## $ ID : int 1 2 3 4 5 6 7 8 9 10 ... ## $ glad : num 0.135 -0.291 0.72 0.444 2.845 ... ## $ cheerful : num 0.541 0.243 0.222 0.93 3.171 ... ## $ happy : num -0.104 0.667 0.472 0.857 3.515 ... ## $ satisfied : num -0.578 2.076 2.169 -1.058 1.573 ... ## $ content : num 0.865 -0.738 -0.273 -1.384 2.341 ... ## $ comfortable: num 0.0294 1.0544 0.0905 -0.0794 1.5987 ... dim(cfaData) #n = 1000, 7 variables ## [1] 1000 7 Let’s examine their means and standard deviations: round(apply(cfaData[,-1], 2, mean), 2) # mean-centered ## glad cheerful happy satisfied content comfortable ## 0.00 0.00 0.01 -0.04 -0.04 -0.04 round(apply(cfaData[,-1], 2, sd), 2) ## glad cheerful happy satisfied content comfortable ## 0.98 0.98 0.98 1.01 1.08 0.95 Let’s call up the lavaan library and run some CFA’s! library(lavaan) 8.2 PART I: One-Factor CFA, Fixed Loading 8.2.1 Fixed Loading, AKA Marker Variable method. FYI, the three equations for the three indicators are: Glad = lambda1*posAffect(eta) + u1 Cheerful = lambda2*posAffect(eta) + u2 Happy = lambda3*posAffect(eta) + u3 Let’s first follow the equations above and write the syntax (disturbances are automatically included): mod1.wrong&lt;- &quot; glad ~ posAffect cheerful ~ posAffect happy ~ posAffect &quot; fit1.wrong = lavaan::sem(model = mod1.wrong, data = cfaData, fixed.x=FALSE) Oops - an error message! Error in lav_data_full(data = data, group = group, cluster = cluster, : lavaan ERROR: missing observed variables in dataset: posAffect This is because posAffect is a latent variable and we have to use =~ to define a latent variable: mod1.wrong&lt;-&#39; posAffect =~ Glad + Cheerful + Happy &#39; fit1.wrong = lavaan::sem(model = mod1.wrong, data = cfaData, fixed.x=FALSE) Error in lavaan::lavaan(model = mod1.wrong, data = cfaData, fixed.x = FALSE, : lavaan ERROR: missing observed variables in dataset: Glad Cheerful Happy Error, why? The variable names in the model syntax have to match the column names EXACTLY, even the letter cases. Let’s try again: mod1&lt;-&#39; posAffect =~ glad + cheerful + happy &#39; Let’s explain the lavaan model syntax! mod1 is used to name our model. Since posAffect is a latent variable (it’s not in the data), we cannot follow the equations above and write syntax like glad ~ posAffect Instead, we specify a CFA measurement model in mod1. NEW SYNTAX ALERT: Using =~ means “manifested by” In the code above we can see that our latent construct ‘posAffect’ is manifested by glad, cheerful, and happy By default, the loading of glad is fixed at 1 (Fixed Loading Method) Next we name the fitted object ‘fit1’ to see our output. fit1 = lavaan::sem(mod1, data = cfaData, fixed.x=FALSE) This summary will show us the loadings (I also requested standardized results): summary(fit1, standardized = T) ## lavaan 0.6-10 ended normally after 20 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.693 0.705 ## cheerful 1.117 0.059 18.782 0.000 0.774 0.787 ## happy 1.066 0.057 18.786 0.000 0.739 0.757 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.485 0.030 16.238 0.000 0.485 0.503 ## .cheerful 0.367 0.030 12.062 0.000 0.367 0.380 ## .happy 0.407 0.030 13.751 0.000 0.407 0.427 ## posAffect 0.480 0.043 11.270 0.000 1.000 1.000 df = 0 (why?) Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all posAffect =~ glad 1.000 0.693 0.705 cheerful 1.117 0.059 18.782 0.000 0.774 0.787 happy 1.066 0.057 18.786 0.000 0.739 0.757 What does this mean? 1 unit change in posAffect produces: 1-unit change in “glad” (marker indicator) 1.117-unit change in “cheerful” (1.117 times greater than the effect on “glad”) 1.066-unit change in “happy” (1.066 times greater than the effect on “glad”) Variances: Unique factor variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .glad 0.485 0.030 16.238 0.000 0.485 0.503 .cheerful 0.367 0.030 12.062 0.000 0.367 0.380 .happy 0.407 0.030 13.751 0.000 0.407 0.427 The leftover unique factor variances remain substantial Meaning that none of the indicators is a perfect measure of posAffect but they all contribute significantly to the measurement of posAffect (the standardized loadings above larger than 0.6) Followed by the latent factor variance. Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all posAffect 0.480 0.043 11.270 0.000 1.000 1.000 8.2.2 Change marker indicator If you’d like to fix the 2nd loading to 1: mod1b_wrong&lt;-&#39; posAffect =~ glad + 1*cheerful + happy &#39; won’t work. You will get something like this: Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all posAffect =~ glad 1.000 0.734 0.733 cheerful 1.000 0.734 0.759 happy 1.009 0.046 22.052 0.000 0.741 0.759 You’ll have to change the order of the indicators to move cheerful to the front of the variable list: mod1b&lt;-&#39; posAffect =~ cheerful + glad + happy &#39; Or use *NA to specify which loading to keep free and use *1 to specify the marker variable whose loading to be fixed at 1 mod1b&lt;-&#39; posAffect =~ NA*glad + 1*cheerful + NA*happy &#39; Here we named the fitted object ‘fit1b’ to see our output. fit1b = lavaan::sem(mod1b, data = cfaData, fixed.x=FALSE) summary(fit1b, standardized = T) ## lavaan 0.6-10 ended normally after 19 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 0.895 0.048 18.782 0.000 0.693 0.705 ## cheerful 1.000 0.774 0.787 ## happy 0.954 0.050 19.130 0.000 0.739 0.757 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.485 0.030 16.238 0.000 0.485 0.503 ## .cheerful 0.367 0.030 12.062 0.000 0.367 0.380 ## .happy 0.407 0.030 13.751 0.000 0.407 0.427 ## posAffect 0.599 0.048 12.616 0.000 1.000 1.000 The loadings can be obtained by dividing those in fit1 by 1.117 (i.e., they change proportionally). The variances of unique factors and latent factor remain unchanged. 8.3 PART II: One-Factor CFA, Fixed Factor Variance 8.3.1 Fixed Factor Method Keep using the same syntax but assign a new name mod2: mod2&lt;-&#39; posAffect =~ glad + cheerful + happy &#39; To fix the variance of the latent variable to 1, add std.lv=T to sem() function: fit2&lt;-lavaan::sem(mod2, data = cfaData, fixed.x=FALSE, std.lv=T) summary(fit2, standardized = TRUE) ## lavaan 0.6-10 ended normally after 11 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 0.693 0.031 22.540 0.000 0.693 0.705 ## cheerful 0.774 0.031 25.233 0.000 0.774 0.787 ## happy 0.739 0.030 24.226 0.000 0.739 0.757 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.485 0.030 16.238 0.000 0.485 0.503 ## .cheerful 0.367 0.030 12.062 0.000 0.367 0.380 ## .happy 0.407 0.030 13.751 0.000 0.407 0.427 ## posAffect 1.000 1.000 1.000 Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all posAffect =~ glad 0.693 0.031 22.540 0.000 0.693 0.705 cheerful 0.774 0.031 25.233 0.000 0.774 0.787 happy 0.739 0.030 24.226 0.000 0.739 0.757 1-SD change in the factor (posAffect) causes: 0.693-unit change in glad (on its raw scale) 0.774-unit change in cheerful (on its raw scale) 0.739-unit change in happy (on its raw scale) Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .glad 0.485 0.030 16.238 0.000 0.485 0.503 .cheerful 0.367 0.030 12.062 0.000 0.367 0.380 .happy 0.407 0.030 13.751 0.000 0.407 0.427 posAffect 1.000 1.000 1.000 We see that posAffect now has variance (=sd) of 1 All loadings were freely estimated, no loading is 1. and the unique factor variances are the same as before 8.4 Exercise: One-factor CFA Model Could you use the indicators satisfied, content, and comfortable to build a one-factor CFA model to measure a latent variable called Satisfaction? Use the Fixed Loading and the Fixed Factor Methods and compare their estimates. 8.4.1 Fixed Loading 8.4.2 Fixed Factor "],["week8_1-lavaan-lab-6-two-factor-cfa-model.html", "Chapter 9 Week8_1: Lavaan Lab 6 Two-factor CFA Model 9.1 Data Prep 9.2 PART I: Two-Factor CFA, Fixed Loading 9.3 PART II: Two-Factor CFA, Fixed Factor Variance", " Chapter 9 Week8_1: Lavaan Lab 6 Two-factor CFA Model 9.1 Data Prep We will continue to use cfaInClassData.csv in this lab. Let’s read this dataset in: cfaData&lt;- read.csv(&quot;cfaInclassData.csv&quot;, header = T) Load up the lavaan library and run some CFA’s! library(lavaan) 9.2 PART I: Two-Factor CFA, Fixed Loading 9.2.1 Fixed Loading, AKA Marker Variable method. Let’s write up the model syntax for the measurement model with two factors: fixedIndTwoFacSyntax &lt;- &quot; #Factor Specification posAffect =~ glad + happy + cheerful satisfaction =~ satisfied + content + comfortable &quot; Here we named the fitted object ‘fixedIndTwoFacRun’ to see our output: fixedIndTwoFacRun = lavaan::sem(model = fixedIndTwoFacSyntax, data = cfaData, fixed.x=FALSE) Get a summary using summary() function, add standardized=T to request standardized parameter estimates: summary(fixedIndTwoFacRun, standardized=T) ## lavaan 0.6-10 ended normally after 24 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 13 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 2.957 ## Degrees of freedom 8 ## P-value (Chi-square) 0.937 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.694 0.706 ## happy 1.067 0.055 19.294 0.000 0.740 0.758 ## cheerful 1.112 0.057 19.458 0.000 0.772 0.785 ## satisfaction =~ ## satisfied 1.000 0.773 0.767 ## content 1.068 0.052 20.525 0.000 0.826 0.762 ## comfortable 0.918 0.045 20.336 0.000 0.709 0.746 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect ~~ ## satisfaction 0.262 0.025 10.284 0.000 0.488 0.488 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.484 0.029 16.647 0.000 0.484 0.501 ## .happy 0.405 0.028 14.389 0.000 0.405 0.425 ## .cheerful 0.371 0.029 13.004 0.000 0.371 0.384 ## .satisfied 0.419 0.029 14.326 0.000 0.419 0.412 ## .content 0.491 0.034 14.542 0.000 0.491 0.419 ## .comfortable 0.400 0.026 15.315 0.000 0.400 0.443 ## posAffect 0.482 0.042 11.439 0.000 1.000 1.000 ## satisfaction 0.597 0.047 12.686 0.000 1.000 1.000 Here is the fun part. Plot the fitted model using semPaths() function from the semPlot package: library(semPlot) semPaths(fixedIndTwoFacRun) semPaths(fixedIndTwoFacRun, what = &#39;est&#39;) # under &quot;Estimate&quot; df = 8 (why?) Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all posAffect =~ glad 1.000 0.694 0.706 happy 1.067 0.055 19.294 0.000 0.740 0.758 cheerful 1.112 0.057 19.458 0.000 0.772 0.785 satisfaction =~ satisfied 1.000 0.773 0.767 content 1.068 0.052 20.525 0.000 0.826 0.762 comfortable 0.918 0.045 20.336 0.000 0.709 0.746 What does this mean? 1 unit change in posAffect (factor) produces: 1-unit change in “glad” (marker indicator) 1.067-unit change in “happy” (1.067 times greater than the effect on “glad”) 1.112-unit change in “cheerful” (1.112 times greater than the effect on “glad”) 1 unit change in satisfaction (factor) produces: 1-unit change in “satisfied” (marker indicator) 1.068-unit change in “content” (1.068 times greater than the effect on “satisfied”) 0.918-unit change in “comfortable” (0.918 times greater than the effect on “satisfied”) Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .glad 0.484 0.029 16.647 0.000 0.484 0.501 .happy 0.405 0.028 14.389 0.000 0.405 0.425 .cheerful 0.371 0.029 13.004 0.000 0.371 0.384 .satisfied 0.419 0.029 14.326 0.000 0.419 0.412 .content 0.491 0.034 14.542 0.000 0.491 0.419 .comfortable 0.400 0.026 15.315 0.000 0.400 0.443 The leftover unique factor variances remain substantial Meaning that none of the indicators is a perfect measure of posAffect or satisfaction but they all contribute significantly to the measurement of latent variables (the standardized loadings above larger than 0.6) Followed by two factor variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all posAffect 0.482 0.042 11.439 0.000 1.000 1.000 satisfaction 0.597 0.047 12.686 0.000 1.000 1.000 which were freely estimated using Fixed Loading scaling method. Both posAffect and satisfaction are variable across participants (sig* according to p-values) posAffect seems to be more stable than satisfaction (0.482&lt;0.597). Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all posAffect ~~ satisfaction 0.262 0.025 10.284 0.000 0.488 0.488 covariance between posAffect and satisfaction is 0.262 standardized covariance (correlation) between posAffect and satisfaction is 0.488 (sig*) 0.262/sqrt(0.482)/sqrt(0.597) = 0.488 posAffect is positively correlated with satisfaction. Participants who have a high level posAffect tend to have a high level of satisfaction. 9.2.2 How well does this model fit to the data? The model-implied variance of glad is the sum of: Tracing 1: lam1*psi1*lam1 = 1.0*0.482*1.0 = 0.482 Tracing 2: sig2_u1 = sig2_glad = 0.484 which is 0.966. What is the sample variance of glad? var(cfaData$glad) = 0.9664733, which is super close to the model-implied one! meaning our CFA model is doing a pretty good job at explaining the sample variance of glad. The proportion of variance of glad explained by posAffect is: Tracing 1 / (Tracing 1+Tracing 2) = 49.9% Unexplained: 50.1% [Exercise] What about the model-implied variance of comfortable? Tracing 1: 0.918*.597*.918 = .503 Tracing 2: .400 Total .903 Proportion of variance explained: .503/.903 Sample var: var(cfaData$comfortable) = 0.904 [Exercise] What about the model-implied covariance between glad and comfortable? (e.g., cov(y1, y6)) Tracing 1: 1*.262*.918 = .241 Sample cov: cov(cfaData\\(glad, cfaData\\)comfortable) = .230 Overall speaking, how is our model doing at explaining the sample covariance matrix? Remember the fitted() function we used during path analysis R labs? Apply fitted() function to the fitted object fixedIndTwoFacRun (not on the model syntax fixedIndTwoFacSyntax!!): fitted(fixedIndTwoFacRun) ## $cov ## glad happy cherfl satsfd contnt cmfrtb ## glad 0.966 ## happy 0.514 0.953 ## cheerful 0.536 0.571 0.967 ## satisfied 0.262 0.279 0.291 1.016 ## content 0.280 0.298 0.311 0.638 1.173 ## comfortable 0.240 0.256 0.267 0.548 0.586 0.903 Sigma &lt;- fitted(fixedIndTwoFacRun)$cov How close is Sigma to S? Rearrange the rows and columns of Sigma (important!) and take the difference between Sigma and S (S = cov(cfaData[,-1])) ## glad cheerful happy satisfied content comfortable ## glad 0.9664733 0.5370642 0.5123945 0.2781506 0.2813310 0.2295491 ## cheerful 0.5370642 0.9678301 0.5724804 0.2820439 0.3168253 0.2611358 ## happy 0.5123945 0.5724804 0.9537015 0.2851151 0.3127966 0.2438086 ## satisfied 0.2781506 0.2820439 0.2851151 1.0169865 0.6352282 0.5511595 ## content 0.2813310 0.3168253 0.3127966 0.6352282 1.1739185 0.5872279 ## comfortable 0.2295491 0.2611358 0.2438086 0.5511595 0.5872279 0.9042505 diff = Sigma[colnames(S), colnames(S)] - S round(diff, 3) ## glad cheerful happy satisfied content comfortable ## glad -0.001 -0.001 0.001 -0.016 -0.002 0.011 ## cheerful -0.001 -0.001 -0.001 0.009 -0.006 0.006 ## happy 0.001 -0.001 -0.001 -0.006 -0.015 0.012 ## satisfied -0.016 0.009 -0.006 -0.001 0.003 -0.003 ## content -0.002 -0.006 -0.015 0.003 -0.001 -0.002 ## comfortable 0.011 0.006 0.012 -0.003 -0.002 -0.001 print(paste0(&quot;The difference between S and Sigma ranged between &quot;, round(min(diff),4), &quot; and &quot;, round(max(diff),4), &quot;.&quot;)) ## [1] &quot;The difference between S and Sigma ranged between -0.0164 and 0.0124.&quot; This is the closest Sigma can get to S. Any other set of parameter estimates would yield bigger differences with S. Have you wondered about how we obtain the parameter estimates in the output? Estimation down the road… 9.2.3 Fundamental Equation of SEM Just some bonus stuff… You can inspect the fitted object using inspect() and save the object as InspFit1. InspFit1 &lt;- inspect(fixedIndTwoFacRun, what = &quot;est&quot;) Extract Lambda matrix from InspFit1. Lambda &lt;- InspFit1$lambda Lambda ## psAffc stsfct ## glad 1.000 0.000 ## happy 1.067 0.000 ## cheerful 1.112 0.000 ## satisfied 0.000 1.000 ## content 0.000 1.068 ## comfortable 0.000 0.918 Extract Psi matrix from InspFit1. Psi &lt;- InspFit1$psi Psi ## psAffc stsfct ## posAffect 0.482 ## satisfaction 0.262 0.597 Extract Theta matrix from InspFit1. Theta &lt;- InspFit1$theta Theta ## glad happy cherfl satsfd contnt cmfrtb ## glad 0.484 ## happy 0.000 0.405 ## cheerful 0.000 0.000 0.371 ## satisfied 0.000 0.000 0.000 0.419 ## content 0.000 0.000 0.000 0.000 0.491 ## comfortable 0.000 0.000 0.000 0.000 0.000 0.400 Use the three matrices above to calculate the model-implied covariance matrix and save is as SIGMA: SIGMA &lt;- Lambda%*%Psi%*%t(Lambda)+Theta A shortcut function to obtain the SIGMA matrix is to use fitted() function, as shown above… all.equal(Sigma, SIGMA) ## [1] TRUE 9.2.4 Interpretion How do I interpret the results? Introduce the scaling method I used; Based on the loadings, acknowledge that the indicators are not perfect measures of the latent factors, but they all contribute significantly to the measurement of latent factors (the standardized loadings above larger than 0.6); Report the the proportions of variance explained on each indicator; Describe the discrepancy between S and Sigma (where the main differences lie, and whether the differences are concerning); Interpret the correlation among the latent factors (size, sign, positive/negative, sig/non-sig). 9.2.5 Standardized solutions: Std.lv vs. Std.all Std.lv: This is the solution you’ll get using Fixed Factor Variance Scaling Method; All factor variances are fixed as 1.0; Factor covariance is the same as factor correlation; All factor loadings are freely estimated so that the model-implied covariance matrix remains the same; All unique factor variances remain unchanged. For example, under Std.lv, the model-implied variance of glad is the sum of: Tracing 1: lam1*psi1*lam1 = 0.694*1.0*0.694 = 0.482 Tracing 2: sig2_u1 = sig2_glad = 0.484 Proportion of variance explained: 0.482/0.966 = 50.1% which is still 0.966. [Exercise] Why is 0.709 the loading of comfortable under Std.lv? Under Std.lv, the model-implied variance of comfortable is the sum of: Tracing 1: Tracing 2: Proportion of variance explained: Std.all: All factor variances are fixed as 1.0; Factor covariance is the same as factor correlation; All factor loadings and unique factor variances are re-estimated so that the model-implied variances of indicators are all 1.0; For example, under Std.all, the model-implied variance of glad is the sum of: Tracing 1: lam1*psi1*lam1 = 0.706*1.0*0.706 = 0.499 Tracing 2: sig2_u1 = sig2_glad = 0.501 which add to 1.0. [Exercise] Why is 0.746 the loading of comfortable under Std.all? Under Std.all, the model-implied variance of comfortable is the sum of: Tracing 1: Tracing 2: Why the hassle? The solution is fully standardized so that squaring Std.all loadings is equivalent to the proportion of variance explained: 0.706*0.706 (std.all) 1.0*0.482*1.0/0.966 (fixed loading) 0.694*1.0*0.694/0.966 (fixed variance; std.lv) = 49.9% 9.3 PART II: Two-Factor CFA, Fixed Factor Variance 9.3.1 Fixed Factor Method Keep using the same syntax but assign a new name: fixedFacTwoFacSyntax &lt;- &quot; #Factor Specification posAffect =~ glad + happy + cheerful satisfaction =~ satisfied + content + comfortable &quot; To fix the variance of the latent variable to 1, add std.lv=T to sem() function: fixedFacTwoFacRun = sem(model = fixedFacTwoFacSyntax, data = cfaData, fixed.x=FALSE, std.lv=T) Get a summary using summary() function, add standardized=T to request standardized parameter estimates summary(fixedFacTwoFacRun, standardized=T) ## lavaan 0.6-10 ended normally after 16 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 13 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 2.957 ## Degrees of freedom 8 ## P-value (Chi-square) 0.937 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 0.694 0.030 22.878 0.000 0.694 0.706 ## happy 0.740 0.030 24.806 0.000 0.740 0.758 ## cheerful 0.772 0.030 25.798 0.000 0.772 0.785 ## satisfaction =~ ## satisfied 0.773 0.030 25.373 0.000 0.773 0.767 ## content 0.826 0.033 25.207 0.000 0.826 0.762 ## comfortable 0.709 0.029 24.584 0.000 0.709 0.746 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect ~~ ## satisfaction 0.488 0.032 15.097 0.000 0.488 0.488 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.484 0.029 16.647 0.000 0.484 0.501 ## .happy 0.405 0.028 14.389 0.000 0.405 0.425 ## .cheerful 0.371 0.029 13.004 0.000 0.371 0.384 ## .satisfied 0.419 0.029 14.326 0.000 0.419 0.412 ## .content 0.491 0.034 14.542 0.000 0.491 0.419 ## .comfortable 0.400 0.026 15.315 0.000 0.400 0.443 ## posAffect 1.000 1.000 1.000 ## satisfaction 1.000 1.000 1.000 df = 8 # same! Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all posAffect =~ glad 0.694 0.030 22.878 0.000 0.694 0.706 happy 0.740 0.030 24.806 0.000 0.740 0.758 cheerful 0.772 0.030 25.798 0.000 0.772 0.785 satisfaction =~ satisfied 0.773 0.030 25.373 0.000 0.773 0.767 content 0.826 0.033 25.207 0.000 0.826 0.762 comfortable 0.709 0.029 24.584 0.000 0.709 0.746 1-SD change in the factor (posAffect) causes: 0.694-unit change in glad (on its raw scale) 0.740-unit change in happy (on its raw scale) 0.772-unit change in cheerful (on its raw scale) 1-SD change in the factor (satisfaction) causes: 0.773-unit change in satisfied (on its raw scale) 0.826-unit change in content (on its raw scale) 0.709-unit change in comfortable (on its raw scale) Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all posAffect ~~ satisfaction 0.262 0.025 10.284 0.000 0.488 0.488 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .glad 0.484 0.029 16.647 0.000 0.484 0.501 .happy 0.405 0.028 14.389 0.000 0.405 0.425 .cheerful 0.371 0.029 13.004 0.000 0.371 0.384 .satisfied 0.419 0.029 14.326 0.000 0.419 0.412 .content 0.491 0.034 14.542 0.000 0.491 0.419 .comfortable 0.400 0.026 15.315 0.000 0.400 0.443 remain unchanged. Followed by two factor variances. Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all posAffect 1.000 1.000 1.000 satisfaction 1.000 1.000 1.000 "],["week8_2-lavaan-lab-7-two-factor-sr-model.html", "Chapter 10 Week8_2: Lavaan Lab 7 Two-factor SR Model 10.1 Data Prep 10.2 PART I: Two-Factor SR, Fixed Loading 10.3 PART II: Two-Factor SR, Fixed Factor Variance 10.4 PART III: Exercise (what fun!): 3-Factor SR Model", " Chapter 10 Week8_2: Lavaan Lab 7 Two-factor SR Model 10.1 Data Prep Again, we use cfaInClassData.csv in this lab Let’s read this dataset in: cfaData&lt;- read.csv(&quot;cfaInclassData.csv&quot;, header = T) Load up the lavaan library: library(lavaan) 10.2 PART I: Two-Factor SR, Fixed Loading 10.2.1 Fixed Loading, AKA Marker Variable method. Let’s write up the model syntax for the structural regression (SR) model with two factors: srSyntax &lt;- &quot; #Factor Specification posAffect =~ glad + cheerful + happy satisfaction =~ satisfied + content + comfortable #Structural Regression! satisfaction ~ posAffect &quot; Here we named the fitted object ‘srRun’ to see our output: srRun = lavaan::sem(model = srSyntax, data = cfaData, fixed.x=FALSE) Get a summary using summary() function, add standardized=T to request standardized parameter estimates: summary(srRun, standardized = T) ## lavaan 0.6-10 ended normally after 22 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 13 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 2.957 ## Degrees of freedom 8 ## P-value (Chi-square) 0.937 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.694 0.706 ## cheerful 1.112 0.057 19.458 0.000 0.772 0.785 ## happy 1.067 0.055 19.294 0.000 0.740 0.758 ## satisfaction =~ ## satisfied 1.000 0.773 0.767 ## content 1.068 0.052 20.525 0.000 0.826 0.762 ## comfortable 0.918 0.045 20.336 0.000 0.709 0.746 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## satisfaction ~ ## posAffect 0.544 0.047 11.490 0.000 0.488 0.488 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.484 0.029 16.647 0.000 0.484 0.501 ## .cheerful 0.371 0.029 13.004 0.000 0.371 0.384 ## .happy 0.405 0.028 14.389 0.000 0.405 0.425 ## .satisfied 0.419 0.029 14.326 0.000 0.419 0.412 ## .content 0.491 0.034 14.542 0.000 0.491 0.419 ## .comfortable 0.400 0.026 15.315 0.000 0.400 0.443 ## posAffect 0.482 0.042 11.439 0.000 1.000 1.000 ## .satisfaction 0.455 0.038 11.869 0.000 0.762 0.762 The above syntax reproduces the SR analysis from the class slides. CFA part under “Latent Variables” remains unchanged “Covariances” section no longer exists satisfaction is also removed under “Variances” Instead, in the Regressions section: Regressions: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all satisfaction ~ posAffect 0.544 0.047 11.490 0.000 0.488 0.488 returns the regression slope of posAffect (b = 0.544). One-unit change in posAffect is leading to one-unit change in satisfaction. Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .satisfaction 0.455 0.038 11.869 0.000 0.762 0.762 returns the disturbance variance of satisfaction (sigma_(d_2)^2, not psi_2!) 10.3 PART II: Two-Factor SR, Fixed Factor Variance 10.3.1 Fixed Factor Method Here we named the fitted object ‘srRun2’. We add std.lv=T to fix all latent factor variances to 1: srRun2 = sem(model = srSyntax, data = cfaData, fixed.x=FALSE, std.lv=T) summary(srRun2, standardized = T) ## lavaan 0.6-10 ended normally after 17 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 13 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 2.957 ## Degrees of freedom 8 ## P-value (Chi-square) 0.937 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 0.694 0.030 22.878 0.000 0.694 0.706 ## cheerful 0.772 0.030 25.798 0.000 0.772 0.785 ## happy 0.740 0.030 24.806 0.000 0.740 0.758 ## satisfaction =~ ## satisfied 0.675 0.028 23.737 0.000 0.773 0.767 ## content 0.721 0.031 23.611 0.000 0.826 0.762 ## comfortable 0.619 0.027 23.124 0.000 0.709 0.746 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## satisfaction ~ ## posAffect 0.559 0.049 11.501 0.000 0.488 0.488 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.484 0.029 16.647 0.000 0.484 0.501 ## .cheerful 0.371 0.029 13.004 0.000 0.371 0.384 ## .happy 0.405 0.028 14.389 0.000 0.405 0.425 ## .satisfied 0.419 0.029 14.326 0.000 0.419 0.412 ## .content 0.491 0.034 14.542 0.000 0.491 0.419 ## .comfortable 0.400 0.026 15.315 0.000 0.400 0.443 ## posAffect 1.000 1.000 1.000 ## .satisfaction 1.000 0.762 0.762 This doesn’t work as expected, why? Therefore, for SR models, we should always go with fixed loading scaling approach. 10.4 PART III: Exercise (what fun!): 3-Factor SR Model Suppose that in a 3-factor SR model: Positive Affect is measured by Happy and Cheerful Satisfaction is measured by Satisfied and Content Pleasure is measured by Glad and Comfortable Satisfaction is predicted by both Positive Affect and Pleasure Can you use cfaData to fit such a model? "],["week8_2-lavaan-lab-8-estimation-methods.html", "Chapter 11 Week8_2: Lavaan Lab 8 Estimation Methods 11.1 PART I: Hypothetical Example 11.2 PART II: ULS on the Positive Affect Example 11.3 PART III: Calculate ULS test statistic manually 11.4 PART IV: ML vs ULS vs WLS 11.5 PART V: Improper Solutions", " Chapter 11 Week8_2: Lavaan Lab 8 Estimation Methods In this lab, we will learn how to estimate parameters in CFA/SR models. Load up the lavaan library: library(lavaan) 11.1 PART I: Hypothetical Example 11.1.1 One-factor CFA model A made-up sample covariance matrix with n = 200: n = 200 S_3fac = matrix(c(5, 2, 3.5, 2, 3, 2, 3.5, 2, 6), 3, 3, dimnames = list(c(&#39;Y1&#39;, &#39;Y2&#39;, &#39;Y3&#39;), c(&#39;Y1&#39;, &#39;Y2&#39;, &#39;Y3&#39;))) S_3fac ## Y1 Y2 Y3 ## Y1 5.0 2 3.5 ## Y2 2.0 3 2.0 ## Y3 3.5 2 6.0 Fit a one-factor CFA to the sample covariance matrix: one_fac_syntax &lt;- &quot; eta =~ Y1 + Y2 + Y3 &quot; Request Unweighted Least Squares (ULS): one_fac_fit2 &lt;- sem(one_fac_syntax, sample.cov = S_3fac, sample.nobs = n, estimator = &quot;ULS&quot;, fixed.x = FALSE) summary(one_fac_fit2, standardized = T) ## lavaan 0.6-10 ended normally after 23 iterations ## ## Estimator ULS ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 200 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Unstructured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## eta =~ ## Y1 1.000 1.871 0.837 ## Y2 0.571 0.023 24.496 0.000 1.069 0.617 ## Y3 1.000 0.050 19.950 0.000 1.871 0.764 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .Y1 1.500 0.202 7.423 0.000 1.500 0.300 ## .Y2 1.857 0.094 19.749 0.000 1.857 0.619 ## .Y3 2.500 0.202 12.372 0.000 2.500 0.417 ## eta 3.500 0.189 18.497 0.000 1.000 1.000 Sigma: fitted(one_fac_fit2)$cov ## Y1 Y2 Y3 ## Y1 5.0 ## Y2 2.0 3.0 ## Y3 3.5 2.0 6.0 Sigma = fitted(one_fac_fit2)$cov diff = Sigma[colnames(S_3fac), colnames(S_3fac)] - S_3fac round(diff,3) ## Y1 Y2 Y3 ## Y1 0 0 0 ## Y2 0 0 0 ## Y3 0 0 0 all zeros. Meaning that Sigma = S. 11.2 PART II: ULS on the Positive Affect Example Let’s read this dataset in: cfaData&lt;- read.csv(&quot;cfaInclassData.csv&quot;, header = T) Fit a two-factor CFA model: fixedIndTwoFacSyntax &lt;- &quot; #Factor Specification posAffect =~ glad + happy + cheerful satisfaction =~ satisfied + content + comfortable &quot; two_fac_fit_uls &lt;- sem(fixedIndTwoFacSyntax, data = cfaData, fixed.x = FALSE, estimator = &quot;ULS&quot;) summary(two_fac_fit_uls, standardized = T) ## lavaan 0.6-10 ended normally after 33 iterations ## ## Estimator ULS ## Optimization method NLMINB ## Number of model parameters 13 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 0.837 ## Degrees of freedom 8 ## P-value (Unknown) NA ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Unstructured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.696 0.708 ## happy 1.067 0.066 16.205 0.000 0.743 0.760 ## cheerful 1.105 0.068 16.157 0.000 0.769 0.781 ## satisfaction =~ ## satisfied 1.000 0.775 0.768 ## content 1.073 0.064 16.845 0.000 0.832 0.768 ## comfortable 0.906 0.053 17.124 0.000 0.702 0.738 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect ~~ ## satisfaction 0.264 0.017 15.795 0.000 0.489 0.489 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.482 0.050 9.597 0.000 0.482 0.499 ## .happy 0.402 0.054 7.488 0.000 0.402 0.422 ## .cheerful 0.377 0.056 6.762 0.000 0.377 0.390 ## .satisfied 0.417 0.055 7.615 0.000 0.417 0.410 ## .content 0.482 0.060 8.106 0.000 0.482 0.411 ## .comfortable 0.411 0.049 8.322 0.000 0.411 0.455 ## posAffect 0.484 0.039 12.395 0.000 1.000 1.000 ## satisfaction 0.600 0.045 13.455 0.000 1.000 1.000 Sigma: S = cov(cfaData[,-1]) Sigma = fitted(two_fac_fit_uls)$cov[colnames(S), colnames(S)] diff = Sigma - S round(diff,3) ## glad cheerful happy satisfied content comfortable ## glad 0.000 -0.002 0.004 -0.014 0.002 0.009 ## cheerful -0.002 0.000 -0.002 0.009 -0.004 0.003 ## happy 0.004 -0.002 0.000 -0.003 -0.011 0.011 ## satisfied -0.014 0.009 -0.003 0.000 0.009 -0.007 ## content 0.002 -0.004 -0.011 0.009 0.000 -0.003 ## comfortable 0.009 0.003 0.011 -0.007 -0.003 0.000 print(paste0(&quot;The difference between S and Sigma ranged between &quot;, round(min(diff),4), &quot; and &quot;, round(max(diff),4), &quot;.&quot;)) ## [1] &quot;The difference between S and Sigma ranged between -0.0143 and 0.0113.&quot; Sigma is not the same as S, but close. The sum of squared differences is: sum(diff[lower.tri(diff,diag = T)]^2) ## [1] 0.0008375874 11.3 PART III: Calculate ULS test statistic manually ULS test statistic is calculated as: T_uls = (1000-1)*sum(diff[lower.tri(diff,diag = T)]^2) T_uls ## [1] 0.8367498 One can also obtain vectors of S and Sigma first: s = lav_matrix_vech(S) sigma = lav_matrix_vech(Sigma) and calculate the ULS test statistic: T_uls = (1000-1)*sum((s - sigma)^2) T_uls ## [1] 0.8367498 No p-value for ULS test statistic as there is no known distribution for this test statistic i.e., no suitable method for model fit evaluation 11.4 PART IV: ML vs ULS vs WLS 11.4.1 ML Estimation two_fac_fit_ml &lt;- sem(fixedIndTwoFacSyntax, data = cfaData, fixed.x = FALSE, estimator = &quot;ML&quot;) 11.4.2 WLS Estimation two_fac_fit_wls &lt;- sem(fixedIndTwoFacSyntax, data = cfaData, fixed.x = FALSE, estimator = &quot;WLS&quot;) 11.4.3 Compare the parameter estimates coefTable = parameterEstimates(two_fac_fit_ml)[,1:3] coefTable = cbind(coefTable, ML = parameterEstimates(two_fac_fit_ml)$est, ULS = parameterEstimates(two_fac_fit_uls)$est, WLS = parameterEstimates(two_fac_fit_wls)$est) coefTable ## lhs op rhs ML ULS WLS ## 1 posAffect =~ glad 1.0000000 1.0000000 1.0000000 ## 2 posAffect =~ happy 1.0665969 1.0674339 1.0645600 ## 3 posAffect =~ cheerful 1.1121981 1.1045607 1.1150756 ## 4 satisfaction =~ satisfied 1.0000000 1.0000000 1.0000000 ## 5 satisfaction =~ content 1.0683189 1.0732316 1.0719435 ## 6 satisfaction =~ comfortable 0.9175967 0.9059151 0.9151164 ## 7 glad ~~ glad 0.4838829 0.4823845 0.4835721 ## 8 happy ~~ happy 0.4048385 0.4021235 0.4052179 ## 9 cheerful ~~ cheerful 0.3711009 0.3772155 0.3643077 ## 10 satisfied ~~ satisfied 0.4185958 0.4165368 0.4182162 ## 11 content ~~ content 0.4909589 0.4823048 0.4902739 ## 12 comfortable ~~ comfortable 0.4003673 0.4114721 0.4034071 ## 13 posAffect ~~ posAffect 0.4816239 0.4840887 0.4812181 ## 14 satisfaction ~~ satisfaction 0.5973737 0.6004497 0.5964489 ## 15 posAffect ~~ satisfaction 0.2617756 0.2638580 0.2619159 11.5 PART V: Improper Solutions Going back to the 1-factor toy example… Suppose we have a new covariance matrix now: S_3fac_new = matrix(c(5, 1, 3.5, 1, 3, 2, 3.5, 2, 6), 3, 3, dimnames = list(c(&#39;Y1&#39;, &#39;Y2&#39;, &#39;Y3&#39;), c(&#39;Y1&#39;, &#39;Y2&#39;, &#39;Y3&#39;))) S_3fac_new ## Y1 Y2 Y3 ## Y1 5.0 1 3.5 ## Y2 1.0 3 2.0 ## Y3 3.5 2 6.0 The one-factor syntax is still: one_fac_syntax &lt;- &quot; eta =~ Y1 + Y2 + Y3 &quot; ML Estimation: one_fac_fit_new &lt;- sem(one_fac_syntax, sample.cov = S_3fac_new, sample.nobs = n, estimator = &quot;ML&quot;, fixed.x = FALSE) lavaan WARNING: some estimated ov variances are negative negative residual variances ULS doesn’t help either summary(one_fac_fit_new, standardized = T) ## lavaan 0.6-10 ended normally after 28 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 200 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## eta =~ ## Y1 1.000 1.320 0.592 ## Y2 0.571 0.092 6.181 0.000 0.754 0.436 ## Y3 2.000 0.424 4.714 0.000 2.639 1.080 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .Y1 3.234 0.459 7.040 0.000 3.234 0.650 ## .Y2 2.416 0.264 9.150 0.000 2.416 0.810 ## .Y3 -0.995 1.309 -0.760 0.447 -0.995 -0.167 ## eta 1.741 0.499 3.487 0.000 1.000 1.000 Label and constraint sig3 to be larger than 0: one_fac_syntax_const &lt;- &quot; eta =~ Y1 + Y2 + Y3 Y3~~sig3*Y3 # constraints sig3 &gt; 0 &quot; ML Estimation: one_fac_fit_new2 &lt;- sem(one_fac_syntax_const, sample.cov = S_3fac_new, sample.nobs = n, fixed.x = FALSE) summary(one_fac_fit_new2, standardized = T) ## lavaan 0.6-10 ended normally after 71 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## Number of inequality constraints 1 ## ## Number of observations 200 ## ## Model Test User Model: ## ## Test statistic 0.806 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## eta =~ ## Y1 1.000 1.425 0.639 ## Y2 0.571 0.090 6.357 0.000 0.814 0.471 ## Y3 1.714 0.146 11.749 0.000 2.443 1.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .Y3 (sig3) 0.000 0.000 0.000 ## .Y1 2.944 0.294 10.000 0.000 2.944 0.592 ## .Y2 2.322 0.232 10.000 0.000 2.322 0.778 ## eta 2.031 0.401 5.065 0.000 1.000 1.000 ## ## Constraints: ## |Slack| ## sig3 - 0 0.000 "]]
