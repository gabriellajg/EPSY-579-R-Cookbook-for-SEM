[["index.html", "R Cookbook for Structural Equation Modeling Chapter 1 Course", " R Cookbook for Structural Equation Modeling Ge Jiang QUERIES, University of Illinois at Urbana-Champaign Chapter 1 Course Structural Equation Modeling (SEM) is a general class of multivariate techniques that models relationships between latent variables and observed variables (“measurement models”) and relationships among latent variables (“structural models”) simultaneously. Students will learn the theoretical background of SEM as well as the techniques using programming language R. Topics covered in this class include mediation/moderation model; confirmatory factor analysis; model fit evaluation; multi-group SEM; latent growth modeling; MTMM model; and SEM with categorical variables. 4 graduate hours. No professional credit. Prerequisite: EPSY 580 and EPSY 581, or equivalents. This site is supposed to serve as a repository for R codes used in lab sessions of a graduate-level method course EPSY 579. *Disclaimer: Opinions are my own and not the views of my employer. "],["into-to-r.html", "Chapter 2 Into to R 2.1 R as a calculator 2.2 Assigning Objects and Basic Data Entry 2.3 Removing an object from the workspace 2.4 Formal Rules for Indexing Objects in R 2.5 Examples", " Chapter 2 Into to R In this lab, we will learn the basic operations of R. Let’s first meet the commenter in R: #: I am a comment, indicated by a number sign (also called a pound sign, or, a hashtag). 2.1 R as a calculator In many ways, R is just a fancy calculator: 2 + 3 ## [1] 5 To run this command ON A MAC, highlight it and press COMMAND+ENTER. ON A PC, press CTRL+R. If you ‘run’ the comments, it will automatically run the next executable line comments after the command will show in the console as well In R, you can perform: 2 + 1 #addition 7 - 3 #subtraction 2 * 4 #multiplication 8 / 2 #division You can also work with exponents: 4^2 #4 to the 2nd power 4**2 #same thing Square Root: sqrt(16) And perform a variety of other operations. Here’s a helpful table: Arithmetic Operators #===========#==================#===========# # Operator # Meaning # Example # #===========#==================#===========# # + # Addition # 2 + 2 # #-----------#------------------#-----------# # - # Subtraction # 5 - 3 # #-----------#------------------#-----------# # * # Multiplication # 3 * 4 # #-----------#------------------#-----------# # / # Division # 12 / 3 # #-----------#------------------#-----------# # ^ or ** # Power # 3^3; 2**4 # #-----------#------------------#-----------# # sqrt() # square root # sqrt(16) # #-----------#------------------#-----------# # abs() # absolute value # abs(-5) # #-----------#------------------#-----------# Like any calculator, order of operations counts in R: 3*6+5/4 (3*6)+(5/4) #same 3*(6+5)/4 #different 3*((6+5)/4) #same as above Remember PEMDAS? (Parentheses, Exponents, Multiplication, Division, Addition, Subtraction) Given two or more operations in a single expression, PEMDAS tells you the order of the calculation. 2.2 Assigning Objects and Basic Data Entry Run these commands: a &lt;- 3 # &lt;- assigns a value to a name on the right. b = 4 # = also assigns a value to a name on the right; These lines have assigned the numbers 3 and 6 to the labels a and b, respectively. Now try running these lines, which simply display the contents of a and b: a ## [1] 3 b ## [1] 4 Now try running these lines, to call A and B: A B Returns: Error: object ‘A’ not found Error: object ‘B’ not found Why? Because R is caSE-seNSitivE We can perform the same operations on a and b as above: a + b ## [1] 7 We can also assign more than one number to a label, to form a numeric vector. This is accomplished with the c() function, which concatenates a string of values separated by commas. vec &lt;- c(1, 3, 5, 7, 9) vec ## [1] 1 3 5 7 9 separated by spaces, not commas vec2 &lt;- c(2, 4, 6, 8, 10) vec2 ## [1] 2 4 6 8 10 You can similarly perform mathematical operations on these vectors. Here are some basic ones: vec + vec2 #vector addition ## [1] 3 7 11 15 19 vec*2 #scalar multiplication. ## [1] 2 6 10 14 18 vec*a #scalar multiplication. ## [1] 3 9 15 21 27 ELEMENTWISE multiplication: vec*vec2 ## [1] 2 12 30 56 90 ELEMENTWISE division: vec/vec2 ## [1] 0.5000000 0.7500000 0.8333333 0.8750000 0.9000000 2.3 Removing an object from the workspace You can remove objects using the function (rm) For example: remember the object a that we created earlier? a ## [1] 3 let’s remove it: rm(a) Now try to call a: a Error: object ‘a’ not found. 2.4 Formal Rules for Indexing Objects in R There are many clever ways to index and retrieve subsets of objects in R, as we shall see, but all of them boil down to 3 formal rules. By supplying a vector of integers indicating the number(s) of the elements/rows/columns to be subsetted. A vector of POSITIVE INTEGERS indicates the elements to be selected. Vectors can be indexed: stringvec &lt;- c(&quot;Chen&quot;, &quot;Julia&quot;, &quot;Lee&quot;, &quot;Mike&quot;, &quot;Winston&quot;, &quot;Coach&quot;) stringvec[c(1,2,3)] A vector of NEGATIVE INTEGERS indicates the elements NOT to be selected (to be removed). stringvec[c(-1,-2,-3)] By supplying a character vector indicating the names() of the elements/rows/columns to be selected. (row/colnames for table objects). By supplying a logical vector of TRUE and FALSE (T and F) of the same length as the vector or dimension to be subsetted. In this case, elements flagged as TRUE will be selected and those flagged as FALSE will be omitted. COROLLARY: A vector may be indexed in any of these three ways OR BY SUPPLYING AS AN INDEX ANY OBJECT OR OPERATION THAT RETURNS ONE OF THESE THREE THINGS. Let us demonstrate each of these things in turn: 2.5 Examples 1a. Positive integers indicating element numbers. stringvec[c(1,3,5)] #Returns 1st, 3rd, and 5th elements 1b. Negative integers indicating element numbers to be omitted: stringvec[-c(2,4,6)] Note that this is because -c(2,4,6) Negates all three numbers. Also note that: stringvec[c(1,-2,3)] Returns an error. this is because selecting certain (positive) numbers already implies omitting others, so the negative integer is confusing and redundant. Character vector corresponding to element names. Let’s give our object some names: names(stringvec) &lt;- paste(&quot;Friend&quot;, 1:length(stringvec), sep=&quot;&quot;) stringvec stringvec[&quot;Friend1&quot;] stringvec[c(&quot;Friend3&quot;,&quot;Friend5&quot;)] stringvec[paste(&quot;Friend&quot;, c(1, 2, 5), sep=&quot;&quot;)] Logical Vector: Let’s say we want to select “Chen”, “Lee”, “Winston”, and “Coach”. stringvec[c(T, F, T, F, T, T)] Now let’s create a vector that stores each character’s gender: gender &lt;- factor(c(1, 2, 1, 2, 1, 1), levels = c(1,2), labels = c(&quot;Male&quot;, &quot;Female&quot;)) Now we can select “Chen”, “Lee”, “Winston”, and “Coach” by simply entering: stringvec[gender == &quot;Male&quot;] Or we could select Julia and Mike using: stringvec[gender == &quot;Female&quot;] We could get even more creative … stringvec[(gender == &quot;Female&quot; | stringvec == &quot;Chen&quot;)] Why do all of these things work and actually return sensible results? It’s because they all return logical vectors of the appropriate length, with #TRUE values in the slots we want. We can demonstrate this by running these commands outside of the braces: gender == &quot;Male&quot; gender == &quot;Female&quot; Even though this is a completely separate variable, these commands return logical vectors of the appropriate length, with TRUE and FALSE values in the appropriate places. (gender == &quot;Female&quot; | stringvec == &quot;Chen&quot;) Here again, same thing. Although different types of objects we will discuss have different numbers of dimensions and different formats, if you remember these THREE WAYS TO SUBSET AN OBJECT (integers = element index, characters = element name, logical = flag element as TRUE), you will be a master at subsetting any object in R. "],["lavaan-lab-1-path-analysis-model.html", "Chapter 3 Lavaan Lab 1: Path Analysis Model 3.1 Reading-In and Working With Realistic Datasets In R 3.2 Sample Covariance Matrices using the cov() function 3.3 Installing Packages 3.4 Loading Packages (Libraries) That You Have Installed 3.5 Using Lavaan For Path Models 3.6 Plotting SEM model 3.7 Exercise: How would you fit the model in Saunders et al. (2016)?", " Chapter 3 Lavaan Lab 1: Path Analysis Model In this lab, we will learn how to: install a package called lavaan in R perform path analysis using the lavaan package 3.1 Reading-In and Working With Realistic Datasets In R 3.1.0.1 To begin, we will read the file that we will use for our SEM lab (eatingDisorderSimData.csv). Try running this function, as written: file.choose() Using the GUI (graphical user interface) window that pops up, select the file eatingDisorderSimData.csv This should produce a file path like this (note: below is a Mac version): /Your/File/Path/eatingDisorderSimData.csv You can copy this path into the read.csv and put it in the file = argument of the function: read.csv() is a function for reading in .csv files. Assign the name labData to the dataset in R using &lt;- labData &lt;- read.csv(file = &quot;/Users/gejiang/Box Sync/MacSync/Teaching/590SEM/Spring 2022/Week 4/R/eatingDisorderSimData.csv&quot;, header = TRUE) Important Argument: header = if header = TRUE, indicates that your dataset has column names that are to be read in separate from the data. if header = FALSE, indicates that your dataset does NOT have column names, and therefore the first row of the dataset should be read as data. 3.1.0.2 Or you could NEST the file.choose() function inside the read.csv function labData &lt;- read.csv(file = file.choose(), header = T) Because file.choose() returns the file path, putting this inside the read.csv function is the same as writing the path inside the function! 3.1.0.3 Pros and Cons of writing the full file path vs. using read.csv(file = file.choose(), header = T) If you write down the full file path and put it in the function, then the next time you run this R script you can easily read in your data without searching through your directories and folders. However, if you move your file to a different folder in the future, you’ll need to change the directory path in your R script. file.choose() is very easy and user-friendly. Using this method allows you to find your datafile even if you’ve moved it to a different folder. However, it is slightly more effortful to go in and select your folder each time. 3.1.0.4 Gabriella recommends: Set your working directory to the directory that contains the dataset, and simply load your data by typing the name of the .csv file: setwd(&quot;~/Box Sync/MacSync/Teaching/590SEM/Spring 2022/Week 4/R&quot;) labData &lt;- read.csv(file = &quot;eatingDisorderSimData.csv&quot;, header = T, sep = &quot;,&quot;) This serves to save all your future analyses in your working directory. read.csv() is related to a broader function called read.table. The read.table function has a sep = argument sep = If sep = “,” this indicates a comma-separated (.csv) file If sep = ” ” this indicates a tab-delimited (“white space” delimited) file, such as a .txt 3.1.0.5 Finally, point and click always works… library(readr) eatingDisorderSimData &lt;- read_csv(&quot;eatingDisorderSimData.csv&quot;) View(eatingDisorderSimData) 3.2 Sample Covariance Matrices using the cov() function 3.2.0.1 Quick review: str(labData) #structure ## &#39;data.frame&#39;: 1339 obs. of 7 variables: ## $ BMI : num 0.377 0.302 -1.098 -1.13 -2.797 ... ## $ SelfEsteem : num 0.0685 -0.3059 1.4755 -0.1329 1.3538 ... ## $ Accu : num 1.782 0.491 -0.682 2.224 0.892 ... ## $ DietSE : num -0.0544 -2.3957 0.168 1.1851 0.5131 ... ## $ Restrictive: num -0.525 2.067 0.364 -1.656 0.743 ... ## $ Bulimia : num 0.432 0.196 -1.434 -0.675 -0.858 ... ## $ Risk : num 0.508 0.91 -0.777 -0.554 -0.314 ... head(labData) #first few lines ## BMI SelfEsteem Accu DietSE Restrictive Bulimia Risk ## 1 0.3769721 0.0685226 1.7822103 -0.05436952 -0.5251424 0.4322272 0.50794715 ## 2 0.3015484 -0.3058876 0.4909857 -2.39569010 2.0671867 0.1959765 0.90996098 ## 3 -1.0980232 1.4754543 -0.6819827 0.16801384 0.3638750 -1.4337656 -0.77678045 ## 4 -1.1304059 -0.1329290 2.2235223 1.18505959 -1.6557519 -0.6748446 -0.55411733 ## 5 -2.7965343 1.3537804 0.8922687 0.51311551 0.7431860 -0.8575733 -0.31385631 ## 6 0.7205735 -1.9361462 -1.0307704 0.79749119 -1.8609143 0.3290163 0.08012833 colnames(labData) #column names ## [1] &quot;BMI&quot; &quot;SelfEsteem&quot; &quot;Accu&quot; &quot;DietSE&quot; &quot;Restrictive&quot; &quot;Bulimia&quot; &quot;Risk&quot; How many observations are in this dataset? Number of observations = number of rows, with 1 person per row nrow(labData) #1339 ## [1] 1339 let’s save this number as n n &lt;- nrow(labData) Let’s look at the sample covariance matrix of these variables using the cov() function: cov(labData) ## BMI SelfEsteem Accu DietSE Restrictive Bulimia Risk ## BMI 1.07399214 -0.1380786 -0.02076620 -0.10688665 -0.13056197 0.16177119 0.07938074 ## SelfEsteem -0.13807862 1.0021547 0.03501750 0.10557111 -0.11991676 -0.31717764 -0.22864713 ## Accu -0.02076620 0.0350175 0.97176431 -0.02069863 -0.09050653 -0.09549788 -0.10327073 ## DietSE -0.10688665 0.1055711 -0.02069863 0.96607293 -0.15678475 -0.21922044 0.07119772 ## Restrictive -0.13056197 -0.1199168 -0.09050653 -0.15678475 1.01695732 0.58684522 0.78960193 ## Bulimia 0.16177119 -0.3171776 -0.09549788 -0.21922044 0.58684522 1.03637890 0.87337921 ## Risk 0.07938074 -0.2286471 -0.10327073 0.07119772 0.78960193 0.87337921 1.05356717 let’s save this sample cov as capital S: S = cov(labData) If we wanted, we could look at a subset of the dataset, e.g.,: cov(labData[,c(&quot;BMI&quot;, &quot;SelfEsteem&quot;, &quot;Accu&quot;)]) ## BMI SelfEsteem Accu ## BMI 1.0739921 -0.1380786 -0.0207662 ## SelfEsteem -0.1380786 1.0021547 0.0350175 ## Accu -0.0207662 0.0350175 0.9717643 This is often useful if our analysis will only contain certain variables. If only two variables: cov(labData$BMI, labData$SelfEsteem) ## [1] -0.1380786 If only one variable (variance): cov(labData$BMI, labData$BMI) ## [1] 1.073992 3.3 Installing Packages We will mostly be using the lavaan package to perform SEM analyses, so let’s use the install.packages() function to install it first install.packages(&quot;lavaan&quot;) lavaan stands for LAtent VAriable ANalysis using R. lavaan website: http://lavaan.ugent.be Check out the tutorials and examples! 3.4 Loading Packages (Libraries) That You Have Installed AFTER YOU’VE INSTALLED A PACKAGE ONE TIME, YOU DON’T HAVE TO EVER INSTALL IT AGAIN, UNLESS YOU DELETE AND REINSTALL R FOR SOME REASON. HOWEVER, NOW THAT THESE FUNCTIONS ARE INSTALLED IN R ON YOUR MACHINE, YOU MUST LOAD THE LIBRARY EVERY TIME YOU OPEN R AND WISH TO USE IT. To do this, use the library() function: library(lavaan) This is lavaan 0.6-9 lavaan is FREE software! Please report any bugs. Don’t worry about the “BETA” warning, this package is awesome! This may seem like a pain, but roll with it. The good news is that once you do it, you have access to a whole library of SEM functions. If you boot up R and receive error msgs like “could not find function”sem”” IT IS PROBABLY BECAUSE YOU HAVEN’T LOADED THE lavaan PACKAGE. Check out the help page of a particular function, say sem(): help(sem) ?sem 3.5 Using Lavaan For Path Models Every analysis in lavaan has three main parts. Part I: Writing the Model Syntax Part II: Analyzing the Model Using Your Dataset Part III: Examining the results. 3.5.1 PART I: Follow the set of equations we wrote in class: Self-Efficacy = BMI + Self-Esteem + Disturbance Bulimic Symptoms = BMI + Self-Esteem + Self-Efficacy + Disturbance Restrictive Symptoms = BMI + Self-Esteem + Self-Efficacy + Disturbance Overall Risk = BMI + Self-Esteem + Self-Efficacy + Acculturation + Disturbance Let’s write some model syntax: ex1PathSyntax &lt;- &quot; #opening a quote # Tilda ~ : Regression # M ~ X regression (X predicts M) # Each line corresponds to an equation # Disturbance is automatically included for each regression # (i.e. no extra term needed) DietSE ~ BMI + SelfEsteem #DietSE is predicted by BMI and SelfEsteem Bulimia ~ DietSE + BMI + SelfEsteem Restrictive ~ DietSE + BMI + SelfEsteem Risk ~ DietSE + BMI + SelfEsteem + Accu &quot; Things to note here: We are calling our saved model syntax object ex1PathSyntax We assign it using &lt;- as usual Then we open a quotation ” Then we write each part of the model on separate lines. Then we close the quotation ” The variables names need to match those in the dataset (case matters!) Add comments inside the model syntax using hashtag 3.5.2 PART II Let’s run our model! To run this model, we will start by using the sem() function. Sensible defaults for estimating CFA models like assumptions of linear regression, so we don’t actually have to write some constraints into the model above Alternatively, one can use lavaan() function [with the fewest default settings] or cfa() function [with similar defaults as sem() function] To use lavaan(), you have to specify all 22 parameters in the model. 3.5.2.1 ex1fit You can run the sem() function using two different sources of data: The raw dataset, using: lavaan::sem(model = modelSyntax, data = yourDataset) example: ex1fit &lt;- lavaan::sem(model = ex1PathSyntax, data = labData) If you encounter errors like: Error in if ((!is.matrix(model)) | ncol(model) != 3) stop(“model argument must be a 3-column matrix”) : argument is of length zero IT IS PROBABLY BECAUSE YOU HAVEN’T LOADED THE lavaan PACKAGE. To make sure you are using the sem() function from the lavaan package, add PackageName:: before a function: ex1fit &lt;- lavaan::sem(model = ex1PathSyntax, data = labData) Then we can obtain complete results using the summary() function: summary(ex1fit) ## lavaan 0.6-12 ended normally after 36 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 19 ## ## Number of observations 1339 ## ## Model Test User Model: ## ## Test statistic 16.429 ## Degrees of freedom 3 ## P-value (Chi-square) 0.001 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## DietSE ~ ## BMI -0.088 0.026 -3.381 0.001 ## SelfEsteem 0.093 0.027 3.480 0.001 ## Bulimia ~ ## DietSE -0.185 0.026 -6.994 0.000 ## BMI 0.096 0.025 3.796 0.000 ## SelfEsteem -0.284 0.026 -10.871 0.000 ## Restrictive ~ ## DietSE -0.166 0.027 -6.039 0.000 ## BMI -0.154 0.026 -5.892 0.000 ## SelfEsteem -0.123 0.027 -4.561 0.000 ## Risk ~ ## DietSE 0.105 0.028 3.766 0.000 ## BMI 0.055 0.027 2.057 0.040 ## SelfEsteem -0.232 0.028 -8.425 0.000 ## Accu 0.007 0.009 0.783 0.434 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .Bulimia ~~ ## .Restrictive 0.536 0.029 18.389 0.000 ## .Risk 0.814 0.034 23.983 0.000 ## .Restrictive ~~ ## .Risk 0.785 0.034 22.996 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .DietSE 0.946 0.037 25.875 0.000 ## .Bulimia 0.890 0.034 25.875 0.000 ## .Restrictive 0.955 0.037 25.875 0.000 ## .Risk 0.989 0.038 25.875 0.000 The covariance matrix, using: lavaan::sem(model = modelSyntax, sample.cov = yourCovarianceMatrix, sample.nobs = numberOfObservationsInYourDataset) This is to illustrate that WITH COMPLETE DATA, you can run SEM analyses using only covariances as input and obtain the same results as with raw data! This positions SEM for meta-analysis and replication studies. example: ex1fit_S &lt;- lavaan::sem(model = ex1PathSyntax, sample.cov = S, sample.nobs = n) summary(ex1fit_S) ## lavaan 0.6-12 ended normally after 36 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 19 ## ## Number of observations 1339 ## ## Model Test User Model: ## ## Test statistic 16.429 ## Degrees of freedom 3 ## P-value (Chi-square) 0.001 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## DietSE ~ ## BMI -0.088 0.026 -3.381 0.001 ## SelfEsteem 0.093 0.027 3.480 0.001 ## Bulimia ~ ## DietSE -0.185 0.026 -6.994 0.000 ## BMI 0.096 0.025 3.796 0.000 ## SelfEsteem -0.284 0.026 -10.871 0.000 ## Restrictive ~ ## DietSE -0.166 0.027 -6.039 0.000 ## BMI -0.154 0.026 -5.892 0.000 ## SelfEsteem -0.123 0.027 -4.561 0.000 ## Risk ~ ## DietSE 0.105 0.028 3.766 0.000 ## BMI 0.055 0.027 2.057 0.040 ## SelfEsteem -0.232 0.028 -8.425 0.000 ## Accu 0.007 0.009 0.783 0.434 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .Bulimia ~~ ## .Restrictive 0.536 0.029 18.389 0.000 ## .Risk 0.814 0.034 23.983 0.000 ## .Restrictive ~~ ## .Risk 0.785 0.034 22.996 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .DietSE 0.946 0.037 25.875 0.000 ## .Bulimia 0.890 0.034 25.875 0.000 ## .Restrictive 0.955 0.037 25.875 0.000 ## .Risk 0.989 0.038 25.875 0.000 The . before a variable name refers to its disturbance. e.g., .Bulimia refers to the disturbance of Bulimia, not Bulimia itself You should get exactly the same output in ex1fit and ex1fit_S. Wait, Gabriella, the df is not 6…. This is because sem() by default assumes that disturbances of endogenous variables covary among themselves (which, in our model, are not correlated at all!) The estimates of disturbance covariances are presented under “Covariances” in the output: Covariances: Estimate Std.Err z-value P(&gt;|z|) .Bulimia ~~ .Restrictive 0.536 0.029 18.389 0.000 .Risk 0.814 0.034 23.983 0.000 .Restrictive ~~ .Risk 0.785 0.034 22.996 0.000 3.5.2.2 ex1PathSyntax_noCov To change those defaults, one needs to explicitly fix those disturbance covariances at 0 (this is a strong assumption, I know…): http://lavaan.ugent.be/tutorial/syntax2.html ex1PathSyntax_noCov &lt;- &quot; #opening a quote # ~~ indicates a two-headed arrow (variance or covariance) # 0* in front of the 2nd variable fixes the covariance at 0 DietSE ~ BMI + SelfEsteem #DietSE is predicted by BMI and SelfEsteem Bulimia ~ DietSE + BMI + SelfEsteem Restrictive ~ DietSE + BMI + SelfEsteem Risk ~ DietSE + BMI + SelfEsteem + Accu #Disturbance covariances (fixed at 0): DietSE ~~ 0*Bulimia DietSE ~~ 0*Restrictive DietSE ~~ 0*Risk Bulimia ~~ 0*Restrictive Bulimia ~~ 0*Risk Restrictive ~~ 0*Risk # These lines above say that there is no covariance among the disturbances of all endogenous variables &quot; ex1fit_noCov &lt;- lavaan::sem(model = ex1PathSyntax_noCov, data = labData) summary(ex1fit_noCov) ## lavaan 0.6-12 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 16 ## ## Number of observations 1339 ## ## Model Test User Model: ## ## Test statistic 3536.813 ## Degrees of freedom 6 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## DietSE ~ ## BMI -0.088 0.026 -3.381 0.001 ## SelfEsteem 0.093 0.027 3.480 0.001 ## Bulimia ~ ## DietSE -0.185 0.026 -6.994 0.000 ## BMI 0.096 0.025 3.796 0.000 ## SelfEsteem -0.284 0.026 -10.871 0.000 ## Restrictive ~ ## DietSE -0.166 0.027 -6.039 0.000 ## BMI -0.154 0.026 -5.892 0.000 ## SelfEsteem -0.123 0.027 -4.561 0.000 ## Risk ~ ## DietSE 0.102 0.028 3.686 0.000 ## BMI 0.053 0.026 2.000 0.045 ## SelfEsteem -0.228 0.027 -8.332 0.000 ## Accu -0.095 0.027 -3.449 0.001 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .DietSE ~~ ## .Bulimia 0.000 ## .Restrictive 0.000 ## .Risk 0.000 ## .Bulimia ~~ ## .Restrictive 0.000 ## .Risk 0.000 ## .Restrictive ~~ ## .Risk 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .DietSE 0.946 0.037 25.875 0.000 ## .Bulimia 0.890 0.034 25.875 0.000 ## .Restrictive 0.955 0.037 25.875 0.000 ## .Risk 0.979 0.038 25.875 0.000 df = 6 and Covariances: Estimate Std.Err z-value P(&gt;|z|) .DietSE ~~ .Bulimia 0.000 .Restrictive 0.000 .Risk 0.000 .Bulimia ~~ .Restrictive 0.000 .Risk 0.000 .Restrictive ~~ .Risk 0.000 Wait, where are the variances and covariances of exogenous variables? They are not included in the output because they are estimated PERFECTLY 3.5.2.3 ex1fit_noCov_freeX fixed.x=FALSE asks for the variances/covariances/means of the exogenous variables to be freely estimated instead of being fixed at the values found from the sample This usually makes no difference from ex1fit_noCov, except that it prints more lines ex1fit_noCov_freeX &lt;- lavaan::sem(model = ex1PathSyntax_noCov, data = labData, fixed.x = FALSE) summary(ex1fit_noCov_freeX) ## lavaan 0.6-12 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 22 ## ## Number of observations 1339 ## ## Model Test User Model: ## ## Test statistic 3536.813 ## Degrees of freedom 6 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## DietSE ~ ## BMI -0.088 0.026 -3.381 0.001 ## SelfEsteem 0.093 0.027 3.480 0.001 ## Bulimia ~ ## DietSE -0.185 0.026 -6.994 0.000 ## BMI 0.096 0.025 3.796 0.000 ## SelfEsteem -0.284 0.026 -10.871 0.000 ## Restrictive ~ ## DietSE -0.166 0.027 -6.039 0.000 ## BMI -0.154 0.026 -5.892 0.000 ## SelfEsteem -0.123 0.027 -4.561 0.000 ## Risk ~ ## DietSE 0.102 0.028 3.686 0.000 ## BMI 0.053 0.026 2.000 0.045 ## SelfEsteem -0.228 0.027 -8.332 0.000 ## Accu -0.095 0.027 -3.449 0.001 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .DietSE ~~ ## .Bulimia 0.000 ## .Restrictive 0.000 ## .Risk 0.000 ## .Bulimia ~~ ## .Restrictive 0.000 ## .Risk 0.000 ## .Restrictive ~~ ## .Risk 0.000 ## BMI ~~ ## SelfEsteem -0.138 0.029 -4.828 0.000 ## Accu -0.021 0.028 -0.744 0.457 ## SelfEsteem ~~ ## Accu 0.035 0.027 1.298 0.194 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .DietSE 0.946 0.037 25.875 0.000 ## .Bulimia 0.890 0.034 25.875 0.000 ## .Restrictive 0.955 0.037 25.875 0.000 ## .Risk 0.979 0.038 25.875 0.000 ## BMI 1.073 0.041 25.875 0.000 ## SelfEsteem 1.001 0.039 25.875 0.000 ## Accu 0.971 0.038 25.875 0.000 3.5.2.4 ex1fit_noCov_lavaan As a bonus, here is how you would write the model syntax if you use lavaan() instead of sem()… ex1PathSyntax_lavaan &lt;- &quot; #opening a quote # ~~ indicates a two-headed arrow (variance or covariance) #regression coefficients (12) DietSE ~ BMI + SelfEsteem Bulimia ~ DietSE + BMI + SelfEsteem Restrictive ~ DietSE + BMI + SelfEsteem Risk ~ DietSE + BMI + SelfEsteem + Accu #variances of exogenous variables (3) BMI ~~ BMI SelfEsteem ~~ SelfEsteem Accu ~~ Accu #disturbance variances (4) DietSE ~~ DietSE Bulimia ~~ Bulimia Restrictive ~~ Restrictive Risk ~~ Risk #covariances among exogenous variables (3) BMI ~~ SelfEsteem BMI ~~ Accu SelfEsteem ~~ Accu #total: 22 parameters &quot; ex1fit_noCov_lavaan &lt;- lavaan(model = ex1PathSyntax_lavaan, data = labData) summary(ex1fit_noCov_lavaan) ## lavaan 0.6-12 ended normally after 11 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 22 ## ## Number of observations 1339 ## ## Model Test User Model: ## ## Test statistic 3536.813 ## Degrees of freedom 6 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## DietSE ~ ## BMI -0.088 0.026 -3.381 0.001 ## SelfEsteem 0.093 0.027 3.480 0.001 ## Bulimia ~ ## DietSE -0.185 0.026 -6.994 0.000 ## BMI 0.096 0.025 3.796 0.000 ## SelfEsteem -0.284 0.026 -10.871 0.000 ## Restrictive ~ ## DietSE -0.166 0.027 -6.039 0.000 ## BMI -0.154 0.026 -5.892 0.000 ## SelfEsteem -0.123 0.027 -4.561 0.000 ## Risk ~ ## DietSE 0.102 0.028 3.686 0.000 ## BMI 0.053 0.026 2.000 0.045 ## SelfEsteem -0.228 0.027 -8.332 0.000 ## Accu -0.095 0.027 -3.449 0.001 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## BMI ~~ ## SelfEsteem -0.138 0.029 -4.828 0.000 ## Accu -0.021 0.028 -0.744 0.457 ## SelfEsteem ~~ ## Accu 0.035 0.027 1.298 0.194 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## BMI 1.073 0.041 25.875 0.000 ## SelfEsteem 1.001 0.039 25.875 0.000 ## Accu 0.971 0.038 25.875 0.000 ## .DietSE 0.946 0.037 25.875 0.000 ## .Bulimia 0.890 0.034 25.875 0.000 ## .Restrictive 0.955 0.037 25.875 0.000 ## .Risk 0.979 0.038 25.875 0.000 which yields the same output as ex1fit_noCov_freeX. 3.5.3 Sigma Matrices Let’s have a look at the model-implied covarinace matrix from our final model ex1fit_noCov_freeX and save it as Sigma: fitted(ex1fit_noCov_freeX) ## $cov ## DietSE Bulimi Rstrct Risk BMI SlfEst Accu ## DietSE 0.965 ## Bulimia -0.219 1.036 ## Restrictive -0.157 0.051 1.016 ## Risk 0.069 0.060 0.005 1.052 ## BMI -0.107 0.162 -0.130 0.079 1.073 ## SelfEsteem 0.105 -0.317 -0.120 -0.228 -0.138 1.001 ## Accu 0.005 -0.013 -0.002 -0.101 -0.021 0.035 0.971 Sigma &lt;- fitted(ex1fit_noCov_freeX)$cov How close is Sigma to S? Rearrange the rows and columns of Sigma (important!) and take the difference diff = Sigma[colnames(S), colnames(S)] - S round(diff, 3) ## BMI SelfEsteem Accu DietSE Restrictive Bulimia Risk ## BMI -0.001 0.000 0.000 0.000 0.000 0.000 0.000 ## SelfEsteem 0.000 -0.001 0.000 0.000 0.000 0.000 0.000 ## Accu 0.000 0.000 -0.001 0.026 0.089 0.083 0.003 ## DietSE 0.000 0.000 0.026 -0.001 0.000 0.000 -0.002 ## Restrictive 0.000 0.000 0.089 0.000 -0.001 -0.536 -0.785 ## Bulimia 0.000 0.000 0.083 0.000 -0.536 -0.001 -0.814 ## Risk 0.000 0.000 0.003 -0.002 -0.785 -0.814 -0.001 How about the default model that include disturbance covariances? Sigma0 &lt;- fitted(ex1fit)$cov diff0 = Sigma0[colnames(S), colnames(S)] - S round(diff0, 3) ## BMI SelfEsteem Accu DietSE Restrictive Bulimia Risk ## BMI -0.001 0.000 0.000 0.000 0.000 0.000 0.000 ## SelfEsteem 0.000 -0.001 0.000 0.000 0.000 0.000 0.000 ## Accu 0.000 0.000 -0.001 0.026 0.089 0.083 0.101 ## DietSE 0.000 0.000 0.026 -0.001 0.000 0.000 0.000 ## Restrictive 0.000 0.000 0.089 0.000 -0.001 0.000 0.000 ## Bulimia 0.000 0.000 0.083 0.000 0.000 -0.001 0.000 ## Risk 0.000 0.000 0.101 0.000 0.000 0.000 0.001 3.5.3.1 Gabriella’s Practical Tips: To begin with, constraint the disturbance covariances to be 0 ; Keep the model if the model fits the data well; Relax the constraints the disturbance covariances if the initial model did not fit well. 3.5.4 PART III: Summarizing Our Analysis: There are some useful options we can ask for with summary(): summary(ex1fit_noCov_freeX, fit.measures = T) #include model fit measures summary(ex1fit_noCov_freeX, standardized = T) #This includes standardized estimates. std.all contains usual regression standardization. summary(ex1fit_noCov_freeX, ci = T) #Include confidence intervals # Add them all! If we JUST want the parameter estimates: parameterEstimates(ex1fit_noCov_freeX) ## lhs op rhs est se z pvalue ci.lower ci.upper ## 1 DietSE ~ BMI -0.088 0.026 -3.381 0.001 -0.138 -0.037 ## 2 DietSE ~ SelfEsteem 0.093 0.027 3.480 0.001 0.041 0.146 ## 3 Bulimia ~ DietSE -0.185 0.026 -6.994 0.000 -0.237 -0.133 ## 4 Bulimia ~ BMI 0.096 0.025 3.796 0.000 0.046 0.145 ## 5 Bulimia ~ SelfEsteem -0.284 0.026 -10.871 0.000 -0.335 -0.233 ## 6 Restrictive ~ DietSE -0.166 0.027 -6.039 0.000 -0.220 -0.112 ## 7 Restrictive ~ BMI -0.154 0.026 -5.892 0.000 -0.205 -0.103 ## 8 Restrictive ~ SelfEsteem -0.123 0.027 -4.561 0.000 -0.176 -0.070 ## 9 Risk ~ DietSE 0.102 0.028 3.686 0.000 0.048 0.157 ## 10 Risk ~ BMI 0.053 0.026 2.000 0.045 0.001 0.105 ## 11 Risk ~ SelfEsteem -0.228 0.027 -8.332 0.000 -0.282 -0.175 ## 12 Risk ~ Accu -0.095 0.027 -3.449 0.001 -0.149 -0.041 ## 13 DietSE ~~ Bulimia 0.000 0.000 NA NA 0.000 0.000 ## 14 DietSE ~~ Restrictive 0.000 0.000 NA NA 0.000 0.000 ## 15 DietSE ~~ Risk 0.000 0.000 NA NA 0.000 0.000 ## 16 Bulimia ~~ Restrictive 0.000 0.000 NA NA 0.000 0.000 ## 17 Bulimia ~~ Risk 0.000 0.000 NA NA 0.000 0.000 ## 18 Restrictive ~~ Risk 0.000 0.000 NA NA 0.000 0.000 ## 19 DietSE ~~ DietSE 0.946 0.037 25.875 0.000 0.874 1.018 ## 20 Bulimia ~~ Bulimia 0.890 0.034 25.875 0.000 0.822 0.957 ## 21 Restrictive ~~ Restrictive 0.955 0.037 25.875 0.000 0.883 1.028 ## 22 Risk ~~ Risk 0.979 0.038 25.875 0.000 0.905 1.054 ## 23 BMI ~~ BMI 1.073 0.041 25.875 0.000 0.992 1.154 ## 24 BMI ~~ SelfEsteem -0.138 0.029 -4.828 0.000 -0.194 -0.082 ## 25 BMI ~~ Accu -0.021 0.028 -0.744 0.457 -0.075 0.034 ## 26 SelfEsteem ~~ SelfEsteem 1.001 0.039 25.875 0.000 0.926 1.077 ## 27 SelfEsteem ~~ Accu 0.035 0.027 1.298 0.194 -0.018 0.088 ## 28 Accu ~~ Accu 0.971 0.038 25.875 0.000 0.897 1.045 parameterEstimates(ex1fit_noCov_freeX, standardized = T) #include standardized solution.... ## lhs op rhs est se z pvalue ci.lower ci.upper std.lv std.all std.nox ## 1 DietSE ~ BMI -0.088 0.026 -3.381 0.001 -0.138 -0.037 -0.088 -0.092 -0.089 ## 2 DietSE ~ SelfEsteem 0.093 0.027 3.480 0.001 0.041 0.146 0.093 0.095 0.095 ## 3 Bulimia ~ DietSE -0.185 0.026 -6.994 0.000 -0.237 -0.133 -0.185 -0.179 -0.179 ## 4 Bulimia ~ BMI 0.096 0.025 3.796 0.000 0.046 0.145 0.096 0.097 0.094 ## 5 Bulimia ~ SelfEsteem -0.284 0.026 -10.871 0.000 -0.335 -0.233 -0.284 -0.279 -0.279 ## 6 Restrictive ~ DietSE -0.166 0.027 -6.039 0.000 -0.220 -0.112 -0.166 -0.162 -0.162 ## 7 Restrictive ~ BMI -0.154 0.026 -5.892 0.000 -0.205 -0.103 -0.154 -0.158 -0.153 ## 8 Restrictive ~ SelfEsteem -0.123 0.027 -4.561 0.000 -0.176 -0.070 -0.123 -0.122 -0.122 ## 9 Risk ~ DietSE 0.102 0.028 3.686 0.000 0.048 0.157 0.102 0.098 0.098 ## 10 Risk ~ BMI 0.053 0.026 2.000 0.045 0.001 0.105 0.053 0.053 0.052 ## 11 Risk ~ SelfEsteem -0.228 0.027 -8.332 0.000 -0.282 -0.175 -0.228 -0.223 -0.223 ## 12 Risk ~ Accu -0.095 0.027 -3.449 0.001 -0.149 -0.041 -0.095 -0.091 -0.092 ## 13 DietSE ~~ Bulimia 0.000 0.000 NA NA 0.000 0.000 0.000 0.000 0.000 ## 14 DietSE ~~ Restrictive 0.000 0.000 NA NA 0.000 0.000 0.000 0.000 0.000 ## 15 DietSE ~~ Risk 0.000 0.000 NA NA 0.000 0.000 0.000 0.000 0.000 ## 16 Bulimia ~~ Restrictive 0.000 0.000 NA NA 0.000 0.000 0.000 0.000 0.000 ## 17 Bulimia ~~ Risk 0.000 0.000 NA NA 0.000 0.000 0.000 0.000 0.000 ## 18 Restrictive ~~ Risk 0.000 0.000 NA NA 0.000 0.000 0.000 0.000 0.000 ## 19 DietSE ~~ DietSE 0.946 0.037 25.875 0.000 0.874 1.018 0.946 0.980 0.980 ## 20 Bulimia ~~ Bulimia 0.890 0.034 25.875 0.000 0.822 0.957 0.890 0.859 0.859 ## 21 Restrictive ~~ Restrictive 0.955 0.037 25.875 0.000 0.883 1.028 0.955 0.940 0.940 ## 22 Risk ~~ Risk 0.979 0.038 25.875 0.000 0.905 1.054 0.979 0.931 0.931 ## 23 BMI ~~ BMI 1.073 0.041 25.875 0.000 0.992 1.154 1.073 1.000 1.073 ## 24 BMI ~~ SelfEsteem -0.138 0.029 -4.828 0.000 -0.194 -0.082 -0.138 -0.133 -0.138 ## 25 BMI ~~ Accu -0.021 0.028 -0.744 0.457 -0.075 0.034 -0.021 -0.020 -0.021 ## 26 SelfEsteem ~~ SelfEsteem 1.001 0.039 25.875 0.000 0.926 1.077 1.001 1.000 1.001 ## 27 SelfEsteem ~~ Accu 0.035 0.027 1.298 0.194 -0.018 0.088 0.035 0.035 0.035 ## 28 Accu ~~ Accu 0.971 0.038 25.875 0.000 0.897 1.045 0.971 1.000 0.971 For standardized solutions, there is also this function: standardizedSolution(ex1fit_noCov_freeX, type = &quot;std.all&quot;) ## lhs op rhs est.std se z pvalue ci.lower ci.upper ## 1 DietSE ~ BMI -0.092 0.027 -3.395 0.001 -0.146 -0.039 ## 2 DietSE ~ SelfEsteem 0.095 0.027 3.496 0.000 0.042 0.148 ## 3 Bulimia ~ DietSE -0.179 0.025 -7.093 0.000 -0.228 -0.129 ## 4 Bulimia ~ BMI 0.097 0.026 3.811 0.000 0.047 0.148 ## 5 Bulimia ~ SelfEsteem -0.279 0.025 -11.284 0.000 -0.328 -0.231 ## 6 Restrictive ~ DietSE -0.162 0.026 -6.115 0.000 -0.213 -0.110 ## 7 Restrictive ~ BMI -0.158 0.027 -5.962 0.000 -0.210 -0.106 ## 8 Restrictive ~ SelfEsteem -0.122 0.027 -4.593 0.000 -0.175 -0.070 ## 9 Risk ~ DietSE 0.098 0.027 3.702 0.000 0.046 0.150 ## 10 Risk ~ BMI 0.053 0.027 2.003 0.045 0.001 0.106 ## 11 Risk ~ SelfEsteem -0.223 0.026 -8.536 0.000 -0.274 -0.172 ## 12 Risk ~ Accu -0.091 0.026 -3.462 0.001 -0.143 -0.039 ## 13 DietSE ~~ Bulimia 0.000 0.000 NA NA 0.000 0.000 ## 14 DietSE ~~ Restrictive 0.000 0.000 NA NA 0.000 0.000 ## 15 DietSE ~~ Risk 0.000 0.000 NA NA 0.000 0.000 ## 16 Bulimia ~~ Restrictive 0.000 0.000 NA NA 0.000 0.000 ## 17 Bulimia ~~ Risk 0.000 0.000 NA NA 0.000 0.000 ## 18 Restrictive ~~ Risk 0.000 0.000 NA NA 0.000 0.000 ## 19 DietSE ~~ DietSE 0.980 0.008 129.769 0.000 0.965 0.995 ## 20 Bulimia ~~ Bulimia 0.859 0.018 48.727 0.000 0.824 0.894 ## 21 Restrictive ~~ Restrictive 0.940 0.013 74.768 0.000 0.915 0.965 ## 22 Risk ~~ Risk 0.931 0.013 69.534 0.000 0.904 0.957 ## 23 BMI ~~ BMI 1.000 0.000 NA NA 1.000 1.000 ## 24 BMI ~~ SelfEsteem -0.133 0.027 -4.958 0.000 -0.186 -0.080 ## 25 BMI ~~ Accu -0.020 0.027 -0.744 0.457 -0.074 0.033 ## 26 SelfEsteem ~~ SelfEsteem 1.000 0.000 NA NA 1.000 1.000 ## 27 SelfEsteem ~~ Accu 0.035 0.027 1.300 0.194 -0.018 0.089 ## 28 Accu ~~ Accu 1.000 0.000 NA NA 1.000 1.000 How does it work? ?standardizedSolution 3.6 Plotting SEM model # install.packages(&quot;semPlot&quot;) library(semPlot) # Plot! semPaths(ex1fit_noCov_freeX) # estimates instead of paths only semPaths(ex1fit_noCov_freeX, what=&#39;est&#39;, edge.label.cex=1.25, curvePivot = TRUE, fade=FALSE) # standardized solutions semPaths(ex1fit_noCov_freeX, what=&#39;std&#39;, edge.label.cex=1.25, curvePivot = TRUE, fade=FALSE) semPaths(ex1fit_noCov_freeX, what=&#39;est&#39;, rotation = 2, # default rotation = 1 with four options edge.label.cex=1.25, curvePivot = TRUE, fade=FALSE) 3.6.1 customize it your way semPaths(ex1fit_noCov_freeX, whatLabels=&quot;est&quot;, # plot model not parm ests rotation = 2, # default rotation = 1 with four options asize = 5, # arrows&#39; size esize = 2, # width of paths&#39; lines / curves edge.label.cex = 0.8, # font size of regr&#39;n coeffs sizeMan = 10, # font size of manifest variable names nCharNodes = 0, nCharEdges = 0, # don&#39;t limit variable name lengths fade = FALSE, # don&#39;t weight path width to reflect strength curvePivot = TRUE, # make straight edges instead of round ones curve = 2, # pull covariances&#39; curves out a little style = &quot;lisrel&quot;, # no variances vs. # &quot;ram&quot;&#39;s 2-headed for variances color = &quot;green&quot;, # color of variables edge.color = &quot;black&quot;, # color of edges/paths layout = &quot;tree2&quot;, # tree, spring, circle, circle2 residuals = TRUE) # residuals variances included in the path diagram semPaths(ex1fit_noCov_freeX, what=&#39;est&#39;, rotation = 2, # default rotation = 1 with four options curve = 2, # pull covariances&#39; curves out a little nCharNodes = 0, nCharEdges = 0, # don&#39;t limit variable name lengths sizeMan = 8, # font size of manifest variable names style = &quot;lisrel&quot;, # single-headed arrows vs. # &quot;ram&quot;&#39;s 2-headed for variances edge.label.cex=1.2, curvePivot = TRUE, fade=FALSE) 3.7 Exercise: How would you fit the model in Saunders et al. (2016)? "],["lavaan-lab-2-mediation-and-indirect-effects.html", "Chapter 4 Lavaan Lab 2: Mediation and Indirect Effects 4.1 Reading-In and Working With Realistic Datasets In R 4.2 Using Lavaan For Mediation Models - Preacher &amp; Hayes’s 4.3 PART I: # Follow the two equations of M (DietSE) &amp; Y (Bulimia) 4.4 PART II Let’s run our model! 4.5 PART III: Summarizing Our Analysis: 4.6 PART IV: Bootstrap confidence intervals 4.7 In-Class Exercise: Use Lavaan to estimate and interpret the following model 4.8 Exercise: Eating Disorder Mediation Analysis", " Chapter 4 Lavaan Lab 2: Mediation and Indirect Effects In this lab, we will learn how to: perform a simple mediation analysis using Preacher &amp; Hayes (2004) + Bootstrap test mediation effects in the eating disorder path model 4.1 Reading-In and Working With Realistic Datasets In R If your data (eatingDisorderSimData.csv) is stored in you current working directory, then simply load your data by typing the name of the .csv file: labData &lt;- read.csv(file = &quot;eatingDisorderSimData.csv&quot;, header = T, sep = &quot;,&quot;) 4.2 Using Lavaan For Mediation Models - Preacher &amp; Hayes’s Load the package: library(lavaan) Part I: Writing the Model Syntax Part II: Analyzing the Model Using Your Dataset Part III: Examining the results. 4.3 PART I: # Follow the two equations of M (DietSE) &amp; Y (Bulimia) Diet Self-Efficacy = BMI + Disturbance Bulimic Symptoms = BMI + Diet Self-Efficacy + Disturbance Let’s write some model syntax: ex1MediationSyntax &lt;- &quot; #opening a quote #Regressions DietSE ~ BMI #M ~ X regression (a path) Bulimia ~ BMI + DietSE #Y ~ X + M regression (c prime and b) &quot; No need to fix disturbance covariances in simple mediation as none was estimated 4.4 PART II Let’s run our model! let fixed.x=FALSE to print more lines ex1fit_freeX &lt;- lavaan::sem(model = ex1MediationSyntax, data = labData, fixed.x = FALSE) summary(ex1fit_freeX) ## lavaan 0.6-12 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 1339 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## DietSE ~ ## BMI -0.100 0.026 -3.861 0.000 ## Bulimia ~ ## BMI 0.129 0.026 4.960 0.000 ## DietSE -0.213 0.028 -7.725 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .DietSE 0.955 0.037 25.875 0.000 ## .Bulimia 0.968 0.037 25.875 0.000 ## BMI 1.073 0.041 25.875 0.000 note that there are six parameter estimates and df = 0. But the output does not include the mediation effect a*b? 4.4.1 Label the mediation effect Let’s learn how to label parameters great tutorial example: http://lavaan.ugent.be/tutorial/mediation.html To label a parameter, include the coefficient label and an asterisk * before the variable to be labelled. E.g., y ~ b1x + b2m This would give x the label b1 and m the label b2 in the y regression. ex2MediationSyntax &lt;- &quot; #opening a quote #Regressions DietSE ~ a*BMI #Label the a coefficient in the M regression. Bulimia ~ cPrime*BMI + b*DietSE #Label the direct effect (cPrime) of X and direct effect of M (b) in the Y regression. &quot; What does this do? ex2fit &lt;- lavaan::sem(model = ex2MediationSyntax, data = labData, fixed.x=FALSE) summary(ex2fit) ## lavaan 0.6-12 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 1339 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## DietSE ~ ## BMI (a) -0.100 0.026 -3.861 0.000 ## Bulimia ~ ## BMI (cPrm) 0.129 0.026 4.960 0.000 ## DietSE (b) -0.213 0.028 -7.725 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .DietSE 0.955 0.037 25.875 0.000 ## .Bulimia 0.968 0.037 25.875 0.000 ## BMI 1.073 0.041 25.875 0.000 The regression coefficients have labels now! 4.4.2 Define a new term for the mediation effect a*b …using the labels we just created in ex2MediationSyntax The := operator in lavaan defines new terms to be tested: (name of a new term) := operator ex3MediationSyntax &lt;- &quot; #opening a quote #Regressions DietSE ~ a*BMI #Label the a coefficient in the M regression. Bulimia ~ cPrime*BMI + b*DietSE #Label the direct effect (cPrime) of X and direct effect of M (b) in the Y regression. #Define New Parameters ab := a*b #the product term is computed as a*b c := cPrime + ab #having defined ab, we can use this here. &quot; ex3fit &lt;- lavaan::sem(model = ex3MediationSyntax, data = labData, fixed.x=FALSE) summary(ex3fit) ## lavaan 0.6-12 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 1339 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## DietSE ~ ## BMI (a) -0.100 0.026 -3.861 0.000 ## Bulimia ~ ## BMI (cPrm) 0.129 0.026 4.960 0.000 ## DietSE (b) -0.213 0.028 -7.725 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .DietSE 0.955 0.037 25.875 0.000 ## .Bulimia 0.968 0.037 25.875 0.000 ## BMI 1.073 0.041 25.875 0.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## ab 0.021 0.006 3.454 0.001 ## c 0.151 0.027 5.678 0.000 Now there are two significance tests of the indirect effect ab and the total effect c! Question: why didn’t the #parameters change? Note: defining a new term is NOT equivalent to adding a new parameter! You can create as many terms as your want without changing the #parameters and the df 4.5 PART III: Summarizing Our Analysis: We can request standardized coefficients very easily by adding a statement to the summary command. summary(ex3fit, standardized = TRUE) #This includes standardized estimates. std.all contains usual regression standardization. ## lavaan 0.6-12 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 1339 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## DietSE ~ ## BMI (a) -0.100 0.026 -3.861 0.000 ## Bulimia ~ ## BMI (cPrm) 0.129 0.026 4.960 0.000 ## DietSE (b) -0.213 0.028 -7.725 0.000 ## Std.lv Std.all ## ## -0.100 -0.105 ## ## 0.129 0.132 ## -0.213 -0.205 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .DietSE 0.955 0.037 25.875 0.000 ## .Bulimia 0.968 0.037 25.875 0.000 ## BMI 1.073 0.041 25.875 0.000 ## Std.lv Std.all ## 0.955 0.989 ## 0.968 0.935 ## 1.073 1.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## ab 0.021 0.006 3.454 0.001 ## c 0.151 0.027 5.678 0.000 ## Std.lv Std.all ## 0.021 0.022 ## 0.151 0.153 summary(ex3fit, ci = T) #Include confidence intervals ## lavaan 0.6-12 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 1339 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## DietSE ~ ## BMI (a) -0.100 0.026 -3.861 0.000 ## Bulimia ~ ## BMI (cPrm) 0.129 0.026 4.960 0.000 ## DietSE (b) -0.213 0.028 -7.725 0.000 ## ci.lower ci.upper ## ## -0.150 -0.049 ## ## 0.078 0.181 ## -0.267 -0.159 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .DietSE 0.955 0.037 25.875 0.000 ## .Bulimia 0.968 0.037 25.875 0.000 ## BMI 1.073 0.041 25.875 0.000 ## ci.lower ci.upper ## 0.882 1.027 ## 0.895 1.041 ## 0.992 1.154 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## ab 0.021 0.006 3.454 0.001 ## c 0.151 0.027 5.678 0.000 ## ci.lower ci.upper ## 0.009 0.033 ## 0.099 0.203 or both! summary(ex3fit, standardized = TRUE, ci = T) ## lavaan 0.6-12 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 1339 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## DietSE ~ ## BMI (a) -0.100 0.026 -3.861 0.000 ## Bulimia ~ ## BMI (cPrm) 0.129 0.026 4.960 0.000 ## DietSE (b) -0.213 0.028 -7.725 0.000 ## ci.lower ci.upper Std.lv Std.all ## ## -0.150 -0.049 -0.100 -0.105 ## ## 0.078 0.181 0.129 0.132 ## -0.267 -0.159 -0.213 -0.205 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .DietSE 0.955 0.037 25.875 0.000 ## .Bulimia 0.968 0.037 25.875 0.000 ## BMI 1.073 0.041 25.875 0.000 ## ci.lower ci.upper Std.lv Std.all ## 0.882 1.027 0.955 0.989 ## 0.895 1.041 0.968 0.935 ## 0.992 1.154 1.073 1.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## ab 0.021 0.006 3.454 0.001 ## c 0.151 0.027 5.678 0.000 ## ci.lower ci.upper Std.lv Std.all ## 0.009 0.033 0.021 0.022 ## 0.099 0.203 0.151 0.153 Important: the default significance tests of defined parameters in lavaan is Sobel’s test. 4.6 PART IV: Bootstrap confidence intervals 4.6.1 The default one is boot.ci.type = “perc” You can request bootstrap standard errors in sem() using se = “bootstrap” and bootstrap = 1000 set.seed(2022) ex3Boot &lt;- lavaan::sem(model = ex3MediationSyntax, data = labData, se = &quot;bootstrap&quot;, bootstrap = 1000, fixed.x=FALSE) This requires the full dataset - need more than the covariance matrix. se = “bootstrap” requests bootstrap standard errors. bootstrap = 1000 requests 1000 bootstrap samples. Request bootstrap CI: summary(ex3Boot, ci = TRUE) ## lavaan 0.6-12 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 1339 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Bootstrap ## Number of requested bootstrap draws 1000 ## Number of successful bootstrap draws 1000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## DietSE ~ ## BMI (a) -0.100 0.026 -3.779 0.000 -0.152 -0.048 ## Bulimia ~ ## BMI (cPrm) 0.129 0.025 5.099 0.000 0.080 0.177 ## DietSE (b) -0.213 0.027 -7.820 0.000 -0.269 -0.159 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .DietSE 0.955 0.035 26.909 0.000 0.884 1.031 ## .Bulimia 0.968 0.038 25.378 0.000 0.890 1.041 ## BMI 1.073 0.044 24.482 0.000 0.991 1.165 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## ab 0.021 0.006 3.319 0.001 0.010 0.035 ## c 0.151 0.026 5.814 0.000 0.099 0.200 Now we have bootstrap standard error and percentile confidence interval for ab! 4.6.2 BC (bias-corrected) confidence interval What about other types of bootstrap confidence intervals? You can request a BC (bias-corrected) by adding an argument boot.ci.type = “bca.simple” to parameterEstimates(): parameterEstimates(ex3Boot, level = 0.95, boot.ci.type=&quot;bca.simple&quot;) ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 DietSE ~ BMI a -0.100 0.026 -3.779 0.000 -0.154 -0.048 ## 2 Bulimia ~ BMI cPrime 0.129 0.025 5.099 0.000 0.076 0.176 ## 3 Bulimia ~ DietSE b -0.213 0.027 -7.820 0.000 -0.264 -0.157 ## 4 DietSE ~~ DietSE 0.955 0.035 26.909 0.000 0.888 1.035 ## 5 Bulimia ~~ Bulimia 0.968 0.038 25.378 0.000 0.898 1.045 ## 6 BMI ~~ BMI 1.073 0.044 24.482 0.000 0.990 1.165 ## 7 ab := a*b ab 0.021 0.006 3.319 0.001 0.011 0.036 ## 8 c := cPrime+ab c 0.151 0.026 5.814 0.000 0.097 0.198 which returns a 95% BC confidence interval. This approach will yield similar results to the PROCESS Macro in SPSS with bias-corrected standard errors. 4.7 In-Class Exercise: Use Lavaan to estimate and interpret the following model ex4MediationSyntax &lt;- &quot; #Regressions DietSE ~ a*SelfEsteem Risk ~ cPrime*SelfEsteem + b*DietSE #Define New Parameters ab := a*b #the product term is computed as a*b c := cPrime + ab #having defined ab, we can use this here. &quot; ex4fit &lt;- lavaan::sem(model = ex4MediationSyntax, data = labData, fixed.x=FALSE) summary(ex4fit, ci = T) ## lavaan 0.6-12 ended normally after 1 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 1339 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## DietSE ~ ## SlfEstm (a) 0.105 0.027 3.949 0.000 0.053 0.158 ## Risk ~ ## SlfEstm (cPrm) -0.239 0.027 -8.728 0.000 -0.292 -0.185 ## DietSE (b) 0.100 0.028 3.583 0.000 0.045 0.154 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .DietSE 0.954 0.037 25.875 0.000 0.882 1.027 ## .Risk 0.991 0.038 25.875 0.000 0.916 1.066 ## SelfEsteem 1.001 0.039 25.875 0.000 0.926 1.077 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## ab 0.011 0.004 2.653 0.008 0.003 0.018 ## c -0.228 0.027 -8.352 0.000 -0.282 -0.175 Bootstrap confidence intervals: set.seed(2022) ex4Boot &lt;- lavaan::sem(model = ex4MediationSyntax, data = labData, se = &quot;bootstrap&quot;, bootstrap = 1000, fixed.x=FALSE) parameterEstimates(ex4Boot, level = 0.95, boot.ci.type=&quot;bca.simple&quot;, standardized = TRUE) ## lhs op rhs label est se z pvalue ci.lower ci.upper std.lv std.all std.nox ## 1 DietSE ~ SelfEsteem a 0.105 0.024 4.364 0.000 0.055 0.151 0.105 0.107 0.107 ## 2 Risk ~ SelfEsteem cPrime -0.239 0.026 -9.162 0.000 -0.291 -0.189 -0.239 -0.233 -0.233 ## 3 Risk ~ DietSE b 0.100 0.028 3.609 0.000 0.049 0.156 0.100 0.096 0.096 ## 4 DietSE ~~ DietSE 0.954 0.035 26.887 0.000 0.888 1.032 0.954 0.988 0.988 ## 5 Risk ~~ Risk 0.991 0.038 25.844 0.000 0.919 1.072 0.991 0.941 0.941 ## 6 SelfEsteem ~~ SelfEsteem 1.001 0.040 25.164 0.000 0.928 1.083 1.001 1.000 1.001 ## 7 ab := a*b ab 0.011 0.004 2.752 0.006 0.004 0.020 0.011 0.010 0.010 ## 8 c := cPrime+ab c -0.228 0.026 -8.693 0.000 -0.281 -0.176 -0.228 -0.223 -0.222 4.8 Exercise: Eating Disorder Mediation Analysis Give it a try before peaking the answers! Hints: Label the regression coefficients: b1 - b12; Fix all disturbance covariances at 0; Define mediation effects and total effects for each of the six mediation models using the labels; Request bootstrap standard errors using se = “bootstrap”; Print and interpret the mediation effects; (Optional) Identify and interpret the inconsistent mediation effects. I’ll get you started: 4.8.1 Step 1: Labeling and defining the parameters 4.8.2 Step 2: Fix all disturbance covariances at 0 ex5PathSyntax_noCov &lt;- &quot; #opening a quote DietSE ~ b1*BMI + b5*SelfEsteem #DietSE is predicted by BMI and SelfEsteem Bulimia ~ b10*DietSE + b2*BMI + b6*SelfEsteem Restrictive ~ b11*DietSE + b3*BMI + b7*SelfEsteem Risk ~ b12*DietSE + b4*BMI + b8*SelfEsteem + b9*Accu #Disturbance covariances (fixed at 0): DietSE ~~ 0*Bulimia # ~~ indicates a two-headed arrow (variance or covariance) DietSE ~~ 0*Restrictive # 0* in front of the 2nd variable fixes the covariance at 0 DietSE ~~ 0*Risk # These lines say that all endogenous variables have no correlated disturbance variances Bulimia ~~ 0*Restrictive Bulimia ~~ 0*Risk Restrictive ~~ 0*Risk &quot; 4.8.3 Step 3: Define new terms for mediation effects Recall: Define New Parameters ab := ab #the product term is computed as ab ex5MediationSyntax &lt;- &quot; DietSE ~ b1*BMI + b5*SelfEsteem #DietSE is predicted by BMI and SelfEsteem Bulimia ~ b10*DietSE + b2*BMI + b6*SelfEsteem Restrictive ~ b11*DietSE + b3*BMI + b7*SelfEsteem Risk ~ b12*DietSE + b4*BMI + b8*SelfEsteem + b9*Accu #Disturbance covariances (fixed at 0): DietSE ~~ 0*Bulimia # ~~ indicates a two-headed arrow (variance or covariance) DietSE ~~ 0*Restrictive # 0* in front of the 2nd variable fixes the covariance at 0 DietSE ~~ 0*Risk # These lines say that all endogenous variables have no correlated disturbance variances Bulimia ~~ 0*Restrictive Bulimia ~~ 0*Risk Restrictive ~~ 0*Risk #Define New Parameters med1 := b1*b10 total1 := b2 + med1 med2 := b1*b11 total2 := b3 + med2 med3 := b1*b12 total3 := b4 + med3 med4 := b5*b10 total4 := b6 + med4 med5 := b5*b11 total5 := b7 + med5 med6 := b5*b12 total6 := b8 + med6 # difference term for significance testing diff1 := med1 - med4 &quot; ex5fit &lt;- lavaan::sem(model = ex5MediationSyntax, data = labData, fixed.x=FALSE) summary(ex5fit, ci = T) 4.8.4 Step 4: Bootstrap confidence intervals: 4.8.5 Step 5: Print and interpret the mediation effects; set.seed(2022) ex5Boot &lt;- lavaan::sem(model = ex5MediationSyntax, data = labData, se = &quot;bootstrap&quot;, bootstrap = 1000, fixed.x=FALSE) parameterEstimates(ex5Boot, level = 0.95, boot.ci.type=&quot;bca.simple&quot;, standardized = TRUE) ## lhs op rhs label est se z pvalue ci.lower ci.upper std.lv std.all std.nox ## 1 DietSE ~ BMI b1 -0.088 0.026 -3.336 0.001 -0.141 -0.036 -0.088 -0.092 -0.089 ## 2 DietSE ~ SelfEsteem b5 0.093 0.024 3.862 0.000 0.044 0.139 0.093 0.095 0.095 ## 3 Bulimia ~ DietSE b10 -0.185 0.026 -7.001 0.000 -0.237 -0.128 -0.185 -0.179 -0.179 ## 4 Bulimia ~ BMI b2 0.096 0.025 3.844 0.000 0.047 0.142 0.096 0.097 0.094 ## 5 Bulimia ~ SelfEsteem b6 -0.284 0.025 -11.284 0.000 -0.338 -0.237 -0.284 -0.279 -0.279 ## 6 Restrictive ~ DietSE b11 -0.166 0.027 -6.192 0.000 -0.217 -0.109 -0.166 -0.162 -0.162 ## 7 Restrictive ~ BMI b3 -0.154 0.025 -6.173 0.000 -0.203 -0.107 -0.154 -0.158 -0.153 ## 8 Restrictive ~ SelfEsteem b7 -0.123 0.026 -4.668 0.000 -0.174 -0.071 -0.123 -0.122 -0.122 ## 9 Risk ~ DietSE b12 0.102 0.027 3.737 0.000 0.050 0.157 0.102 0.098 0.098 ## 10 Risk ~ BMI b4 0.053 0.026 2.037 0.042 0.003 0.104 0.053 0.053 0.052 ## 11 Risk ~ SelfEsteem b8 -0.228 0.026 -8.654 0.000 -0.281 -0.178 -0.228 -0.223 -0.223 ## 12 Risk ~ Accu b9 -0.095 0.028 -3.399 0.001 -0.151 -0.041 -0.095 -0.091 -0.092 ## 13 DietSE ~~ Bulimia 0.000 0.000 NA NA 0.000 0.000 0.000 0.000 0.000 ## 14 DietSE ~~ Restrictive 0.000 0.000 NA NA 0.000 0.000 0.000 0.000 0.000 ## 15 DietSE ~~ Risk 0.000 0.000 NA NA 0.000 0.000 0.000 0.000 0.000 ## 16 Bulimia ~~ Restrictive 0.000 0.000 NA NA 0.000 0.000 0.000 0.000 0.000 ## 17 Bulimia ~~ Risk 0.000 0.000 NA NA 0.000 0.000 0.000 0.000 0.000 ## 18 Restrictive ~~ Risk 0.000 0.000 NA NA 0.000 0.000 0.000 0.000 0.000 ## 19 DietSE ~~ DietSE 0.946 0.035 27.052 0.000 0.880 1.024 0.946 0.980 0.980 ## 20 Bulimia ~~ Bulimia 0.890 0.036 24.842 0.000 0.826 0.963 0.890 0.859 0.859 ## 21 Restrictive ~~ Restrictive 0.955 0.036 26.606 0.000 0.892 1.033 0.955 0.940 0.940 ## 22 Risk ~~ Risk 0.979 0.038 25.718 0.000 0.912 1.062 0.979 0.931 0.931 ## 23 BMI ~~ BMI 1.073 0.044 24.482 0.000 0.990 1.165 1.073 1.000 1.073 ## 24 BMI ~~ SelfEsteem -0.138 0.029 -4.802 0.000 -0.197 -0.083 -0.138 -0.133 -0.138 ## 25 BMI ~~ Accu -0.021 0.027 -0.755 0.450 -0.079 0.030 -0.021 -0.020 -0.021 ## 26 SelfEsteem ~~ SelfEsteem 1.001 0.040 25.164 0.000 0.928 1.083 1.001 1.000 1.001 ## 27 SelfEsteem ~~ Accu 0.035 0.027 1.304 0.192 -0.016 0.090 0.035 0.035 0.035 ## 28 Accu ~~ Accu 0.971 0.037 26.425 0.000 0.905 1.052 0.971 1.000 0.971 ## 29 med1 := b1*b10 med1 0.016 0.006 2.925 0.003 0.007 0.029 0.016 0.017 0.016 ## 30 total1 := b2+med1 total1 0.112 0.025 4.439 0.000 0.060 0.159 0.112 0.114 0.110 ## 31 med2 := b1*b11 med2 0.015 0.005 2.825 0.005 0.006 0.026 0.015 0.015 0.014 ## 32 total2 := b3+med2 total2 -0.139 0.025 -5.495 0.000 -0.192 -0.091 -0.139 -0.143 -0.138 ## 33 med3 := b1*b12 med3 -0.009 0.004 -2.536 0.011 -0.018 -0.003 -0.009 -0.009 -0.009 ## 34 total3 := b4+med3 total3 0.044 0.026 1.681 0.093 -0.005 0.096 0.044 0.044 0.043 ## 35 med4 := b5*b10 med4 -0.017 0.005 -3.399 0.001 -0.029 -0.009 -0.017 -0.017 -0.017 ## 36 total4 := b6+med4 total4 -0.301 0.026 -11.652 0.000 -0.356 -0.253 -0.301 -0.296 -0.296 ## 37 med5 := b5*b11 med5 -0.015 0.005 -3.238 0.001 -0.026 -0.007 -0.015 -0.015 -0.015 ## 38 total5 := b7+med5 total5 -0.139 0.027 -5.154 0.000 -0.191 -0.085 -0.139 -0.138 -0.138 ## 39 med6 := b5*b12 med6 0.010 0.004 2.641 0.008 0.004 0.018 0.010 0.009 0.009 ## 40 total6 := b8+med6 total6 -0.219 0.027 -8.227 0.000 -0.273 -0.168 -0.219 -0.213 -0.213 ## 41 diff1 := med1-med4 diff1 0.034 0.008 4.243 0.000 0.020 0.051 0.034 0.034 0.033 4.8.6 Plot it! library(semPlot) semPaths(ex5Boot, what=&#39;est&#39;, rotation = 2, # default rotation = 1 with four options curve = 2, # pull covariances&#39; curves out a little nCharNodes = 0, nCharEdges = 0, # don&#39;t limit variable name lengths sizeMan = 8, # font size of manifest variable names style = &quot;lisrel&quot;, # single-headed arrows vs. # &quot;ram&quot;&#39;s 2-headed for variances edge.label.cex=1.2, curvePivot = TRUE, fade=FALSE) "],["lavaan-lab-3-moderation-and-conditional-effects.html", "Chapter 5 Lavaan Lab 3: Moderation and Conditional Effects 5.1 Reading-In Datasets 5.2 Interactions in Regression Using lm() 5.3 Interactions in Lavaan 5.4 Visual inspection of interactions 5.5 Centering Continuous Moderator 5.6 Interactions in Lavaan (Continuous Moderator) 5.7 Simple Slopes Analysis 5.8 Visual inspection of interactions (lm approach) 5.9 JOHNSON-NEYMAN INTERVAL 5.10 Exercise: How Framing Affects Justifications for Giving or Withholding Aid to Disaster Victims", " Chapter 5 Lavaan Lab 3: Moderation and Conditional Effects In this lab, we will learn how to: how to perform moderation using regression and sem test the moderation effects of binary and continuous moderators visualize moderation effects. 5.1 Reading-In Datasets Let’s read this dataset in. Change the file path to whatever directory where you saved the file! cbtData &lt;- read.csv(file = &quot;dataInClass.csv&quot;, header = T) Let’s examine this dataset: head(cbtData) ## ID CBT CBTDummy NeedCog NegThoughts Depression NeedCogCont ## 1 1 CBT Treatment 1 0 -4.1453029 -5.802172 0.0182802 ## 2 2 Information Only 0 1 2.1775218 5.496665 1.4238703 ## 3 3 CBT Treatment 1 0 -1.5551349 -1.950566 -1.0151726 ## 4 4 Information Only 0 0 0.1679286 2.655801 -0.8547152 ## 5 5 Information Only 0 1 2.5103192 6.855488 0.6759705 ## 6 6 CBT Treatment 1 0 -3.1626670 -2.968198 -0.9123426 str(cbtData) ## &#39;data.frame&#39;: 1000 obs. of 7 variables: ## $ ID : int 1 2 3 4 5 6 7 8 9 10 ... ## $ CBT : chr &quot;CBT Treatment&quot; &quot;Information Only&quot; &quot;CBT Treatment&quot; &quot;Information Only&quot; ... ## $ CBTDummy : int 1 0 1 0 0 1 1 1 1 0 ... ## $ NeedCog : int 0 1 0 0 1 0 0 0 0 0 ... ## $ NegThoughts: num -4.145 2.178 -1.555 0.168 2.51 ... ## $ Depression : num -5.8 5.5 -1.95 2.66 6.86 ... ## $ NeedCogCont: num 0.0183 1.4239 -1.0152 -0.8547 0.676 ... colSums(is.na(cbtData)) ## ID CBT CBTDummy NeedCog NegThoughts Depression NeedCogCont ## 0 0 0 0 0 0 0 Notice that the first two columns are not model variables col 1 is a case ID variable. col 2 is a factor variable indicating CBT vs. Info-Only treatment. Besides, col 5 is a variable that measures negative thoughts. col 7 is a continuous measure of NeedCog. In the first part of this demo, we will work with three variables: CBTDummy, NeedCog, and Depression Let’s look at the covariance matrix of the three variables Multiple ways to accomplish this: cov(cbtData[,-c(1,2,5,7)]) ## CBTDummy NeedCog Depression ## CBTDummy 0.250250250 -0.008508509 -2.3372519 ## NeedCog -0.008508509 0.213732733 0.4371738 ## Depression -2.337251860 0.437173798 31.9301427 cov(cbtData[,c(3,4,6)]) ## CBTDummy NeedCog Depression ## CBTDummy 0.250250250 -0.008508509 -2.3372519 ## NeedCog -0.008508509 0.213732733 0.4371738 ## Depression -2.337251860 0.437173798 31.9301427 cov(cbtData[,c(&quot;CBTDummy&quot;, &quot;NeedCog&quot;, &quot;Depression&quot;)]) ## CBTDummy NeedCog Depression ## CBTDummy 0.250250250 -0.008508509 -2.3372519 ## NeedCog -0.008508509 0.213732733 0.4371738 ## Depression -2.337251860 0.437173798 31.9301427 cor(cbtData[,c(&quot;CBTDummy&quot;, &quot;NeedCog&quot;, &quot;Depression&quot;)]) ## CBTDummy NeedCog Depression ## CBTDummy 1.00000000 -0.03679007 -0.8268330 ## NeedCog -0.03679007 1.00000000 0.1673471 ## Depression -0.82683304 0.16734709 1.0000000 let’s round this to two decimals round(cov(cbtData[,c(&quot;CBTDummy&quot;, &quot;NeedCog&quot;, &quot;Depression&quot;)]), digits = 2) ## CBTDummy NeedCog Depression ## CBTDummy 0.25 -0.01 -2.34 ## NeedCog -0.01 0.21 0.44 ## Depression -2.34 0.44 31.93 What about the means? round(apply(cbtData[,c(&quot;CBTDummy&quot;, &quot;NeedCog&quot;, &quot;Depression&quot;)], 2, mean), 2) ## CBTDummy NeedCog Depression ## 0.50 0.31 -1.59 Although they are not centered, we will proceed because CBTDummy and NeedCog are both binary. 5.2 Interactions in Regression Using lm() In regression course we learned the lm() function, which stands for linear model. To include an interaction in regression, simply use an : to create a product in the formula: interactionModel &lt;- lm(formula = Depression ~ CBTDummy + NeedCog + CBTDummy:NeedCog, data = cbtData) NOTE: R is very helpful, in that if you just put an asterisk *, it includes all lower-order terms! interactionModel &lt;- lm(formula = Depression ~ CBTDummy*NeedCog, data = cbtData) Let’s look at this interaction model: summary(interactionModel) ## ## Call: ## lm(formula = Depression ~ CBTDummy * NeedCog, data = cbtData) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.7785 -1.4280 0.0662 1.6252 6.8283 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.2684 0.1345 9.428 &lt;2e-16 *** ## CBTDummy -6.8119 0.1880 -36.240 &lt;2e-16 *** ## NeedCog 5.5586 0.2356 23.590 &lt;2e-16 *** ## CBTDummy:NeedCog -8.0093 0.3384 -23.666 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.47 on 996 degrees of freedom ## Multiple R-squared: 0.8095, Adjusted R-squared: 0.809 ## F-statistic: 1411 on 3 and 996 DF, p-value: &lt; 2.2e-16 Let’s interpret this … (In class) 5.3 Interactions in Lavaan Now let us write the same model using lavaan. Load the package: library(lavaan) 5.3.1 IMPORTANT NOTE Because lavaan uses the * for assigning coefficient labels, this cannot be used to create interaction terms. Instead, we have to create the product term in the dataset first, before running our model. This is easy to do. General Format: existingDataFrame$variableName &lt;- vectorToBeAssignedAsNewVariable cbtData$CBTxNeedCog &lt;- cbtData$CBTDummy * cbtData$NeedCog You can name the product term arbitrarily: cbtData$fourth &lt;- cbtData$CBTDummy * cbtData$NeedCog Let’s look at cbtData again: head(cbtData) ## ID CBT CBTDummy NeedCog NegThoughts Depression NeedCogCont CBTxNeedCog ## 1 1 CBT Treatment 1 0 -4.1453029 -5.802172 0.0182802 0 ## 2 2 Information Only 0 1 2.1775218 5.496665 1.4238703 0 ## 3 3 CBT Treatment 1 0 -1.5551349 -1.950566 -1.0151726 0 ## 4 4 Information Only 0 0 0.1679286 2.655801 -0.8547152 0 ## 5 5 Information Only 0 1 2.5103192 6.855488 0.6759705 0 ## 6 6 CBT Treatment 1 0 -3.1626670 -2.968198 -0.9123426 0 Now you have a new variable called CBTxNeedCog at the end. 5.3.2 Follow the equation of Y (Depression): Depression = CBTDummy + NeedCog + CBTDummy*NeedCog + Disturbance Let’s write some model syntax (with the labels): interactionSyntax &lt;- &quot; #Regression with interaction #with labels Depression ~ b1*CBTDummy + b2*NeedCog + b3*CBTxNeedCog &quot; let fixed.x=FALSE to print more lines: inter_fit1 &lt;- lavaan::sem(model = interactionSyntax, data = cbtData, fixed.x = FALSE) If you’d like lavaan to print means and intercepts, we need to ask sem() to include the meanstructure: inter_fit1 &lt;- lavaan::sem(model = interactionSyntax, data = cbtData, fixed.x =FALSE, meanstructure = TRUE) summary(inter_fit1) ## lavaan 0.6-12 ended normally after 41 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 14 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## Depression ~ ## CBTDummy (b1) -6.812 0.188 -36.312 0.000 ## NeedCog (b2) 5.559 0.235 23.637 0.000 ## CBTxNedCg (b3) -8.009 0.338 -23.713 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## CBTDummy ~~ ## NeedCog -0.008 0.007 -1.163 0.245 ## CBTxNeedCog 0.073 0.006 12.083 0.000 ## NeedCog ~~ ## CBTxNeedCog 0.101 0.006 16.630 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .Depression 1.268 0.134 9.446 0.000 ## CBTDummy 0.500 0.016 31.623 0.000 ## NeedCog 0.309 0.015 21.147 0.000 ## CBTxNeedCog 0.146 0.011 13.075 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .Depression 6.076 0.272 22.361 0.000 ## CBTDummy 0.250 0.011 22.361 0.000 ## NeedCog 0.214 0.010 22.361 0.000 ## CBTxNeedCog 0.125 0.006 22.361 0.000 How does this compare to our regression model? summary(interactionModel) ## ## Call: ## lm(formula = Depression ~ CBTDummy * NeedCog, data = cbtData) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.7785 -1.4280 0.0662 1.6252 6.8283 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.2684 0.1345 9.428 &lt;2e-16 *** ## CBTDummy -6.8119 0.1880 -36.240 &lt;2e-16 *** ## NeedCog 5.5586 0.2356 23.590 &lt;2e-16 *** ## CBTDummy:NeedCog -8.0093 0.3384 -23.666 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.47 on 996 degrees of freedom ## Multiple R-squared: 0.8095, Adjusted R-squared: 0.809 ## F-statistic: 1411 on 3 and 996 DF, p-value: &lt; 2.2e-16 Same…but sem is more verbose. 5.4 Visual inspection of interactions One way to plot the interactions is to use the interact_plot() function on the lm() object. Install and load the package interactions first: library(interactions) interact_plot(interactionModel, pred = &quot;CBTDummy&quot;, modx = &quot;NeedCog&quot;) 5.5 Centering Continuous Moderator Now let’s work with the continuous measure of NeedCog directly: mean(cbtData$NeedCogCont) ## [1] 0.005925852 sd(cbtData$NeedCogCont) ## [1] 0.9974319 NeedCogCont has been standardized already, which is helpful. If not, we use scale() function to center a continuous variable Usage: scale(x, center = TRUE, scale = TRUE) If you just need to center a variable, you disable scale=FALSE centeredNeedCog &lt;- scale(cbtData$NeedCogCont, center = TRUE, scale = FALSE) hist(centeredNeedCog) For now, we will leave these variables as is in our dataset. But the scale() function is good to know. 5.6 Interactions in Lavaan (Continuous Moderator) Just like for binary NeedCog moderator, we have to manually create a product term in the dataset first before running our model. This is easy to do: cbtData$CBTxNeedCogCont &lt;- cbtData$CBTDummy * cbtData$NeedCogCont Let’s look at cbtData again: head(cbtData) ## ID CBT CBTDummy NeedCog NegThoughts Depression NeedCogCont CBTxNeedCog CBTxNeedCogCont ## 1 1 CBT Treatment 1 0 -4.1453029 -5.802172 0.0182802 0 0.0182802 ## 2 2 Information Only 0 1 2.1775218 5.496665 1.4238703 0 0.0000000 ## 3 3 CBT Treatment 1 0 -1.5551349 -1.950566 -1.0151726 0 -1.0151726 ## 4 4 Information Only 0 0 0.1679286 2.655801 -0.8547152 0 0.0000000 ## 5 5 Information Only 0 1 2.5103192 6.855488 0.6759705 0 0.0000000 ## 6 6 CBT Treatment 1 0 -3.1626670 -2.968198 -0.9123426 0 -0.9123426 Time to write some lavaan model syntax (with labels): interactionSyntax2 &lt;- &quot; #Regression Depression ~ b1*CBTDummy + b2*NeedCogCont + b3*CBTxNeedCogCont &quot; Let’s ask sem() to include the meanstructure: inter_fit2 &lt;- lavaan::sem(model = interactionSyntax2, data = cbtData, fixed.x =FALSE, meanstructure = TRUE) summary(inter_fit2) ## lavaan 0.6-12 ended normally after 35 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 14 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## Depression ~ ## CBTDummy (b1) -9.245 0.113 -82.096 0.000 ## NeedCgCnt (b2) 3.305 0.079 42.064 0.000 ## CBTxNdCgC (b3) -4.967 0.113 -43.943 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## CBTDummy ~~ ## NeedCogCont -0.020 0.016 -1.249 0.212 ## CBTxNeedCogCnt -0.008 0.011 -0.764 0.445 ## NeedCogCont ~~ ## CBTxNeedCogCnt 0.480 0.027 18.055 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .Depression 2.931 0.080 36.794 0.000 ## CBTDummy 0.500 0.016 31.623 0.000 ## NeedCogCont 0.006 0.032 0.188 0.851 ## CBTxNeedCogCnt -0.017 0.022 -0.764 0.445 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .Depression 3.166 0.142 22.361 0.000 ## CBTDummy 0.250 0.011 22.361 0.000 ## NeedCogCont 0.994 0.044 22.361 0.000 ## CBTxNeedCogCnt 0.480 0.021 22.361 0.000 5.7 Simple Slopes Analysis pick-a-point (Rogosa, 1980) and plot the simple slopes of X at designated levels of Z: mean(cbtData$NeedCogCont) #0 ## [1] 0.005925852 sd(cbtData$NeedCogCont) # almost 1 ## [1] 0.9974319 mean(cbtData$NeedCogCont) - sd(cbtData$NeedCogCont) # 1sd below the mean ## [1] -0.991506 mean(cbtData$NeedCogCont) + sd(cbtData$NeedCogCont) # 1sd above the mean ## [1] 1.003358 interactionSyntax3 &lt;- &quot; #Regression Depression ~ b1*CBTDummy + b2*NeedCogCont + b3*CBTxNeedCogCont #regression coefficient labels #Simple Slopes SSHigh := b1+b3*1 #Since sd(NeedCogCont) = approximately 1, this is +1 SD SSMod := b1+b3*0 #at the mean of (centered) NeedCogCont SSLow := b1+b3*(-1) #Low Simple Slope is at -1 (1 SD below since SD = 1) &quot; inter_fit3 &lt;- lavaan::sem(model = interactionSyntax3, data = cbtData, fixed.x =FALSE, meanstructure = TRUE) summary(inter_fit3) ## lavaan 0.6-12 ended normally after 35 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 14 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## Depression ~ ## CBTDummy (b1) -9.245 0.113 -82.096 0.000 ## NeedCgCnt (b2) 3.305 0.079 42.064 0.000 ## CBTxNdCgC (b3) -4.967 0.113 -43.943 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## CBTDummy ~~ ## NeedCogCont -0.020 0.016 -1.249 0.212 ## CBTxNeedCogCnt -0.008 0.011 -0.764 0.445 ## NeedCogCont ~~ ## CBTxNeedCogCnt 0.480 0.027 18.055 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .Depression 2.931 0.080 36.794 0.000 ## CBTDummy 0.500 0.016 31.623 0.000 ## NeedCogCont 0.006 0.032 0.188 0.851 ## CBTxNeedCogCnt -0.017 0.022 -0.764 0.445 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .Depression 3.166 0.142 22.361 0.000 ## CBTDummy 0.250 0.011 22.361 0.000 ## NeedCogCont 0.994 0.044 22.361 0.000 ## CBTxNeedCogCnt 0.480 0.021 22.361 0.000 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ## SSHigh -14.212 0.159 -89.281 0.000 ## SSMod -9.245 0.113 -82.096 0.000 ## SSLow -4.279 0.160 -26.755 0.000 Now we have tests of the simple slopes at low, moderate, and high values of the moderator! Along with significance tests. 5.8 Visual inspection of interactions (lm approach) Interactions in Regression Using lm() To include ab interaction in regression, simply use an * to create a product in the formula. interactionModel2 &lt;- lm(Depression ~ CBTDummy*NeedCogCont, cbtData) summary(interactionModel2) ## ## Call: ## lm(formula = Depression ~ CBTDummy * NeedCogCont, data = cbtData) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.7282 -1.1381 0.0649 1.2229 4.4762 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.93065 0.07981 36.72 &lt;2e-16 *** ## CBTDummy -9.24546 0.11284 -81.93 &lt;2e-16 *** ## NeedCogCont 3.30525 0.07873 41.98 &lt;2e-16 *** ## CBTDummy:NeedCogCont -4.96674 0.11325 -43.85 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.783 on 996 degrees of freedom ## Multiple R-squared: 0.9008, Adjusted R-squared: 0.9005 ## F-statistic: 3013 on 3 and 996 DF, p-value: &lt; 2.2e-16 pick-a-point (Rogosa, 1980) and plot the simple slopes of X at designated levels of Z: library(interactions) interact_plot(interactionModel2, pred = &quot;CBTDummy&quot;, modx = &quot;NeedCogCont&quot;) 5.9 JOHNSON-NEYMAN INTERVAL interactions::johnson_neyman(interactionModel2, pred = &quot;CBTDummy&quot;, modx = &quot;NeedCogCont&quot;, alpha = 0.05) ## JOHNSON-NEYMAN INTERVAL ## ## When NeedCogCont is OUTSIDE the interval [-1.96, -1.77], the slope of CBTDummy is p &lt; .05. ## ## Note: The range of observed values of NeedCogCont is [-2.83, 3.31] 5.10 Exercise: How Framing Affects Justifications for Giving or Withholding Aid to Disaster Victims For this exercise, we will use a real dataset in a study by Chapman and Lickel (2016). This study was interested in examining the relation between Climate Change and Disasters: How Framing Affects Justifications for Giving or Withholding Aid to Disaster Victims? Researchers hypothesizes that Framing a natural disaster as the product of climate change impacts attitudes toward disaster victims and humanitarian relief. The predictor is X/Frame: Participants read a story about a humanitarian crisis caused by a drought in Africa. X = 1: Half of the participants were told that the drought was caused by climate change (the climate change condition) X = 0: The other half were not told anything about the specific cause of the drought and thus had no reason to believe it wasn’t the result of natural causes (the natural causes condition). The outcome is Y/Donate: the participants’ willingness to donate to the victims was assessed using a set of questions. Responses were made on a set of 7-point scales, with higher scores reflecting a greater willingness to donate to the victims The moderator is W/Skeptic: The belief whether climate change is a real phenomenon was also measured. The moderation model looks at whether the attribution frame manipulation (X) might have had a different effect on people’s willingness to donate (Y) depending on their climate change skepticism (M) 5.10.1 Data Prep The following example data are from Chapman and Lickel (2016) Also example data in Chapter 12 of Hayes (2017) Simply load the .rda into R: load(&quot;disaster.rda&quot;) head(disaster) ## id frame donate justify skeptic ## 1 1 1 5.6 2.95 1.8 ## 2 2 1 4.2 2.85 5.2 ## 3 3 1 4.2 3.00 3.2 ## 4 4 1 4.6 3.30 1.0 ## 5 5 1 3.0 5.00 7.6 ## 6 6 0 5.0 3.20 4.2 str(disaster) ## &#39;data.frame&#39;: 211 obs. of 5 variables: ## $ id : num 1 2 3 4 5 6 7 8 9 10 ... ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.0&quot; ## ..- attr(*, &quot;display_width&quot;)= int 6 ## $ frame : num 1 1 1 1 1 0 0 1 0 0 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Experimental condition&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.2&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:2] 0 1 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;naturally caused disaster&quot; &quot;climate change caused disaster&quot; ## $ donate : num 5.6 4.2 4.2 4.6 3 5 4.8 6 4.2 4.4 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Positive attitudes toward donating&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.2&quot; ## ..- attr(*, &quot;display_width&quot;)= int 9 ## $ justify: num 2.95 2.85 3 3.3 5 3.2 2.9 1.4 3.25 3.55 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Negative justifications&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.2&quot; ## ..- attr(*, &quot;display_width&quot;)= int 10 ## $ skeptic: num 1.8 5.2 3.2 1 7.6 4.2 4.2 1.2 1.8 8.8 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Climate change skepticism&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.2&quot; If you are able to install package processR, you can also view its help page: install.packages(&quot;processR&quot;) library(processR) data(disaster) # take a look at the dataset: ?disaster You probably have to go to https://www.xquartz.org/ to download and install X11, which is a server required by many R packages, including processR. Now, disaster is a data.frame with 211 obs. of 5 variables: id frame: Experimental condition. 0 = naturally caused disaster, 1 = climate change caused disaster donate: Positive attitudes toward donating justify: Negative justifications skeptic: Climate change skepticism 5.10.2 Moderation with a binary moderator Let me first manually create a binary moderator based on the continuous version of skeptic: disaster$skeptic_b &lt;- ifelse(disaster$skeptic&lt;3, 0, 1) # low and high levels of skeptism of climate change table(disaster$skeptic_b) ## ## 0 1 ## 112 99 Next, can you test the moderation effect of skeptic_b on the path from frame to donate? (you can use either lm or lavaan) Please interpret the coefficients in the model above and visualize the interaction using interact_plot(). "],["lavaan-lab-4-mediated-moderation-moderated-mediation.html", "Chapter 6 Lavaan Lab 4: Mediated Moderation &amp; Moderated Mediation 6.1 PART 1: Mediated Moderation (Indirect Conditional effect) 6.2 PART 2: Moderated Mediation (Conditional Indirect effect)", " Chapter 6 Lavaan Lab 4: Mediated Moderation &amp; Moderated Mediation In this lab, we will learn how to: Estimate the mediated moderation model Estimate the moderated mediation model Bootstrap the effects Conduct simple slope analyses 6.1 PART 1: Mediated Moderation (Indirect Conditional effect) 6.1.1 Step 1: Read-in Data Imagine that we extended our CBT study by adding a mediator: the average number of daily negative thoughts reported at the end of six weeks. The hypothesis we will test is that NegThoughts mediates the CBT*NeedCog -&gt; Depression path Let’s read this dataset in: cbtData &lt;- read.csv(file = &quot;dataInClass.csv&quot;, header = T, sep = &#39;,&#39;) This time we work with the continuous version of the moderator: NeedCogCont. Let’s examine their means and standard deviations: apply(cbtData[,-c(1,2)], 2, mean) ## CBTDummy NeedCog NegThoughts Depression NeedCogCont ## 0.500000000 0.309000000 -1.944249996 -1.589356290 0.005925852 apply(cbtData[,-c(1,2)], 2, sd) ## CBTDummy NeedCog NegThoughts Depression NeedCogCont ## 0.5002502 0.4623124 3.1877174 5.6506763 0.9974319 Why dropping the first two variables? The first two variables ID and CBT are not numerical and have no means. NeedCogCont has been standardized already, which is helpful. If not, don’t forget to use scale() function to center the continuous variables. 6.1.2 Step 2: Create the interaction term for Moderation Analysis To test the moderation effect, we have to manually create a product term in the dataset before running our model: cbtData$CBTxNeedCogCont &lt;- cbtData$CBTDummy * cbtData$NeedCogCont Let’s look at cbtData again: head(cbtData) ## ID CBT CBTDummy NeedCog NegThoughts Depression NeedCogCont CBTxNeedCogCont ## 1 1 CBT Treatment 1 0 -4.1453029 -5.802172 0.0182802 0.0182802 ## 2 2 Information Only 0 1 2.1775218 5.496665 1.4238703 0.0000000 ## 3 3 CBT Treatment 1 0 -1.5551349 -1.950566 -1.0151726 -1.0151726 ## 4 4 Information Only 0 0 0.1679286 2.655801 -0.8547152 0.0000000 ## 5 5 Information Only 0 1 2.5103192 6.855488 0.6759705 0.0000000 ## 6 6 CBT Treatment 1 0 -3.1626670 -2.968198 -0.9123426 -0.9123426 6.1.3 Step 3: Write the syntax and Fit the model load the package: library(lavaan) Follow the two equations to write the model syntax: ex1MedModerationBasic &lt;- &quot; # label the coefficients: NegThoughts ~ a_m1*CBTxNeedCogCont + a_m2*NeedCogCont + a_m3*CBTDummy Depression ~ b1*CBTDummy + b2*NeedCogCont + b3*CBTxNeedCogCont + bM*NegThoughts #Define New Parameter Using := #Mediated Moderation effect MedMod_ab := a_m1*bM TotalMod := MedMod_ab + b3 &quot; Since we included the intercept term, we need to ask sem() to include the meanstructure: ex1fit &lt;- lavaan::sem(model = ex1MedModerationBasic, data = cbtData, fixed.x = FALSE, meanstructure = TRUE) summary(ex1fit, ci = T) ## lavaan 0.6-12 ended normally after 40 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 20 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## NegThoughts ~ ## CBTxNCC (a_m1) -3.071 0.064 -48.104 0.000 -3.196 -2.945 ## NdCgCnt (a_m2) 2.000 0.044 45.075 0.000 1.913 2.087 ## CBTDmmy (a_m3) -5.060 0.064 -79.562 0.000 -5.185 -4.936 ## Depression ~ ## CBTDmmy (b1) -1.240 0.137 -9.055 0.000 -1.509 -0.972 ## NdCgCnt (b2) 0.141 0.061 2.292 0.022 0.020 0.261 ## CBTxNCC (b3) -0.109 0.092 -1.180 0.238 -0.290 0.072 ## NgThght (bM) 1.582 0.025 62.899 0.000 1.533 1.631 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## CBTxNeedCogCont ~~ ## NeedCogCont 0.480 0.027 18.055 0.000 0.428 0.532 ## CBTDummy -0.008 0.011 -0.764 0.445 -0.030 0.013 ## NeedCogCont ~~ ## CBTDummy -0.020 0.016 -1.249 0.212 -0.051 0.011 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .NegThoughts 0.523 0.045 11.618 0.000 0.434 0.611 ## .Depression 2.104 0.038 55.199 0.000 2.029 2.179 ## CBTxNeedCogCnt -0.017 0.022 -0.764 0.445 -0.060 0.026 ## NeedCogCont 0.006 0.032 0.188 0.851 -0.056 0.068 ## CBTDummy 0.500 0.016 31.623 0.000 0.469 0.531 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .NegThoughts 1.010 0.045 22.361 0.000 0.921 1.098 ## .Depression 0.639 0.029 22.361 0.000 0.583 0.695 ## CBTxNeedCogCnt 0.480 0.021 22.361 0.000 0.438 0.522 ## NeedCogCont 0.994 0.044 22.361 0.000 0.907 1.081 ## CBTDummy 0.250 0.011 22.361 0.000 0.228 0.272 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## MedMod_ab -4.858 0.127 -38.211 0.000 -5.107 -4.609 ## TotalMod -4.967 0.113 -43.943 0.000 -5.188 -4.745 Are we done? 6.1.4 Step 4: Bootstrap Version We need to request Bootstrap because this involves testing a mediation effect MedMod_ab. Remember to set a seed: set.seed(2022) ex1Boot &lt;- lavaan::sem(model = ex1MedModerationBasic, data = cbtData, fixed.x = FALSE, meanstructure = TRUE, se = &quot;bootstrap&quot;, bootstrap = 1000) This requires the full dataset - need more than the covariance matrix. se = “bootstrap” requests bootstrap standard errors. bootstrap = 1000 requests 1000 bootstrap samples. Request BC confidence interval: parameterEstimates(ex1Boot, level = 0.95, boot.ci.type=&quot;bca.simple&quot;, standardized = TRUE) ## Warning in norm.inter(t, adj.alpha): extreme order statistics used as endpoints ## lhs op rhs label est se z pvalue ci.lower ci.upper std.lv std.all std.nox ## 1 NegThoughts ~ CBTxNeedCogCont a_m1 -3.071 0.067 -45.675 0.000 -3.195 -2.922 -3.071 -0.668 -0.964 ## 2 NegThoughts ~ NeedCogCont a_m2 2.000 0.048 42.047 0.000 1.902 2.093 2.000 0.626 0.628 ## 3 NegThoughts ~ CBTDummy a_m3 -5.060 0.064 -78.764 0.000 -5.183 -4.930 -5.060 -0.794 -1.588 ## 4 Depression ~ CBTDummy b1 -1.240 0.133 -9.337 0.000 -1.493 -0.979 -1.240 -0.110 -0.220 ## 5 Depression ~ NeedCogCont b2 0.141 0.059 2.379 0.017 0.020 0.256 0.141 0.025 0.025 ## 6 Depression ~ CBTxNeedCogCont b3 -0.109 0.088 -1.232 0.218 -0.270 0.071 -0.109 -0.013 -0.019 ## 7 Depression ~ NegThoughts bM 1.582 0.024 65.833 0.000 1.539 1.632 1.582 0.892 0.892 ## 8 NegThoughts ~~ NegThoughts 1.010 0.042 23.885 0.000 0.937 1.109 1.010 0.099 0.099 ## 9 Depression ~~ Depression 0.639 0.030 21.077 0.000 0.585 0.708 0.639 0.020 0.020 ## 10 CBTxNeedCogCont ~~ CBTxNeedCogCont 0.480 0.034 14.292 0.000 0.411 0.545 0.480 1.000 0.480 ## 11 CBTxNeedCogCont ~~ NeedCogCont 0.480 0.034 14.314 0.000 0.412 0.545 0.480 0.695 0.480 ## 12 CBTxNeedCogCont ~~ CBTDummy -0.008 0.011 -0.781 0.435 -0.030 0.013 -0.008 -0.024 -0.008 ## 13 NeedCogCont ~~ NeedCogCont 0.994 0.044 22.749 0.000 0.909 1.078 0.994 1.000 0.994 ## 14 NeedCogCont ~~ CBTDummy -0.020 0.016 -1.258 0.208 -0.051 0.013 -0.020 -0.040 -0.020 ## 15 CBTDummy ~~ CBTDummy 0.250 0.000 619.384 0.000 0.250 0.250 0.250 1.000 0.250 ## 16 NegThoughts ~1 0.523 0.046 11.401 0.000 0.436 0.616 0.523 0.164 0.164 ## 17 Depression ~1 2.104 0.038 54.754 0.000 2.029 2.177 2.104 0.373 0.373 ## 18 CBTxNeedCogCont ~1 -0.017 0.021 -0.781 0.435 -0.059 0.026 -0.017 -0.024 -0.017 ## 19 NeedCogCont ~1 0.006 0.031 0.190 0.849 -0.062 0.063 0.006 0.006 0.006 ## 20 CBTDummy ~1 0.500 0.016 30.669 0.000 0.466 0.528 0.500 1.000 0.500 ## 21 MedMod_ab := a_m1*bM MedMod_ab -4.858 0.125 -38.863 0.000 -5.089 -4.602 -4.858 -0.596 -0.860 ## 22 TotalMod := MedMod_ab+b3 TotalMod -4.967 0.118 -42.112 0.000 -5.195 -4.720 -4.967 -0.609 -0.879 Warning message: In norm.inter(t, adj.alpha) : extreme order statistics used as endpoints https://rcompanion.org/handbook/E_04.html The BCa (bias corrected, accelerated) is often cited as the best for theoretical reasons. The percentile method is also cited as typically good. However, if you get the “extreme order statistics used as endpoints” warning message, use a different test. For small data sets, the interval from BCa may be wider than for some other methods. 6.1.5 Step 5: Effect size measures Measure 1: Completely Standardized Indirect Effect (CSIE) beta_a_m1 = -0.668 beta_bM = 0.892 es1 = beta_a_m1*beta_bM es1 ## [1] -0.595856 According to Cohen, .01-.09 is small, .10-.25 is medium, and .25 + is large This is a large mediation effect Measure 2: Use unstandardized parameter estimates: TotalMod = -4.967 MedMod_ab = -4.858 prop = MedMod_ab/TotalMod #97.8% prop ## [1] 0.9780552 b3 = -0.109 # pvalue=0.218 # nonsig Mediated% = indirect effect / total effect = ab / c Consistent mediation Complete mediation as the remaining direct effect is nonsig and prop &gt; 80% 6.2 PART 2: Moderated Mediation (Conditional Indirect effect) In this lab, we’ll test this first-stage moderated mediation model in which NeedCog moderates the CBT -&gt; NegThoughts path 6.2.1 Step 1: Product Term We already have the product term in the dataset: cbtData$CBTxNeedCogCont &lt;- cbtData$CBTDummy * cbtData$NeedCogCont If NeedCog moderates the NegThoughts -&gt; Depression path, then we center NegThoughts and create a product term between centered NegThoughts*NeedCogCont (making sense?) 6.2.2 Step 2: Write the syntax and Fit the model ex2ModMediationBasic &lt;- &quot; NegThoughts ~ a1*CBTDummy + a2*NeedCogCont + a3*CBTxNeedCogCont Depression ~ b*NegThoughts + cprime*CBTDummy &quot; We’ll need to define the Index of Moderated Mediation in the syntax: ex2ModMediation &lt;- &quot; #Regressions NegThoughts ~ a1*CBTDummy + a2*NeedCogCont + a3*CBTxNeedCogCont Depression ~ b*NegThoughts + cprime*CBTDummy #Index of Moderated Mediation IndexOfModMed := a3*b &quot; ex2ModMediationBasic &lt;- &quot; NegThoughts ~ a1*CBTDummy + a2*NeedCogCont + a3*CBTxNeedCogCont Depression ~ b*NegThoughts + cprime*CBTDummy &quot; # NegThoughts ~ CBTDummy:NeedCogCont 6.2.3 Step 3: Bootstrap Version Since this model involves tests of indirect effects let’s jump to the bootstrap test: set.seed(2022) ex2Boot &lt;- lavaan::sem(model = ex2ModMediation, data = cbtData, fixed.x = FALSE, meanstructure = TRUE, se = &quot;bootstrap&quot;, bootstrap = 1000) You can further request a BC (bias-corrected) by adding an argument boot.ci.type = “bca.simple” to parameterEstimates(): parameterEstimates(ex2Boot, level = 0.95, boot.ci.type=&quot;bca.simple&quot;, standardized = TRUE) ## Warning in norm.inter(t, adj.alpha): extreme order statistics used as endpoints ## lhs op rhs label est se z pvalue ci.lower ci.upper std.lv std.all std.nox ## 1 NegThoughts ~ CBTDummy a1 -5.060 0.064 -78.764 0.000 -5.183 -4.930 -5.060 -0.794 -1.588 ## 2 NegThoughts ~ NeedCogCont a2 2.000 0.048 42.047 0.000 1.902 2.093 2.000 0.626 0.628 ## 3 NegThoughts ~ CBTxNeedCogCont a3 -3.071 0.067 -45.675 0.000 -3.195 -2.922 -3.071 -0.668 -0.964 ## 4 Depression ~ NegThoughts b 1.617 0.013 124.544 0.000 1.592 1.642 1.617 0.912 0.912 ## 5 Depression ~ CBTDummy cprime -1.066 0.084 -12.758 0.000 -1.216 -0.899 -1.066 -0.094 -0.189 ## 6 NegThoughts ~~ NegThoughts 1.010 0.042 23.885 0.000 0.937 1.109 1.010 0.099 0.099 ## 7 Depression ~~ Depression 0.645 0.031 21.047 0.000 0.592 0.714 0.645 0.020 0.020 ## 8 CBTDummy ~~ CBTDummy 0.250 0.000 619.383 0.000 0.250 0.250 0.250 1.000 0.250 ## 9 CBTDummy ~~ NeedCogCont -0.020 0.016 -1.258 0.208 -0.051 0.013 -0.020 -0.040 -0.020 ## 10 CBTDummy ~~ CBTxNeedCogCont -0.008 0.011 -0.781 0.435 -0.030 0.013 -0.008 -0.024 -0.008 ## 11 NeedCogCont ~~ NeedCogCont 0.994 0.044 22.749 0.000 0.909 1.078 0.994 1.000 0.994 ## 12 NeedCogCont ~~ CBTxNeedCogCont 0.480 0.034 14.314 0.000 0.412 0.545 0.480 0.695 0.480 ## 13 CBTxNeedCogCont ~~ CBTxNeedCogCont 0.480 0.034 14.292 0.000 0.411 0.545 0.480 1.000 0.480 ## 14 NegThoughts ~1 0.523 0.046 11.401 0.000 0.436 0.616 0.523 0.164 0.164 ## 15 Depression ~1 2.089 0.037 56.930 0.000 2.021 2.160 2.089 0.370 0.370 ## 16 CBTDummy ~1 0.500 0.016 30.669 0.000 0.467 0.529 0.500 1.000 0.500 ## 17 NeedCogCont ~1 0.006 0.031 0.190 0.849 -0.062 0.063 0.006 0.006 0.006 ## 18 CBTxNeedCogCont ~1 -0.017 0.021 -0.781 0.435 -0.059 0.026 -0.017 -0.024 -0.017 ## 19 IndexOfModMed := a3*b IndexOfModMed -4.967 0.114 -43.431 0.000 -5.180 -4.727 -4.967 -0.609 -0.879 Defined Parameters: Estimate Std.Err ci.lower ci.upper std.all IndexOfModMed -4.967 0.114 -5.180 -4.727 -0.609 NeedCogCont significantly moderates CBT -&gt; NegThoughts -&gt; Depression indirect effect through moderating the first stage of the indirect effect Since we expect the effect of CBT on Depression to be negative (CBT reduces Depression) And IndexOfModMed is also negative We’ll say NeedCogCont strengthens the indirect effect of CBT on Depression through NegThoughts The higher need for cognition, the stronger the indirect effect, and the more effect mediated by NegThoughts 6.2.4 Step 4: Simple Slopes As a follow-up analysis to a significant moderation effect, we conduct simple slope anlaysis: Let’s use pick-a-point (Rogosa, 1980) and plot the indirect effects at designated levels of NeedCogCont: mean(cbtData$NeedCogCont) #0 ## [1] 0.005925852 sd(cbtData$NeedCogCont) # 1 ## [1] 0.9974319 Three representative levels: mean(cbtData$NeedCogCont) - sd(cbtData$NeedCogCont) # -1 ## [1] -0.991506 mean(cbtData$NeedCogCont) #0 ## [1] 0.005925852 mean(cbtData$NeedCogCont) + sd(cbtData$NeedCogCont) # 1 ## [1] 1.003358 let’s define the Conditional Indirect Effects in the syntax: ex3ModMediation &lt;- &quot; #Regressions NegThoughts ~ a1*CBTDummy + a2*NeedCogCont + a3*CBTxNeedCogCont Depression ~ b*NegThoughts + cprime*CBTDummy #Index of Moderated Mediation IndexOfModMed := a3*b #Simple Slopes aSSLow := a1+a3*(-1) aSSMean := a1+a3*0 aSSHigh := a1+a3*1 #Conditional Indirect Effects abLow := aSSLow*b abMean := aSSMean*b abHigh := aSSHigh*b &quot; set.seed(2022) ex3Boot &lt;- lavaan::sem(model = ex3ModMediation, data = cbtData, se = &quot;bootstrap&quot;, bootstrap = 1000, fixed.x=FALSE, meanstructure = TRUE) parameterEstimates(ex3Boot, level = 0.95, boot.ci.type=&quot;bca.simple&quot;, standardized = TRUE) ## Warning in norm.inter(t, adj.alpha): extreme order statistics used as endpoints ## lhs op rhs label est se z pvalue ci.lower ci.upper std.lv std.all std.nox ## 1 NegThoughts ~ CBTDummy a1 -5.060 0.064 -78.764 0.000 -5.183 -4.930 -5.060 -0.794 -1.588 ## 2 NegThoughts ~ NeedCogCont a2 2.000 0.048 42.047 0.000 1.902 2.093 2.000 0.626 0.628 ## 3 NegThoughts ~ CBTxNeedCogCont a3 -3.071 0.067 -45.675 0.000 -3.195 -2.922 -3.071 -0.668 -0.964 ## 4 Depression ~ NegThoughts b 1.617 0.013 124.544 0.000 1.592 1.642 1.617 0.912 0.912 ## 5 Depression ~ CBTDummy cprime -1.066 0.084 -12.758 0.000 -1.216 -0.899 -1.066 -0.094 -0.189 ## 6 NegThoughts ~~ NegThoughts 1.010 0.042 23.885 0.000 0.937 1.109 1.010 0.099 0.099 ## 7 Depression ~~ Depression 0.645 0.031 21.047 0.000 0.592 0.714 0.645 0.020 0.020 ## 8 CBTDummy ~~ CBTDummy 0.250 0.000 619.383 0.000 0.250 0.250 0.250 1.000 0.250 ## 9 CBTDummy ~~ NeedCogCont -0.020 0.016 -1.258 0.208 -0.051 0.013 -0.020 -0.040 -0.020 ## 10 CBTDummy ~~ CBTxNeedCogCont -0.008 0.011 -0.781 0.435 -0.030 0.013 -0.008 -0.024 -0.008 ## 11 NeedCogCont ~~ NeedCogCont 0.994 0.044 22.749 0.000 0.909 1.078 0.994 1.000 0.994 ## 12 NeedCogCont ~~ CBTxNeedCogCont 0.480 0.034 14.314 0.000 0.412 0.545 0.480 0.695 0.480 ## 13 CBTxNeedCogCont ~~ CBTxNeedCogCont 0.480 0.034 14.292 0.000 0.411 0.545 0.480 1.000 0.480 ## 14 NegThoughts ~1 0.523 0.046 11.401 0.000 0.436 0.616 0.523 0.164 0.164 ## 15 Depression ~1 2.089 0.037 56.930 0.000 2.021 2.160 2.089 0.370 0.370 ## 16 CBTDummy ~1 0.500 0.016 30.669 0.000 0.467 0.529 0.500 1.000 0.500 ## 17 NeedCogCont ~1 0.006 0.031 0.190 0.849 -0.062 0.063 0.006 0.006 0.006 ## 18 CBTxNeedCogCont ~1 -0.017 0.021 -0.781 0.435 -0.059 0.026 -0.017 -0.024 -0.017 ## 19 IndexOfModMed := a3*b IndexOfModMed -4.967 0.114 -43.431 0.000 -5.180 -4.727 -4.967 -0.609 -0.879 ## 20 aSSLow := a1+a3*(-1) aSSLow -1.990 0.092 -21.679 0.000 -2.176 -1.813 -1.990 -0.126 -0.624 ## 21 aSSMean := a1+a3*0 aSSMean -5.060 0.064 -78.724 0.000 -5.183 -4.930 -5.060 -0.794 -1.588 ## 22 aSSHigh := a1+a3*1 aSSHigh -8.131 0.094 -86.243 0.000 -8.313 -7.942 -8.131 -1.462 -2.552 ## 23 abLow := aSSLow*b abLow -3.218 0.151 -21.338 0.000 -3.532 -2.941 -3.218 -0.115 -0.570 ## 24 abMean := aSSMean*b abMean -8.185 0.121 -67.625 0.000 -8.433 -7.944 -8.185 -0.725 -1.449 ## 25 abHigh := aSSHigh*b abHigh -13.152 0.181 -72.721 0.000 -13.513 -12.779 -13.152 -1.334 -2.329 Defined Parameters: Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper std.all IndexOfModMed -4.967 0.114 -43.431 0.000 -5.180 -4.727 -0.609 aSSLow -1.990 0.092 -21.679 0.000 -2.176 -1.813 -0.126 aSSMean -5.060 0.064 -78.724 0.000 -5.183 -4.930 -0.794 aSSHigh -8.131 0.094 -86.243 0.000 -8.313 -7.942 -1.462 abLow -3.218 0.151 -21.338 0.000 -3.532 -2.941 -0.115 abMean -8.185 0.121 -67.625 0.000 -8.433 -7.944 -0.725 abHigh -13.152 0.181 -72.721 0.000 -13.513 -12.779 -1.334 b 1.617 0.013 124.544 0.000 1.592 1.642 0.912 cprime -1.066 0.084 -12.758 0.000 -1.216 -0.899 -0.094 What does a1 tell you? What does a2 tell you? What does a3 tell you? What does IndexOfModMed tell you? What does aSSLow tell you? What does aSSMean tell you? What does aSSHigh tell you? What does b tell you? What does abLow tell you? What does abMean tell you? What does abHigh tell you? What does cprime tell you? the simple slopes of CBT -&gt; NegThoughts (a path) are all negative at three levels of the moderator the indirect effects of CBT -&gt; NegThoughts -&gt; Depression (ab) are all negative at three levels of the moderator 6.2.5 Step 5 JOHNSON-NEYMAN INTERVAL Although johnson_neyman() does not work on lavaan fitted object (yet), one can use a try-and-error approach to figure out the region of significance: First, obtain the minimum and maximum of the moderator NeedCogCont: min(cbtData$NeedCogCont) # -2.83 ## [1] -2.829197 max(cbtData$NeedCogCont) # 3.31 ## [1] 3.310095 ex4_JN &lt;- &quot; #Regressions NegThoughts ~ a1*CBTDummy + a2*NeedCogCont + a3*CBTxNeedCogCont Depression ~ b*NegThoughts + cprime*CBTDummy #Index of Moderated Mediation IndexOfModMed := a3*b #Simple Slopes aSSMin := a1+a3*(-2.83) aSSMin1 := a1+a3*(-1.75) aSSMin2 := a1+a3*(-1.74) aSSMin3 := a1+a3*(-1.58) aSSMin4 := a1+a3*(-1.57) aSSLow := a1+a3*(-1) aSSMean := a1+a3*0 aSSHigh := a1+a3*1 aSSMax := a1+a3*(3.31) #Conditional Indirect Effects abMin := (a1+a3*(-2.83))*b abMin1 := (a1+a3*(-1.75))*b # cutoff1 abMin2 := (a1+a3*(-1.74))*b abMin3 := (a1+a3*(-1.58))*b abMin4 := (a1+a3*(-1.57))*b # cutoff2 abLow := aSSLow*b abMean := aSSMean*b abHigh := aSSHigh*b abMax := (a1+a3*(3.31))*b &quot; set.seed(2022) ex4Boot &lt;- lavaan::sem(model = ex4_JN, data = cbtData, se = &quot;bootstrap&quot;, bootstrap = 1000, fixed.x=FALSE, meanstructure = TRUE) parameterEstimates(ex4Boot, level = 0.95, boot.ci.type=&quot;bca.simple&quot;) ## Warning in norm.inter(t, adj.alpha): extreme order statistics used as endpoints ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 NegThoughts ~ CBTDummy a1 -5.060 0.064 -78.764 0.000 -5.183 -4.930 ## 2 NegThoughts ~ NeedCogCont a2 2.000 0.048 42.047 0.000 1.902 2.093 ## 3 NegThoughts ~ CBTxNeedCogCont a3 -3.071 0.067 -45.675 0.000 -3.195 -2.922 ## 4 Depression ~ NegThoughts b 1.617 0.013 124.544 0.000 1.592 1.642 ## 5 Depression ~ CBTDummy cprime -1.066 0.084 -12.758 0.000 -1.216 -0.899 ## 6 NegThoughts ~~ NegThoughts 1.010 0.042 23.885 0.000 0.937 1.109 ## 7 Depression ~~ Depression 0.645 0.031 21.047 0.000 0.592 0.714 ## 8 CBTDummy ~~ CBTDummy 0.250 0.000 619.383 0.000 0.250 0.250 ## 9 CBTDummy ~~ NeedCogCont -0.020 0.016 -1.258 0.208 -0.051 0.013 ## 10 CBTDummy ~~ CBTxNeedCogCont -0.008 0.011 -0.781 0.435 -0.030 0.013 ## 11 NeedCogCont ~~ NeedCogCont 0.994 0.044 22.749 0.000 0.909 1.078 ## 12 NeedCogCont ~~ CBTxNeedCogCont 0.480 0.034 14.314 0.000 0.412 0.545 ## 13 CBTxNeedCogCont ~~ CBTxNeedCogCont 0.480 0.034 14.292 0.000 0.411 0.545 ## 14 NegThoughts ~1 0.523 0.046 11.401 0.000 0.436 0.616 ## 15 Depression ~1 2.089 0.037 56.930 0.000 2.021 2.160 ## 16 CBTDummy ~1 0.500 0.016 30.669 0.000 0.467 0.529 ## 17 NeedCogCont ~1 0.006 0.031 0.190 0.849 -0.062 0.063 ## 18 CBTxNeedCogCont ~1 -0.017 0.021 -0.781 0.435 -0.059 0.026 ## 19 IndexOfModMed := a3*b IndexOfModMed -4.967 0.114 -43.431 0.000 -5.180 -4.727 ## 20 aSSMin := a1+a3*(-2.83) aSSMin 3.630 0.199 18.215 0.000 3.204 4.011 ## 21 aSSMin1 := a1+a3*(-1.75) aSSMin1 0.313 0.133 2.363 0.018 0.029 0.562 ## 22 aSSMin2 := a1+a3*(-1.74) aSSMin2 0.283 0.132 2.141 0.032 0.000 0.530 ## 23 aSSMin3 := a1+a3*(-1.58) aSSMin3 -0.209 0.123 -1.701 0.089 -0.471 0.018 ## 24 aSSMin4 := a1+a3*(-1.57) aSSMin4 -0.239 0.122 -1.960 0.050 -0.500 -0.013 ## 25 aSSLow := a1+a3*(-1) aSSLow -1.990 0.092 -21.679 0.000 -2.176 -1.813 ## 26 aSSMean := a1+a3*0 aSSMean -5.060 0.064 -78.724 0.000 -5.183 -4.930 ## 27 aSSHigh := a1+a3*1 aSSHigh -8.131 0.094 -86.243 0.000 -8.313 -7.942 ## 28 aSSMax := a1+a3*(3.31) aSSMax -15.224 0.233 -65.232 0.000 -15.658 -14.751 ## 29 abMin := (a1+a3*(-2.83))*b abMin 5.871 0.324 18.092 0.000 5.185 6.487 ## 30 abMin1 := (a1+a3*(-1.75))*b abMin1 0.507 0.214 2.363 0.018 0.047 0.904 ## 31 abMin2 := (a1+a3*(-1.74))*b abMin2 0.457 0.213 2.141 0.032 -0.001 0.853 ## 32 abMin3 := (a1+a3*(-1.58))*b abMin3 -0.338 0.199 -1.700 0.089 -0.759 0.030 ## 33 abMin4 := (a1+a3*(-1.57))*b abMin4 -0.387 0.198 -1.959 0.050 -0.807 -0.020 ## 34 abLow := aSSLow*b abLow -3.218 0.151 -21.338 0.000 -3.532 -2.941 ## 35 abMean := aSSMean*b abMean -8.185 0.121 -67.625 0.000 -8.433 -7.944 ## 36 abHigh := aSSHigh*b abHigh -13.152 0.181 -72.721 0.000 -13.513 -12.779 ## 37 abMax := (a1+a3*(3.31))*b abMax -24.624 0.418 -58.964 0.000 -25.391 -23.738 So our regions of significance are: \\([-2.83, -1.75]\\): In which the ab are positive and significant. Participants with this level of NeegCog experienced elevated levels of depression due to CBT because CBT induces more negative thoughts among them. \\((-1.75, -1.57)\\): In which the ab are NOT significant. \\([-1.57, 3.31]\\): In which the ab are negative and significant. Participants with this level of NeegCog experienced reduced levels of depression due to CBT because CBT reduced negative thoughts for them. "],["r-lab-on-disaster-dataset-chapman-and-lickel-2016.html", "Chapter 7 R Lab on Disaster Dataset (Chapman and Lickel, 2016) 7.1 Data Prep 7.2 Model 1: Simple Linear Regression Model 7.3 Model 2: Simple Mediation Model 7.4 Model 3: Simple Moderation Model 7.5 Model 4a: Moderated Mediation Model - Path a only 7.6 Model 4b: Moderated Mediation Model - Path b only 7.7 Model 4c: Moderation &amp; Mediation Model - Path cprime only 7.8 Model 4d: Moderated Mediation Model - Path a and cprime 7.9 Model 4e: Moderated Mediation Model - Path b and cprime 7.10 Model 4f: Moderated Mediation Model - Path a and b 7.11 Model 4g: Moderated Mediation Model - Path a, b, and cprime 7.12 Conclusions", " Chapter 7 R Lab on Disaster Dataset (Chapman and Lickel, 2016) 7.1 Data Prep The following example data are from Chapman and Lickel (2016) Also example data in Chapter 12 of Hayes (2017) Simply load the .rda into R: load(&quot;disaster.rda&quot;) head(disaster) ## id frame donate justify skeptic ## 1 1 1 5.6 2.95 1.8 ## 2 2 1 4.2 2.85 5.2 ## 3 3 1 4.2 3.00 3.2 ## 4 4 1 4.6 3.30 1.0 ## 5 5 1 3.0 5.00 7.6 ## 6 6 0 5.0 3.20 4.2 str(disaster) ## &#39;data.frame&#39;: 211 obs. of 5 variables: ## $ id : num 1 2 3 4 5 6 7 8 9 10 ... ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.0&quot; ## ..- attr(*, &quot;display_width&quot;)= int 6 ## $ frame : num 1 1 1 1 1 0 0 1 0 0 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Experimental condition&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.2&quot; ## ..- attr(*, &quot;labels&quot;)= Named num [1:2] 0 1 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;naturally caused disaster&quot; &quot;climate change caused disaster&quot; ## $ donate : num 5.6 4.2 4.2 4.6 3 5 4.8 6 4.2 4.4 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Positive attitudes toward donating&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.2&quot; ## ..- attr(*, &quot;display_width&quot;)= int 9 ## $ justify: num 2.95 2.85 3 3.3 5 3.2 2.9 1.4 3.25 3.55 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Negative justifications&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.2&quot; ## ..- attr(*, &quot;display_width&quot;)= int 10 ## $ skeptic: num 1.8 5.2 3.2 1 7.6 4.2 4.2 1.2 1.8 8.8 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Climate change skepticism&quot; ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.2&quot; If you are able to install package processR, you can also view its help page: install.packages(&quot;processR&quot;) # If error message persists, change the repository to CRAN: install.packages(&quot;processR&quot;, repos=&quot;https://cran.rstudio.com/&quot;) library(processR) data(disaster) # take a look at the dataset: ?disaster library(processR) You probably have to go to https://www.xquartz.org/ to download and install X11, which is a server required by many R packages, including processR. Disaster is a data.frame with 211 obs. of 5 variables: id frame: Experimental condition. 0 = naturally caused disaster, 1 = climate change caused disaster donate: Positive attitudes toward donating justify: Negative justifications skeptic: Climate change skepticism 7.1.1 Scatterplot Matrix Before we build linear models, we should plot the relationship between pairs of variables: library(PerformanceAnalytics) chart.Correlation(disaster[,-1]) 7.1.2 p-value or bootstrapped confidence interval? For models that involve mediation effects, we prefer to use bootstrap confidence intervals over p-values for evaluating the significance of parameter estimates. That is, in the parameter table generated by parameterEstimates() function: A coefficient is considered significant when the interval [ci.lower, ci.upper] does not include zero; A coefficient is considered insignificant when the interval [ci.lower, ci.upper] includes zero. In most cases, bootstrap confidence intervals and p-values yield the same conclusions regarding the significance of parameter estimates. If not, bootstrap confidence intervals are used to make the final call. In this document, all bootstrap confidence intervals and p-values yield the same conclusions regarding significances, so I’ll only refer to p-values for the readability of the analyses. 7.2 Model 1: Simple Linear Regression Model With processR, you can draw concept diagram and statistical diagram of mediation and moderation models quite easily. For example: labels=list(X=&quot;frame&quot;,M=&quot;justify&quot;,Y=&quot;donate&quot;,W=&quot;skeptic&quot;) par(mfrow = c(2,1), mar=c(0,0,0,0), oma=c(0,0,0,0)) pmacroModel(0,labels=labels) statisticalDiagram(0, labels=labels) return the diagrams of a simple linear regression model. For Model 1, let’s run a simple linear regression using lm() to estimate the total effect of frame on willingness to donate: lm1 = lm(donate ~ frame , data = disaster) summary(lm1)[[4]] ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.56363636 0.1258337 36.2672028 3.955220e-92 ## frame 0.08388839 0.1818769 0.4612372 6.451081e-01 Note that the [[4]] was added after summary(lm1) to request the coefficient table only. The total effect is c = 0.084 (p = 0.645), not significant. However, we learned in this class that absence of association between X and Y does NOT mean that X isn’t affecting Y (remember inconsistent mediation?). So let’s move on… 7.3 Model 2: Simple Mediation Model Q: If a disaster is framed as the result of a climate change (instead of a natural disaster), do you think it’s justified to withhold aid to the victims, and thus become less willing to donate? par(mfrow = c(1,2), mar=c(0,0,0,0), oma=c(0,0,0,0)) pmacroModel(4,labels=labels) statisticalDiagram(4, labels=labels) Load the lavaan package: library(lavaan) and test the mediation effect (ab) using bootstrap: lm2.syntax &lt;- &#39; donate ~ b*justify + cprime*frame justify ~ a*frame # Define new parameters #The := operator in lavaan defines new parameters. ab:= a*b c:= a*b + cprime &#39; set.seed(2022) lm2.bfit = sem(lm2.syntax, data = disaster, se = &quot;bootstrap&quot;, bootstrap = 1000, fixed.x=FALSE, meanstructure = TRUE) summary(lm2.bfit, ci = T) ## lavaan 0.6-10 ended normally after 19 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 9 ## ## Number of observations 211 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Bootstrap ## Number of requested bootstrap draws 1000 ## Number of successful bootstrap draws 1000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## donate ~ ## justify (b) -0.953 0.065 -14.641 0.000 -1.078 -0.823 ## frame (cprm) 0.212 0.139 1.526 0.127 -0.065 0.492 ## justify ~ ## frame (a) 0.134 0.131 1.023 0.306 -0.133 0.393 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .donate 7.235 0.199 36.380 0.000 6.810 7.595 ## .justify 2.802 0.083 33.785 0.000 2.644 2.967 ## frame 0.479 0.036 13.458 0.000 0.403 0.545 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## .donate 0.948 0.118 8.061 0.000 0.712 1.171 ## .justify 0.856 0.097 8.866 0.000 0.683 1.071 ## frame 0.250 0.003 97.289 0.000 0.241 0.250 ## ## Defined Parameters: ## Estimate Std.Err z-value P(&gt;|z|) ci.lower ci.upper ## ab -0.128 0.125 -1.022 0.307 -0.378 0.125 ## c 0.084 0.188 0.447 0.655 -0.253 0.476 parameterEstimates(lm2.bfit, boot.ci.type = &quot;bca.simple&quot;, standardized = T) ## lhs op rhs label est se z pvalue ci.lower ci.upper std.lv std.all std.nox ## 1 donate ~ justify b -0.953 0.065 -14.641 0.000 -1.083 -0.828 -0.953 -0.673 -0.673 ## 2 donate ~ frame cprime 0.212 0.139 1.526 0.127 -0.062 0.503 0.212 0.081 0.161 ## 3 justify ~ frame a 0.134 0.131 1.023 0.306 -0.147 0.380 0.134 0.072 0.145 ## 4 donate ~~ donate 0.948 0.118 8.061 0.000 0.735 1.214 0.948 0.549 0.549 ## 5 justify ~~ justify 0.856 0.097 8.866 0.000 0.699 1.096 0.856 0.995 0.995 ## 6 frame ~~ frame 0.250 0.003 97.289 0.000 0.244 0.250 0.250 1.000 0.250 ## 7 donate ~1 7.235 0.199 36.380 0.000 6.808 7.590 7.235 5.506 5.506 ## 8 justify ~1 2.802 0.083 33.785 0.000 2.649 2.976 2.802 3.022 3.022 ## 9 frame ~1 0.479 0.036 13.458 0.000 0.390 0.536 0.479 0.958 0.479 ## 10 ab := a*b ab -0.128 0.125 -1.022 0.307 -0.362 0.144 -0.128 -0.049 -0.097 ## 11 c := a*b+cprime c 0.084 0.188 0.447 0.655 -0.233 0.524 0.084 0.032 0.064 From the coefficient table, we can see: a path: a = 0.134 (p = 0.306) b path: b = -0.953 (p = 0.000) indirect effect: ab = -0.128 (p = 0.307) direct effect: cprime = 0.212 (p = 0.127) total effect: c = ab + cprime = 0.084 (p = 0.655) Except for b path, all effects above are not significant. This tells us: The framing of the disaster did not significantly change people’s beliefs about whether providing aid to the victims is justified (a path) Justification for withholding aid did make participants less willing to donate (b path) However, the indirect effect ab was not significant, meaning justification for withholding did not explain the relationship between frame and willingness to donate Let’s switch to moderation model: 7.4 Model 3: Simple Moderation Model Skepticism of climate change cannot be changed by the frame, so skeptic is a moderator, not a mediator, it does not stand in the middle of the pathway par(mfrow = c(1,2), mar=c(0,0,0,0), oma=c(0,0,0,0)) pmacroModel(1,labels=labels) statisticalDiagram(1, labels=labels) Since the moderator skeptic is a continuous measure, we need to mean center it first: disaster$skep_raw = disaster$skeptic disaster$skeptic = scale(disaster$skep_raw, center = TRUE, scale = FALSE) disaster$skep_sd = scale(disaster$skep_raw, center = TRUE, scale = TRUE) disaster$skepxframe = disaster$skeptic * disaster$frame .. and manually create an interaction term by multiplying skeptic and frame: disaster$skepxframe = disaster$skeptic * disaster$frame Let’s examine their means and standard deviations: round(apply(disaster, 2, mean), 2) ## id frame donate justify skeptic skep_raw skep_sd skepxframe ## 106.00 0.48 4.60 2.87 0.00 3.38 0.00 0.02 round(apply(disaster, 2, sd), 2) ## id frame donate justify skeptic skep_raw skep_sd skepxframe ## 61.05 0.50 1.32 0.93 2.03 2.03 1.00 1.40 Let’s write the syntax for the simple moderation model: lm3.syntax &lt;- &#39; #Regression with interaction donate ~ b1*skeptic + b2*frame + b3*skepxframe &#39; lm3.fit = sem(lm3.syntax, data = disaster, fixed.x=FALSE, meanstructure = TRUE) parameterEstimates(lm3.fit, ci = T) ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 donate ~ skeptic b1 -0.140 0.057 -2.433 0.015 -0.252 -0.027 ## 2 donate ~ frame b2 0.103 0.169 0.609 0.542 -0.228 0.433 ## 3 donate ~ skepxframe b3 -0.171 0.083 -2.054 0.040 -0.334 -0.008 ## 4 donate ~~ donate 1.495 0.146 10.271 0.000 1.210 1.780 ## 5 skeptic ~~ skeptic 4.113 0.400 10.271 0.000 3.328 4.897 ## 6 skeptic ~~ frame 0.021 0.070 0.294 0.769 -0.116 0.157 ## 7 skeptic ~~ skepxframe 1.957 0.237 8.249 0.000 1.492 2.423 ## 8 frame ~~ frame 0.250 0.024 10.271 0.000 0.202 0.297 ## 9 frame ~~ skepxframe 0.011 0.048 0.222 0.824 -0.084 0.105 ## 10 skepxframe ~~ skepxframe 1.957 0.191 10.271 0.000 1.584 2.331 ## 11 donate ~1 4.558 0.117 39.092 0.000 4.330 4.787 ## 12 skeptic ~1 0.000 0.140 0.000 1.000 -0.274 0.274 ## 13 frame ~1 0.479 0.034 13.919 0.000 0.411 0.546 ## 14 skepxframe ~1 0.021 0.096 0.213 0.831 -0.168 0.209 b3 = -0.171, p = 0.040 So this moderator, skeptic, is a significant moderator of the frame-donate path, that is, skepticism of climate change could change the effect of framing on willingness to donate. 7.4.1 JOHNSON-NEYMAN INTERVAL interactionModel2 &lt;- lm(donate ~ skeptic*frame, disaster) summary(interactionModel2) ## ## Call: ## lm(formula = donate ~ skeptic * frame, data = disaster) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.8341 -0.7077 0.1659 0.9101 2.6682 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.55815 0.11772 38.719 &lt;2e-16 *** ## skeptic -0.13953 0.05790 -2.410 0.0168 * ## frame 0.10266 0.17016 0.603 0.5469 ## skeptic:frame -0.17071 0.08393 -2.034 0.0432 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.234 on 207 degrees of freedom ## Multiple R-squared: 0.1343, Adjusted R-squared: 0.1218 ## F-statistic: 10.71 on 3 and 207 DF, p-value: 1.424e-06 library(interactions) interactions::johnson_neyman(interactionModel2, pred = &quot;frame&quot;, modx = &quot;skeptic&quot;, alpha = 0.05) ## JOHNSON-NEYMAN INTERVAL ## ## When skeptic is OUTSIDE the interval [-2.59, 22.34], the slope of frame is p &lt; .05. ## ## Note: The range of observed values of skeptic is [-2.38, 5.62] As can be seen, it appears that among those low in climate change skepticism (lower than -2.59), framing the drought as caused by climate change produced a greater willingness to donate (simple slopes were significantly positive) compared to when climate change was not described as the cause. Among climate change skeptics (i.e., those high on the skepticism scale), the willingness to donate to the victims were not affected the framing of the problem (simple slopes were not significantly different from 0). Next, let’s test those Moderated Mediation Models one by one. 7.5 Model 4a: Moderated Mediation Model - Path a only par(mfrow = c(1,2), mar=c(0,0,0,0), oma=c(0,0,0,0)) pmacroModel(7,labels=labels) statisticalDiagram(7, labels=labels) Since the frame-justify (a path) is hypothesized to be moderated by skeptic, the simple slope of justify on frame is a function of skeptic, that is, a1+a3*skeptic The indirect effect through justify also depends on skeptic, calculated as b*(a1+a3*skeptic) Since skeptic is a continuous variable, we will pick three values from it. The chapter in Hayes (2017) picked the 16th, 50th, and 84th percentiles of the distribution using the quantile() function: quantile(disaster$skeptic, probs = c(0.16, 0.5, 0.84)) ## 16% 50% 84% ## -1.7779621 -0.5779621 1.8220379 which are: low: -1.78 median: -0.58 mean: 0 (why) high: 1.82 We’ll also define the index of moderated mediation to be: a3*b (refer to slides of week6_1) Let’s write the syntax for the moderated mediation model: lm4a.syntax &lt;- &#39; donate ~ b*justify + cprime*frame justify ~ a1*frame + a2*skeptic + a3*skepxframe # Define simple slopes and conditional indirect effects using := # index of moderated mediation IndMedMod:= a3*b # simple slope of justify on frame is a1+a3*skeptic aLow: = a1+a3*(-1.78) aMedian: = a1+a3*(-0.58) aMean: = a1+a3*(0) aHigh: = a1+a3*1.82 # conditional indirect effects is b*(a1+a3*skeptic) abLow: = b*aLow abMedian: = b*aMedian abMean: = b*aMean abHigh: = b*aHigh &#39; set.seed(2022) lm4a.fit = sem(lm4a.syntax, data = disaster, se = &quot;bootstrap&quot;, bootstrap = 1000, fixed.x=FALSE, meanstructure = TRUE) parameterEstimates(lm4a.fit, level = 0.95, boot.ci.type = &quot;bca.simple&quot;) ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 donate ~ justify b -0.953 0.065 -14.641 0.000 -1.083 -0.828 ## 2 donate ~ frame cprime 0.212 0.139 1.526 0.127 -0.062 0.503 ## 3 justify ~ frame a1 0.117 0.114 1.023 0.306 -0.113 0.324 ## 4 justify ~ skeptic a2 0.105 0.043 2.459 0.014 0.020 0.194 ## 5 justify ~ skepxframe a3 0.201 0.063 3.201 0.001 0.077 0.326 ## 6 donate ~~ donate 0.948 0.118 8.061 0.000 0.735 1.214 ## 7 justify ~~ justify 0.648 0.066 9.767 0.000 0.539 0.809 ## 8 frame ~~ frame 0.250 0.003 97.289 0.000 0.242 0.250 ## 9 frame ~~ skeptic 0.021 0.074 0.277 0.782 -0.125 0.168 ## 10 frame ~~ skepxframe 0.011 0.052 0.207 0.836 -0.089 0.121 ## 11 skeptic ~~ skeptic 4.113 0.498 8.257 0.000 3.211 5.198 ## 12 skeptic ~~ skepxframe 1.957 0.349 5.606 0.000 1.333 2.721 ## 13 skepxframe ~~ skepxframe 1.957 0.348 5.618 0.000 1.328 2.708 ## 14 donate ~1 7.235 0.199 36.380 0.000 6.808 7.590 ## 15 justify ~1 2.806 0.079 35.374 0.000 2.659 2.976 ## 16 frame ~1 0.479 0.036 13.458 0.000 0.396 0.540 ## 17 skeptic ~1 0.000 0.142 0.000 1.000 -0.274 0.286 ## 18 skepxframe ~1 0.021 0.099 0.206 0.837 -0.173 0.223 ## 19 IndMedMod := a3*b IndMedMod -0.192 0.062 -3.073 0.002 -0.327 -0.076 ## 20 aLow := a1+a3*(-1.78) aLow -0.241 0.154 -1.568 0.117 -0.563 0.045 ## 21 aMedian := a1+a3*(-0.58) aMedian 0.000 0.117 0.004 0.997 -0.240 0.209 ## 22 aMean := a1+a3*(0) aMean 0.117 0.115 1.023 0.307 -0.113 0.324 ## 23 aHigh := a1+a3*1.82 aHigh 0.483 0.168 2.874 0.004 0.138 0.829 ## 24 abLow := b*aLow abLow 0.230 0.149 1.546 0.122 -0.044 0.540 ## 25 abMedian := b*aMedian abMedian 0.000 0.112 -0.004 0.997 -0.201 0.236 ## 26 abMean := b*aMean abMean -0.112 0.109 -1.027 0.305 -0.306 0.103 ## 27 abHigh := b*aHigh abHigh -0.461 0.164 -2.811 0.005 -0.796 -0.130 So we have: interaction of skepxframe on justify: a3 = 0.201 (p = 0.001) The overall effect of frame on justify is significantly moderated by skeptic. That is, how the disaster is framed has a differential effect for people who differ in their climate change skepticism on their beliefs that if it was justified to withhold aid to the victims. Let’s look at the simple slopes. aLow = -0.241 (p = 0.117) aMedian = 0 (p = 0.997) aMean = 0.117 (p = 0.307) aHigh = 0.483 (p = 0.004) Moreover, the simple slope aHigh is positive and aLow is negative. That is, when told that the disaster is the result of climate change instead of natural disaster (changing frame from 0 to 1), those who doubt climate change (high on skepticism) think it’s justified to withhold the aid whereas those who do not doubt (low on skepticism) it think it’s not justified to withhold the aid. Okay! The a path is moderated. What about the mediation effect ab? IndexOfModMed = -0.192 (p = 0.002) IndexOfModMed is sig! Woo-hoo! Indirect effect is moderated, too! So what story does it tell you? abLow = 0.230 (p = 0.122) abMedian = 0.000 (p = 0.997) abMean = -0.112 (p = 0.305) abHigh = -0.461 (p = 0.005) Furthermore, abHigh is negative and abLow is positive, meaning that framing the disaster as caused by climate change leads to less donation for people who doubt climate change (high on skepticism) but it leads to more donation for people who believe it (low on skepticism). The reason for this differential effect is that those who doubt climate change tend to favor the idea of withholding the aid, thus leading to less donation. 7.6 Model 4b: Moderated Mediation Model - Path b only par(mfrow = c(1,2), mar=c(0,0,0,0), oma=c(0,0,0,0)) pmacroModel(14,labels=labels) statisticalDiagram(14, labels=labels) Since justify is also a continuous measure, we need to mean center it first: disaster$just_raw = disaster$justify disaster$justify = scale(disaster$just_raw, center = TRUE, scale = FALSE) disaster$just_sd = scale(disaster$just_raw, center = TRUE, scale = TRUE) and create an interaction term by multiplying skeptic by justify (note that this is a new interaction term!): disaster$skepxjusti = disaster$skeptic * disaster$justify Let’s examine their means and standard deviations: round(apply(disaster[,-1], 2, mean), 2) ## frame donate justify skeptic skep_raw skep_sd skepxframe just_raw just_sd skepxjusti ## 0.48 4.60 0.00 0.00 3.38 0.00 0.02 2.87 0.00 0.83 round(apply(disaster[,-1], 2, sd), 2) ## frame donate justify skeptic skep_raw skep_sd skepxframe just_raw just_sd skepxjusti ## 0.50 1.32 0.93 2.03 2.03 1.00 1.40 0.93 1.00 2.66 Since the b path is hypothesized moderated by skeptic, the simple slope of donation on justify (b path) depends on skeptic, the indirect effect through justify (ab) also depends on skeptic. We’ll define an index of moderated mediation to be: a*b3 (can you derive this?) Let’s write the syntax for the moderated mediation model: lm4b.syntax &lt;- &#39; donate ~ b1*justify + cprime*frame + b2*skeptic + b3*skepxjusti justify ~ a*frame # Define simple slopes and conditional indirect effects using := # index of moderated mediation IndMedMod:= a*b3 # simple slope of donate on justify is b1+b3*skeptic bLow: = b1+b3*(-1.78) bMedian: = b1+b3*(-0.58) bMean: = b1+b3*(0) bHigh: = b1+b3*1.82 # conditional indirect effects is a*(b1+b3*skeptic) abLow: = a*bLow abMedian: = a*bMedian abMean: = a*bMean abHigh: = a*bHigh &#39; set.seed(2022) lm4b.fit = sem(lm4b.syntax, data = disaster, se = &quot;bootstrap&quot;, bootstrap = 1000, fixed.x=FALSE, meanstructure = TRUE) parameterEstimates(lm4b.fit, level = 0.95, boot.ci.type = &quot;bca.simple&quot;) ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 donate ~ justify b1 -0.922 0.079 -11.611 0.000 -1.085 -0.772 ## 2 donate ~ frame cprime 0.204 0.140 1.455 0.146 -0.064 0.492 ## 3 donate ~ skeptic b2 -0.040 0.041 -0.968 0.333 -0.127 0.039 ## 4 donate ~ skepxjusti b3 0.008 0.026 0.327 0.743 -0.043 0.059 ## 5 justify ~ frame a 0.134 0.131 1.023 0.306 -0.147 0.380 ## 6 donate ~~ donate 0.943 0.116 8.128 0.000 0.736 1.200 ## 7 justify ~~ justify 0.856 0.097 8.866 0.000 0.699 1.096 ## 8 frame ~~ frame 0.250 0.003 97.289 0.000 0.244 0.250 ## 9 frame ~~ skeptic 0.021 0.074 0.277 0.782 -0.125 0.168 ## 10 frame ~~ skepxjusti 0.204 0.091 2.233 0.026 0.045 0.414 ## 11 skeptic ~~ skeptic 4.113 0.498 8.257 0.000 3.211 5.198 ## 12 skeptic ~~ skepxjusti 2.067 0.867 2.383 0.017 0.691 4.089 ## 13 skepxjusti ~~ skepxjusti 7.017 2.414 2.907 0.004 3.602 13.111 ## 14 donate ~1 4.499 0.110 40.942 0.000 4.288 4.708 ## 15 justify ~1 -0.064 0.083 -0.775 0.438 -0.218 0.109 ## 16 frame ~1 0.479 0.036 13.458 0.000 0.396 0.540 ## 17 skeptic ~1 0.000 0.142 0.000 1.000 -0.274 0.286 ## 18 skepxjusti ~1 0.828 0.179 4.632 0.000 0.526 1.218 ## 19 IndMedMod := a*b3 IndMedMod 0.001 0.005 0.243 0.808 -0.005 0.018 ## 20 bLow := b1+b3*(-1.78) bLow -0.937 0.103 -9.140 0.000 -1.138 -0.729 ## 21 bMedian := b1+b3*(-0.58) bMedian -0.927 0.085 -10.904 0.000 -1.096 -0.758 ## 22 bMean := b1+b3*(0) bMean -0.922 0.079 -11.605 0.000 -1.085 -0.772 ## 23 bHigh := b1+b3*1.82 bHigh -0.907 0.079 -11.427 0.000 -1.073 -0.752 ## 24 abLow := a*bLow abLow -0.126 0.123 -1.023 0.306 -0.369 0.130 ## 25 abMedian := a*bMedian abMedian -0.125 0.122 -1.023 0.306 -0.361 0.137 ## 26 abMean := a*bMean abMean -0.124 0.121 -1.022 0.307 -0.360 0.135 ## 27 abHigh := a*bHigh abHigh -0.122 0.120 -1.015 0.310 -0.351 0.130 So we have: b3 = 0.008 (p = 0.743) The effect of justify on donate is not moderated by skeptic. That is, justification for withholding aid always leads to less donation has a fixed effect on their willingness to donate (b path) regardless of their climate change skepticism. Let’s look at the simple slopes. bLow = -0.937 (p = 0.000) bMedian = -0.927 (p = 0.000) bMean = -0.922 (p = 0.000) bHigh = -0.907 (p = 0.000) which do not change much as their climate change skepticism change. Justification for withholding aid always leads to less donation. Skeptic is not an effective moderator. Let’s look at the indirect effects IndexOfModMed = 0.001 (p = 0.808) abLow = -0.126 (p = 0.306) abMedian = -0.125 (p = 0.306) abMean = -0.124 (p = 0.307) abHigh = -0.122 (p = 0.310) Similarly, skeptic is not a good moderator for the indirect effect given that IndexOfModMed is not significant and the indirect effects at high/low levels of skepticism do not differ much. 7.7 Model 4c: Moderation &amp; Mediation Model - Path cprime only par(mfrow = c(1,2), mar=c(0,0,0,0), oma=c(0,0,0,0)) pmacroModel(5,labels=labels) statisticalDiagram(5, labels=labels) Since there is NO indirect effect being moderated here, we’ll not define any index of moderation mediation. There is only one c3prime coefficient that quantifies the moderation effect of skeptic on frame-donation path. lm4c.syntax &lt;- &#39; donate ~ b*justify + c1prime*frame + skeptic + c3prime*skepxframe justify ~ a*frame # Define new parameters #The := operator in lavaan defines new parameters. # simple slope of donate on frame is c1prime+c3prime*skeptic cLow: = c1prime+c3prime*(-1.78) cMedian: = c1prime+c3prime*(-0.58) cMean: = c1prime+c3prime*(0) cHigh: = c1prime+c3prime*1.82 # mediation effect ab:= a*b &#39; set.seed(2022) lm4c.fit = sem(lm4c.syntax, data = disaster, se = &quot;bootstrap&quot;, bootstrap = 1000, fixed.x=FALSE, meanstructure = TRUE) parameterEstimates(lm4c.fit, level = 0.95, boot.ci.type = &quot;bca.simple&quot;) ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 donate ~ justify b -0.923 0.078 -11.781 0.000 -1.076 -0.764 ## 2 donate ~ frame c1prime 0.211 0.140 1.503 0.133 -0.064 0.496 ## 3 donate ~ skeptic -0.043 0.057 -0.742 0.458 -0.165 0.061 ## 4 donate ~ skepxframe c3prime 0.015 0.074 0.203 0.839 -0.120 0.171 ## 5 justify ~ frame a 0.134 0.131 1.023 0.306 -0.147 0.380 ## 6 donate ~~ donate 0.943 0.116 8.152 0.000 0.743 1.208 ## 7 justify ~~ justify 0.856 0.097 8.866 0.000 0.699 1.096 ## 8 frame ~~ frame 0.250 0.003 97.289 0.000 0.244 0.250 ## 9 frame ~~ skeptic 0.021 0.074 0.277 0.782 -0.125 0.168 ## 10 frame ~~ skepxframe 0.011 0.052 0.207 0.836 -0.089 0.121 ## 11 skeptic ~~ skeptic 4.113 0.498 8.257 0.000 3.211 5.198 ## 12 skeptic ~~ skepxframe 1.957 0.349 5.606 0.000 1.333 2.721 ## 13 skepxframe ~~ skepxframe 1.957 0.348 5.618 0.000 1.328 2.708 ## 14 donate ~1 4.503 0.109 41.164 0.000 4.283 4.711 ## 15 justify ~1 -0.064 0.083 -0.775 0.438 -0.218 0.109 ## 16 frame ~1 0.479 0.036 13.458 0.000 0.396 0.540 ## 17 skeptic ~1 0.000 0.142 0.000 1.000 -0.274 0.286 ## 18 skepxframe ~1 0.021 0.099 0.206 0.837 -0.173 0.223 ## 19 cLow := c1prime+c3prime*(-1.78) cLow 0.184 0.172 1.074 0.283 -0.150 0.527 ## 20 cMedian := c1prime+c3prime*(-0.58) cMedian 0.202 0.138 1.463 0.143 -0.062 0.482 ## 21 cMean := c1prime+c3prime*(0) cMean 0.211 0.140 1.503 0.133 -0.064 0.496 ## 22 cHigh := c1prime+c3prime*1.82 cHigh 0.238 0.213 1.118 0.264 -0.153 0.662 ## 23 ab := a*b ab -0.124 0.122 -1.020 0.308 -0.356 0.138 Since we have: c3prime = 0.015 (p = 0.839) Skeptic is not a moderator for this frame-donation path (cprime). The mediation effect: ab = -0.124 (p = 0.308) is not significant, just like in Model 2. 7.8 Model 4d: Moderated Mediation Model - Path a and cprime par(mfrow = c(1,2), mar=c(0,0,0,0), oma=c(0,0,0,0)) pmacroModel(8,labels=labels) statisticalDiagram(8, labels=labels) Model 4d is very similar to Model 4a, except that cprime path is also moderated. We’ll still define the index of moderated mediation to be: a3*b lm4d.syntax &lt;- &#39; donate ~ b*justify + c1prime*frame + c2prime*skeptic + c3prime*skepxframe justify ~ a1*frame + a2*skeptic + a3*skepxframe # Define simple slopes and conditional indirect effects using := # index of moderated mediation IndMedMod:= a3*b # simple slope of justify on frame is a1+a3*skeptic aLow: = a1+a3*(-1.78) aMedian: = a1+a3*(-0.58) aMean: = a1+a3*(0) aHigh: = a1+a3*1.82 # conditional indirect effects is b*(a1+a3*skeptic) abLow: = b*aLow abMedian: = b*aMedian abMean: = b*aMean abHigh: = b*aHigh &#39; set.seed(2022) lm4d.fit = sem(lm4d.syntax, data = disaster, se = &quot;bootstrap&quot;, bootstrap = 1000, fixed.x=FALSE, meanstructure = TRUE) parameterEstimates(lm4d.fit, level = 0.95, boot.ci.type = &quot;bca.simple&quot;) ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 donate ~ justify b -0.923 0.078 -11.781 0.000 -1.076 -0.764 ## 2 donate ~ frame c1prime 0.211 0.140 1.503 0.133 -0.064 0.496 ## 3 donate ~ skeptic c2prime -0.043 0.057 -0.742 0.458 -0.165 0.061 ## 4 donate ~ skepxframe c3prime 0.015 0.074 0.203 0.839 -0.120 0.171 ## 5 justify ~ frame a1 0.117 0.114 1.023 0.306 -0.113 0.324 ## 6 justify ~ skeptic a2 0.105 0.043 2.459 0.014 0.020 0.194 ## 7 justify ~ skepxframe a3 0.201 0.063 3.201 0.001 0.077 0.326 ## 8 donate ~~ donate 0.943 0.116 8.152 0.000 0.743 1.208 ## 9 justify ~~ justify 0.648 0.066 9.767 0.000 0.539 0.809 ## 10 frame ~~ frame 0.250 0.003 97.289 0.000 0.244 0.250 ## 11 frame ~~ skeptic 0.021 0.074 0.277 0.782 -0.125 0.168 ## 12 frame ~~ skepxframe 0.011 0.052 0.207 0.836 -0.089 0.121 ## 13 skeptic ~~ skeptic 4.113 0.498 8.257 0.000 3.211 5.198 ## 14 skeptic ~~ skepxframe 1.957 0.349 5.606 0.000 1.333 2.721 ## 15 skepxframe ~~ skepxframe 1.957 0.348 5.618 0.000 1.328 2.708 ## 16 donate ~1 4.503 0.109 41.164 0.000 4.283 4.711 ## 17 justify ~1 -0.060 0.079 -0.759 0.448 -0.208 0.109 ## 18 frame ~1 0.479 0.036 13.458 0.000 0.396 0.540 ## 19 skeptic ~1 0.000 0.142 0.000 1.000 -0.274 0.286 ## 20 skepxframe ~1 0.021 0.099 0.206 0.837 -0.173 0.223 ## 21 IndMedMod := a3*b IndMedMod -0.186 0.062 -2.979 0.003 -0.318 -0.071 ## 22 aLow := a1+a3*(-1.78) aLow -0.241 0.154 -1.568 0.117 -0.563 0.045 ## 23 aMedian := a1+a3*(-0.58) aMedian 0.000 0.117 0.004 0.997 -0.240 0.209 ## 24 aMean := a1+a3*(0) aMean 0.117 0.115 1.023 0.307 -0.113 0.324 ## 25 aHigh := a1+a3*1.82 aHigh 0.483 0.168 2.874 0.004 0.138 0.829 ## 26 abLow := b*aLow abLow 0.222 0.146 1.526 0.127 -0.036 0.532 ## 27 abMedian := b*aMedian abMedian 0.000 0.108 -0.004 0.997 -0.194 0.228 ## 28 abMean := b*aMean abMean -0.108 0.105 -1.026 0.305 -0.301 0.095 ## 29 abHigh := b*aHigh abHigh -0.446 0.162 -2.757 0.006 -0.785 -0.127 Here we have two interaction coefficients: a3 = 0.201 (p = 0.001) c3prime = 0.015 (p = 0.839) so that skeptic is a moderator for the frame-to-justify path (a path) but not a moderator for frame-to-donate path (cprime path). For the frame-to-justify path: aLow = -0.241 (p = 0.117) aMedian = 0 (p = 0.997) aMean = 0.117 (p = 0.307) aHigh = 0.483 (p = 0.004) which are exactly the same as those in Model 4a. The simple slope aHigh is positive and aLow is negative. Those who are high on climate change skepticism think it’s justified to withhold the aid whereas those who are low on the skepticism do not think it’s justified to withhold the aid. IndexOfModMed = -0.186 (p = 0.003) abLow = 0.222 (p = 0.127) abMedian = 0.000 (p = 0.997) abMean = -0.108 (p = 0.305) abHigh = -0.446 (p = 0.006) which are close to those in Model 4a. The indirect effect of donate on frame through justify (ab path) is moderated by skeptic (IndexOfModMed is sig!). Moreover, abHigh is negative and abLow is positive, meaning that framing the disaster as caused by climate change leads to less donation for people who doubt climate change but it leads to more donation for people who believes it because they do not think it’s justified to withhold the aid. 7.9 Model 4e: Moderated Mediation Model - Path b and cprime par(mfrow = c(1,2), mar=c(0,0,0,0), oma=c(0,0,0,0)) pmacroModel(15,labels=labels) statisticalDiagram(15, labels=labels) Model 4e is very similar to Model 4b except that cprime path is also moderated. We’ll still define the index of moderated mediation to be: a*b3 (actually, a*b2 in this diagram) Let’s write the syntax for the moderated mediation model: lm4e.syntax &lt;- &#39; donate ~ b1*justify + b2*skepxjusti + c1prime*frame + c2prime*skeptic + c3prime*skepxframe justify ~ a*frame # Define simple slopes and conditional indirect effects using := # index of moderated mediation IndMedMod:= a*b2 # simple slope of donate on justify is b1+b2*skeptic bLow: = b1+b2*(-1.78) bMedian: = b1+b2*(-0.58) bMean: = b1+b2*(0) bHigh: = b1+b2*1.82 # conditional indirect effects is a*(b1+b2*skeptic) abLow: = a*bLow abMedian: = a*bMedian abMean: = a*bMean abHigh: = a*bHigh &#39; set.seed(2022) lm4e.fit = sem(lm4e.syntax, data = disaster, se = &quot;bootstrap&quot;, bootstrap = 1000, fixed.x=FALSE, meanstructure = TRUE) parameterEstimates(lm4e.fit, level = 0.95, boot.ci.type = &quot;bca.simple&quot;) ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 donate ~ justify b1 -0.925 0.081 -11.473 0.000 -1.085 -0.762 ## 2 donate ~ skepxjusti b2 0.007 0.028 0.258 0.797 -0.047 0.063 ## 3 donate ~ frame c1prime 0.205 0.143 1.432 0.152 -0.075 0.500 ## 4 donate ~ skeptic c2prime -0.043 0.058 -0.749 0.454 -0.165 0.058 ## 5 donate ~ skepxframe c3prime 0.009 0.079 0.119 0.905 -0.139 0.171 ## 6 justify ~ frame a 0.134 0.131 1.023 0.306 -0.147 0.380 ## 7 donate ~~ donate 0.943 0.116 8.148 0.000 0.744 1.212 ## 8 justify ~~ justify 0.856 0.097 8.866 0.000 0.699 1.096 ## 9 skepxjusti ~~ skepxjusti 7.017 2.414 2.907 0.004 3.602 13.111 ## 10 skepxjusti ~~ frame 0.204 0.091 2.233 0.026 0.045 0.414 ## 11 skepxjusti ~~ skeptic 2.067 0.867 2.383 0.017 0.691 4.089 ## 12 skepxjusti ~~ skepxframe 1.828 0.784 2.332 0.020 0.600 3.681 ## 13 frame ~~ frame 0.250 0.003 97.289 0.000 0.244 0.250 ## 14 frame ~~ skeptic 0.021 0.074 0.277 0.782 -0.125 0.168 ## 15 frame ~~ skepxframe 0.011 0.052 0.207 0.836 -0.089 0.121 ## 16 skeptic ~~ skeptic 4.113 0.498 8.257 0.000 3.211 5.198 ## 17 skeptic ~~ skepxframe 1.957 0.349 5.606 0.000 1.333 2.721 ## 18 skepxframe ~~ skepxframe 1.957 0.348 5.618 0.000 1.328 2.708 ## 19 donate ~1 4.499 0.110 40.945 0.000 4.283 4.708 ## 20 justify ~1 -0.064 0.083 -0.775 0.438 -0.218 0.109 ## 21 skepxjusti ~1 0.828 0.179 4.632 0.000 0.526 1.218 ## 22 frame ~1 0.479 0.036 13.458 0.000 0.397 0.540 ## 23 skeptic ~1 0.000 0.142 0.000 1.000 -0.274 0.286 ## 24 skepxframe ~1 0.021 0.099 0.206 0.837 -0.173 0.223 ## 25 IndMedMod := a*b2 IndMedMod 0.001 0.005 0.189 0.850 -0.006 0.018 ## 26 bLow := b1+b2*(-1.78) bLow -0.937 0.103 -9.121 0.000 -1.142 -0.730 ## 27 bMedian := b1+b2*(-0.58) bMedian -0.929 0.085 -10.877 0.000 -1.096 -0.756 ## 28 bMean := b1+b2*(0) bMean -0.925 0.081 -11.467 0.000 -1.085 -0.762 ## 29 bHigh := b1+b2*1.82 bHigh -0.912 0.086 -10.622 0.000 -1.084 -0.741 ## 30 abLow := a*bLow abLow -0.126 0.123 -1.024 0.306 -0.372 0.129 ## 31 abMedian := a*bMedian abMedian -0.125 0.122 -1.022 0.307 -0.366 0.127 ## 32 abMean := a*bMean abMean -0.124 0.122 -1.021 0.307 -0.361 0.132 ## 33 abHigh := a*bHigh abHigh -0.122 0.121 -1.012 0.312 -0.349 0.133 Here we have two interaction coefficients: b2 = 0.007 (p = 0.258) c3prime = 0.009 (p = 0.905) so that skeptic is not a moderator for the b path nor for the cprime path. For the simple slopes of b path: bLow = -0.937 (p = 0.000) bMedian = -0.929 (p = 0.000) bMean = -0.925 (p = 0.000) bHigh = -0.912 (p = 0.000) which do not change much as their climate change skepticism change. Justification for withholding aid always leads to less donation. Skeptic is not an effective moderator. Let’s look at the indirect effects: IndexOfModMed = 0.001 (p = 0.850) abLow = -0.126 (p = 0.306) abMedian = -0.125 (p = 0.307) abMean = -0.124 (p = 0.307) abHigh = -0.122 (p = 0.312) Similarly, skeptic is not a good moderator for the indirect effect given that IndexOfModMed is not significant and the indirect effects at high/low levels of skepticism do not differ much. 7.10 Model 4f: Moderated Mediation Model - Path a and b par(mfrow = c(1,2), mar=c(0,0,0,0), oma=c(0,0,0,0)) pmacroModel(58,labels=labels) statisticalDiagram(58, labels=labels) When both a and b are moderated by the same moderator, we have: a = a1 + a3*skep b = b1 + b3*skep ab = (a1 + a3*skep)*(b1 + b3*skep) = a1*b1 + (a1*b3+a3*b1)*skep + a3*b3*skep^2 So the indirect effect does not depend on the moderator in a linear way. We don’t have a formal definition of index of moderated mediation in this scenario. If we are lucky, we might get both (a1*b3+a3*b1) and a3*b3 to be significant… Let’s write the syntax for the moderated mediation model: lm4f.syntax &lt;- &#39; donate ~ b1*justify + b2*skeptic + b3*skepxjusti + cprime*frame justify ~ a1*frame + a2*skeptic + a3*skepxframe # index of moderated mediation IndMedMod1:= a1*b3+a3*b1 IndMedMod2:= a3*b3 # simple slope of justify on frame is a1+a3*skeptic aLow: = a1+a3*(-1.78) aMedian: = a1+a3*(-0.58) aMean: = a1+a3*(0) aHigh: = a1+a3*1.82 # simple slope of donate on justify is b1+b3*skeptic bLow: = b1+b3*(-1.78) bMedian: = b1+b3*(-0.58) bMean: = b1+b3*(0) bHigh: = b1+b3*1.82 # conditional indirect effects is a*(b1+b3*skeptic) abLow: = aLow*bLow abMedian: = aMedian*bMedian abMean: = aMean*bMean abHigh: = aHigh*bHigh &#39; set.seed(2022) lm4f.fit = sem(lm4f.syntax, data = disaster, se = &quot;bootstrap&quot;, bootstrap = 1000, fixed.x=FALSE, meanstructure = TRUE) parameterEstimates(lm4f.fit, level = 0.95, boot.ci.type = &quot;bca.simple&quot;) ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 donate ~ justify b1 -0.922 0.079 -11.611 0.000 -1.085 -0.772 ## 2 donate ~ skeptic b2 -0.040 0.041 -0.968 0.333 -0.127 0.039 ## 3 donate ~ skepxjusti b3 0.008 0.026 0.327 0.743 -0.043 0.059 ## 4 donate ~ frame cprime 0.204 0.140 1.455 0.146 -0.064 0.492 ## 5 justify ~ frame a1 0.117 0.114 1.023 0.306 -0.113 0.324 ## 6 justify ~ skeptic a2 0.105 0.043 2.459 0.014 0.020 0.194 ## 7 justify ~ skepxframe a3 0.201 0.063 3.201 0.001 0.077 0.326 ## 8 donate ~~ donate 0.943 0.116 8.129 0.000 0.736 1.200 ## 9 justify ~~ justify 0.648 0.066 9.767 0.000 0.539 0.809 ## 10 skeptic ~~ skeptic 4.113 0.498 8.257 0.000 3.211 5.198 ## 11 skeptic ~~ skepxjusti 2.067 0.867 2.383 0.017 0.691 4.089 ## 12 skeptic ~~ frame 0.021 0.074 0.277 0.782 -0.125 0.168 ## 13 skeptic ~~ skepxframe 1.957 0.349 5.606 0.000 1.333 2.721 ## 14 skepxjusti ~~ skepxjusti 7.017 2.414 2.907 0.004 3.602 13.111 ## 15 skepxjusti ~~ frame 0.204 0.091 2.233 0.026 0.045 0.414 ## 16 skepxjusti ~~ skepxframe 1.828 0.784 2.332 0.020 0.600 3.681 ## 17 frame ~~ frame 0.250 0.003 97.289 0.000 0.244 0.250 ## 18 frame ~~ skepxframe 0.011 0.052 0.207 0.836 -0.089 0.121 ## 19 skepxframe ~~ skepxframe 1.957 0.348 5.618 0.000 1.328 2.708 ## 20 donate ~1 4.499 0.110 40.942 0.000 4.288 4.708 ## 21 justify ~1 -0.060 0.079 -0.759 0.448 -0.208 0.109 ## 22 skeptic ~1 0.000 0.142 0.000 1.000 -0.274 0.286 ## 23 skepxjusti ~1 0.828 0.179 4.632 0.000 0.526 1.218 ## 24 frame ~1 0.479 0.036 13.458 0.000 0.396 0.540 ## 25 skepxframe ~1 0.021 0.099 0.206 0.837 -0.173 0.223 ## 26 IndMedMod1 := a1*b3+a3*b1 IndMedMod1 -0.185 0.062 -2.975 0.003 -0.324 -0.071 ## 27 IndMedMod2 := a3*b3 IndMedMod2 0.002 0.005 0.330 0.741 -0.009 0.012 ## 28 aLow := a1+a3*(-1.78) aLow -0.241 0.154 -1.568 0.117 -0.563 0.045 ## 29 aMedian := a1+a3*(-0.58) aMedian 0.000 0.117 0.004 0.997 -0.240 0.209 ## 30 aMean := a1+a3*(0) aMean 0.117 0.115 1.023 0.307 -0.113 0.324 ## 31 aHigh := a1+a3*1.82 aHigh 0.483 0.168 2.874 0.004 0.138 0.829 ## 32 bLow := b1+b3*(-1.78) bLow -0.937 0.103 -9.140 0.000 -1.138 -0.729 ## 33 bMedian := b1+b3*(-0.58) bMedian -0.927 0.085 -10.904 0.000 -1.096 -0.758 ## 34 bMean := b1+b3*(0) bMean -0.922 0.079 -11.605 0.000 -1.085 -0.772 ## 35 bHigh := b1+b3*1.82 bHigh -0.907 0.079 -11.427 0.000 -1.073 -0.752 ## 36 abLow := aLow*bLow abLow 0.226 0.150 1.501 0.133 -0.035 0.578 ## 37 abMedian := aMedian*bMedian abMedian 0.000 0.109 -0.004 0.997 -0.192 0.234 ## 38 abMean := aMean*bMean abMean -0.108 0.105 -1.028 0.304 -0.304 0.096 ## 39 abHigh := aHigh*bHigh abHigh -0.438 0.161 -2.727 0.006 -0.779 -0.123 Here we have two interaction terms: a3 = 0.201 (p = 0.001) b3 = 0.008 (p = 0.743) which means that skeptic is a moderator for the a path but not a moderator for the b path. As for the index of moderated mediation: IndMedMod1 = -0.185 (p = 0.003) IndMedMod2 = 0.002 (p = 0.741) So IndMedMod1 is sig but IndMedMod2 is not (meh)… Let’s examine the indirect effects at different levels: abLow = 0.226 (p = 0.133) abMedian = 0.000 (p = 0.997) abMean = -0.108 (p = 0.304) abHigh = -0.438 (p = 0.006) which does vary as a function of skepticism. So, we still have a significant moderated mediation in this model. 7.11 Model 4g: Moderated Mediation Model - Path a, b, and cprime par(mfrow = c(1,2), mar=c(0,0,0,0), oma=c(0,0,0,0)) pmacroModel(59,labels=labels) statisticalDiagram(59, labels=labels) Let’s write the syntax for the moderated mediation model: lm4g.syntax &lt;- &#39; donate ~ b1*justify + b2*skepxjusti + c1prime*frame + c2prime*skeptic + c3prime*skepxframe justify ~ a1*frame + a2*skeptic + a3*skepxframe # index of moderated mediation IndMedMod1:= a1*b2+a3*b1 IndMedMod2:= a3*b2 # simple slope of justify on frame is a1+a3*skeptic aLow: = a1+a3*(-1.78) aMedian: = a1+a3*(-0.58) aMean: = a1+a3*(0) aHigh: = a1+a3*1.82 # simple slope of donate on justify is b1+b2*skeptic bLow: = b1+b2*(-1.78) bMedian: = b1+b2*(-0.58) bMean: = b1+b2*(0) bHigh: = b1+b2*1.82 # conditional indirect effects is a*(b1+b2*skeptic) abLow: = aLow*bLow abMedian: = aMedian*bMedian abMean: = aMean*bMean abHigh: = aHigh*bHigh &#39; set.seed(2022) lm4g.fit = sem(lm4g.syntax, data = disaster, se = &quot;bootstrap&quot;, bootstrap = 1000, fixed.x=FALSE, meanstructure = TRUE) parameterEstimates(lm4g.fit, level = 0.95, boot.ci.type = &quot;bca.simple&quot;) ## lhs op rhs label est se z pvalue ci.lower ci.upper ## 1 donate ~ justify b1 -0.925 0.081 -11.473 0.000 -1.085 -0.762 ## 2 donate ~ skepxjusti b2 0.007 0.028 0.258 0.797 -0.047 0.063 ## 3 donate ~ frame c1prime 0.205 0.143 1.432 0.152 -0.075 0.500 ## 4 donate ~ skeptic c2prime -0.043 0.058 -0.749 0.454 -0.165 0.058 ## 5 donate ~ skepxframe c3prime 0.009 0.079 0.119 0.905 -0.139 0.171 ## 6 justify ~ frame a1 0.117 0.114 1.023 0.306 -0.113 0.324 ## 7 justify ~ skeptic a2 0.105 0.043 2.459 0.014 0.020 0.194 ## 8 justify ~ skepxframe a3 0.201 0.063 3.201 0.001 0.077 0.326 ## 9 donate ~~ donate 0.943 0.116 8.148 0.000 0.744 1.212 ## 10 justify ~~ justify 0.648 0.066 9.767 0.000 0.539 0.809 ## 11 skepxjusti ~~ skepxjusti 7.017 2.414 2.907 0.004 3.602 13.111 ## 12 skepxjusti ~~ frame 0.204 0.091 2.233 0.026 0.045 0.414 ## 13 skepxjusti ~~ skeptic 2.067 0.867 2.383 0.017 0.691 4.089 ## 14 skepxjusti ~~ skepxframe 1.828 0.784 2.332 0.020 0.600 3.681 ## 15 frame ~~ frame 0.250 0.003 97.289 0.000 0.245 0.250 ## 16 frame ~~ skeptic 0.021 0.074 0.277 0.782 -0.125 0.168 ## 17 frame ~~ skepxframe 0.011 0.052 0.207 0.836 -0.089 0.121 ## 18 skeptic ~~ skeptic 4.113 0.498 8.257 0.000 3.211 5.198 ## 19 skeptic ~~ skepxframe 1.957 0.349 5.606 0.000 1.333 2.721 ## 20 skepxframe ~~ skepxframe 1.957 0.348 5.618 0.000 1.328 2.708 ## 21 donate ~1 4.499 0.110 40.945 0.000 4.283 4.708 ## 22 justify ~1 -0.060 0.079 -0.759 0.448 -0.208 0.109 ## 23 skepxjusti ~1 0.828 0.179 4.632 0.000 0.526 1.218 ## 24 frame ~1 0.479 0.036 13.458 0.000 0.396 0.540 ## 25 skeptic ~1 0.000 0.142 0.000 1.000 -0.274 0.286 ## 26 skepxframe ~1 0.021 0.099 0.206 0.837 -0.173 0.223 ## 27 IndMedMod1 := a1*b2+a3*b1 IndMedMod1 -0.185 0.063 -2.948 0.003 -0.321 -0.071 ## 28 IndMedMod2 := a3*b2 IndMedMod2 0.001 0.006 0.257 0.797 -0.009 0.014 ## 29 aLow := a1+a3*(-1.78) aLow -0.241 0.154 -1.568 0.117 -0.563 0.045 ## 30 aMedian := a1+a3*(-0.58) aMedian 0.000 0.117 0.004 0.997 -0.240 0.209 ## 31 aMean := a1+a3*(0) aMean 0.117 0.115 1.023 0.307 -0.113 0.324 ## 32 aHigh := a1+a3*1.82 aHigh 0.483 0.168 2.874 0.004 0.138 0.829 ## 33 bLow := b1+b2*(-1.78) bLow -0.937 0.103 -9.121 0.000 -1.142 -0.730 ## 34 bMedian := b1+b2*(-0.58) bMedian -0.929 0.085 -10.877 0.000 -1.096 -0.756 ## 35 bMean := b1+b2*(0) bMean -0.925 0.081 -11.467 0.000 -1.085 -0.762 ## 36 bHigh := b1+b2*1.82 bHigh -0.912 0.086 -10.622 0.000 -1.084 -0.741 ## 37 abLow := aLow*bLow abLow 0.226 0.151 1.500 0.134 -0.036 0.580 ## 38 abMedian := aMedian*bMedian abMedian 0.000 0.109 -0.004 0.997 -0.194 0.233 ## 39 abMean := aMean*bMean abMean -0.108 0.105 -1.027 0.304 -0.306 0.094 ## 40 abHigh := aHigh*bHigh abHigh -0.441 0.164 -2.685 0.007 -0.783 -0.118 Here we have interaction terms: a3 = 0.201 (p = 0.001) b2 = 0.007 (p = 0.797) c3prime = 0.009 (p = 0.905) which means that skeptic is a moderator for the a path but not a moderator for the b path or the cprime path. As for the index of moderated mediation: IndMedMod1 = -0.185 (p = 0.003) IndMedMod2 = 0.001 (p = 0.797) So IndMedMod1 is sig but IndMedMod2 is not (again)… Let’s examine the indirect effects at different levels: abLow = 0.226 (p = 0.134) abMedian = 0.000 (p = 0.997) abMean = -0.108 (p = 0.304) abHigh = -0.441 (p = 0.007) which does vary as a function of skepticism. So, we still have a significant moderated mediation in this model. 7.12 Conclusions Although the total effect of frame on donation is not significant to begin with (in Model 1), it should not discourage you from looking for mediators and moderators on any of the paths. In the simple mediation model (model 2), only b path is significantly negative, meaning that justification for withholding aid always leads to less donation regardless of the skepticism towards climate change. Although a path was not significant, again, it should not discourage you from looking for mediators and moderators on that a path. Including a moderator skeptic for a path and testing the moderated mediation models in Model 4a-Model 4g showed that skeptic is only a moderator for the a path, meaning that those who are high on climate change skepticism think it’s justified to withhold the aid whereas those who are low on the skepticism think it’s not justified to withhold the aid. Comparing Model 4a/4d/4f/4g (which all involve moderate a path), all the simple slopes and indirect effects of a path are more or less the same, and I recommend reported model 4g. Our final conclusion is: a path was moderated by skepticism, b path was not moderated by skepticism but b path itself is significant, cprime was not moderated by skepticism. The indirect path ab was also moderated by skepticism. In particular, framing the disaster as caused by climate change (X) leads to less donation (Y) for people who doubt climate change (W_high) but it leads to more donation (Y) for people who believes it (W_low) because they do not think it’s justified to withhold the aid (M). Ignoring this moderator leads to an insignificant mediation effect in Model 2. According to Hayes (2017, p. 439): “Climate change skeptics seem to feel that victims of a climate change induced disaster (compared to one not attributed to climate change) don’t deserve assistance, and this belief may translate into a reduced willingness to personally donate to the victims. This is a negative indirect effect. But among believers in climate change, the opposite effect is observed, with a climate change induced disaster leading believers to see the victims as more worthy of assistance than if the disaster wasn’t caused by climate change, and this is related to a greater willingness to donate. This is a positive indirect effect. Ignoring the contingency of the indirect effect by failing to include moderation by climate change skepticism in the mediation model obscures the conditional nature of the mechanism at work.” "],["lavaan-lab-5-one-factor-cfa-model.html", "Chapter 8 Lavaan Lab 5: One-factor CFA Model 8.1 Data Prep 8.2 PART I: One-Factor CFA, Fixed Loading 8.3 PART II: One-Factor CFA, Fixed Factor Variance 8.4 Exercise: One-factor CFA Model", " Chapter 8 Lavaan Lab 5: One-factor CFA Model In this lab, we will learn how to: Identify the One-factor CFA Model Scale the One-factor CFA Model Estimate the One-factor CFA Model Interpret the One-factor CFA Model 8.1 Data Prep We will use cfaInClassData.csv in this lab. This is a simulated dataset based on Todd Little’s positive affect example. The hypothesis is that a latent variable ‘positive affect’ is measured by three indicators (glad, cheerful, and happy). Let’s read this dataset in: cfaData&lt;- read.csv(&quot;cfaInclassData.csv&quot;, header = T) and examine the dataset: head(cfaData) ## ID glad cheerful happy satisfied content comfortable ## 1 1 0.13521092 0.5413297 -0.1041445 -0.5777446 0.8645383 0.02935020 ## 2 2 -0.29116043 0.2434081 0.6671535 2.0763730 -0.7382832 1.05439183 ## 3 3 0.71975913 0.2218277 0.4722337 2.1685984 -0.2727574 0.09053090 ## 4 4 0.44432030 0.9295414 0.8574083 -1.0575363 -1.3841364 -0.07940091 ## 5 5 2.84476524 3.1710123 3.5145040 1.5725274 2.3406754 1.59866763 ## 6 6 -0.03317526 -0.8434011 -0.1485924 -0.5469343 -1.5750953 -0.69629828 str(cfaData) ## &#39;data.frame&#39;: 1000 obs. of 7 variables: ## $ ID : int 1 2 3 4 5 6 7 8 9 10 ... ## $ glad : num 0.135 -0.291 0.72 0.444 2.845 ... ## $ cheerful : num 0.541 0.243 0.222 0.93 3.171 ... ## $ happy : num -0.104 0.667 0.472 0.857 3.515 ... ## $ satisfied : num -0.578 2.076 2.169 -1.058 1.573 ... ## $ content : num 0.865 -0.738 -0.273 -1.384 2.341 ... ## $ comfortable: num 0.0294 1.0544 0.0905 -0.0794 1.5987 ... dim(cfaData) #n = 1000, 7 variables ## [1] 1000 7 Let’s examine their means and standard deviations: round(apply(cfaData[,-1], 2, mean), 2) # mean-centered ## glad cheerful happy satisfied content comfortable ## 0.00 0.00 0.01 -0.04 -0.04 -0.04 round(apply(cfaData[,-1], 2, sd), 2) ## glad cheerful happy satisfied content comfortable ## 0.98 0.98 0.98 1.01 1.08 0.95 Let’s call up the lavaan library and run some CFA’s! library(lavaan) 8.2 PART I: One-Factor CFA, Fixed Loading 8.2.1 Fixed Loading, AKA Marker Variable method. FYI, the three equations for the three indicators are: Glad = lambda1*posAffect(eta) + u1 Cheerful = lambda2*posAffect(eta) + u2 Happy = lambda3*posAffect(eta) + u3 Let’s first follow the equations above and write the syntax (disturbances are automatically included): mod1.wrong&lt;- &quot; glad ~ posAffect cheerful ~ posAffect happy ~ posAffect &quot; fit1.wrong = lavaan::sem(model = mod1.wrong, data = cfaData, fixed.x=FALSE) Oops - an error message! Error in lav_data_full(data = data, group = group, cluster = cluster, : lavaan ERROR: missing observed variables in dataset: posAffect This is because posAffect is a latent variable and we have to use =~ to define a latent variable: mod1.wrong&lt;-&#39; posAffect =~ Glad + Cheerful + Happy &#39; fit1.wrong = lavaan::sem(model = mod1.wrong, data = cfaData, fixed.x=FALSE) Error in lavaan::lavaan(model = mod1.wrong, data = cfaData, fixed.x = FALSE, : lavaan ERROR: missing observed variables in dataset: Glad Cheerful Happy Error, why? The variable names in the model syntax have to match the column names EXACTLY, even the letter cases. Let’s try again: mod1&lt;-&#39; posAffect =~ glad + cheerful + happy &#39; Let’s explain the lavaan model syntax! mod1 is used to name our model. Since posAffect is a latent variable (it’s not in the data), we cannot follow the equations above and write syntax like glad ~ posAffect Instead, we specify a CFA measurement model in mod1. NEW SYNTAX ALERT: Using =~ means “manifested by” In the code above we can see that our latent construct ‘posAffect’ is manifested by glad, cheerful, and happy By default, the loading of glad is fixed at 1 (Fixed Loading Method) Next we name the fitted object ‘fit1’ to see our output. fit1 = lavaan::sem(mod1, data = cfaData, fixed.x=FALSE) This summary will show us the loadings (I also requested standardized results): summary(fit1, standardized = T) ## lavaan 0.6-12 ended normally after 20 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.693 0.705 ## cheerful 1.117 0.059 18.782 0.000 0.774 0.787 ## happy 1.066 0.057 18.786 0.000 0.739 0.757 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.485 0.030 16.238 0.000 0.485 0.503 ## .cheerful 0.367 0.030 12.062 0.000 0.367 0.380 ## .happy 0.407 0.030 13.751 0.000 0.407 0.427 ## posAffect 0.480 0.043 11.270 0.000 1.000 1.000 df = 0 (why?) Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all posAffect =~ glad 1.000 0.693 0.705 cheerful 1.117 0.059 18.782 0.000 0.774 0.787 happy 1.066 0.057 18.786 0.000 0.739 0.757 What does this mean? 1 unit change in posAffect produces: 1-unit change in “glad” (marker indicator) 1.117-unit change in “cheerful” (1.117 times greater than the effect on “glad”) 1.066-unit change in “happy” (1.066 times greater than the effect on “glad”) Variances: Unique factor variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .glad 0.485 0.030 16.238 0.000 0.485 0.503 .cheerful 0.367 0.030 12.062 0.000 0.367 0.380 .happy 0.407 0.030 13.751 0.000 0.407 0.427 The leftover unique factor variances remain substantial Meaning that none of the indicators is a perfect measure of posAffect but they all contribute significantly to the measurement of posAffect (the standardized loadings above larger than 0.6) Followed by the latent factor variance. Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all posAffect 0.480 0.043 11.270 0.000 1.000 1.000 8.2.2 Change marker indicator If you’d like to fix the 2nd loading to 1: mod1b_wrong&lt;-&#39; posAffect =~ glad + 1*cheerful + happy &#39; won’t work. You will get something like this: Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all posAffect =~ glad 1.000 0.734 0.733 cheerful 1.000 0.734 0.759 happy 1.009 0.046 22.052 0.000 0.741 0.759 You’ll have to change the order of the indicators to move cheerful to the front of the variable list: mod1b&lt;-&#39; posAffect =~ cheerful + glad + happy &#39; Or use *NA to specify which loading to keep free and use *1 to specify the marker variable whose loading to be fixed at 1 mod1b&lt;-&#39; posAffect =~ NA*glad + 1*cheerful + NA*happy &#39; Here we named the fitted object ‘fit1b’ to see our output. fit1b = lavaan::sem(mod1b, data = cfaData, fixed.x=FALSE) summary(fit1b, standardized = T) ## lavaan 0.6-12 ended normally after 19 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 0.895 0.048 18.782 0.000 0.693 0.705 ## cheerful 1.000 0.774 0.787 ## happy 0.954 0.050 19.130 0.000 0.739 0.757 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.485 0.030 16.238 0.000 0.485 0.503 ## .cheerful 0.367 0.030 12.062 0.000 0.367 0.380 ## .happy 0.407 0.030 13.751 0.000 0.407 0.427 ## posAffect 0.599 0.048 12.616 0.000 1.000 1.000 The loadings can be obtained by dividing those in fit1 by 1.117 (i.e., they change proportionally). The variances of unique factors and latent factor remain unchanged. 8.3 PART II: One-Factor CFA, Fixed Factor Variance 8.3.1 Fixed Factor Method Keep using the same syntax but assign a new name mod2: mod2&lt;-&#39; posAffect =~ glad + cheerful + happy &#39; To fix the variance of the latent variable to 1, add std.lv=T to sem() function: fit2&lt;-lavaan::sem(mod2, data = cfaData, fixed.x=FALSE, std.lv=T) summary(fit2, standardized = TRUE) ## lavaan 0.6-12 ended normally after 11 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 0.693 0.031 22.540 0.000 0.693 0.705 ## cheerful 0.774 0.031 25.233 0.000 0.774 0.787 ## happy 0.739 0.030 24.226 0.000 0.739 0.757 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.485 0.030 16.238 0.000 0.485 0.503 ## .cheerful 0.367 0.030 12.062 0.000 0.367 0.380 ## .happy 0.407 0.030 13.751 0.000 0.407 0.427 ## posAffect 1.000 1.000 1.000 Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all posAffect =~ glad 0.693 0.031 22.540 0.000 0.693 0.705 cheerful 0.774 0.031 25.233 0.000 0.774 0.787 happy 0.739 0.030 24.226 0.000 0.739 0.757 1-SD change in the factor (posAffect) causes: 0.693-unit change in glad (on its raw scale) 0.774-unit change in cheerful (on its raw scale) 0.739-unit change in happy (on its raw scale) Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .glad 0.485 0.030 16.238 0.000 0.485 0.503 .cheerful 0.367 0.030 12.062 0.000 0.367 0.380 .happy 0.407 0.030 13.751 0.000 0.407 0.427 posAffect 1.000 1.000 1.000 We see that posAffect now has variance (=sd) of 1 All loadings were freely estimated, no loading is 1. and the unique factor variances are the same as before 8.4 Exercise: One-factor CFA Model Could you use the indicators satisfied, content, and comfortable to build a one-factor CFA model to measure a latent variable called Satisfaction? Use the Fixed Loading and the Fixed Factor Methods and compare their estimates. 8.4.1 Fixed Loading 8.4.2 Fixed Factor "],["lavaan-lab-6-two-factor-cfa-model.html", "Chapter 9 Lavaan Lab 6: Two-factor CFA Model 9.1 Data Prep 9.2 PART I: Two-Factor CFA, Fixed Loading 9.3 PART II: Two-Factor CFA, Fixed Factor Variance", " Chapter 9 Lavaan Lab 6: Two-factor CFA Model 9.1 Data Prep We will continue to use cfaInClassData.csv in this lab. Let’s read this dataset in: cfaData&lt;- read.csv(&quot;cfaInclassData.csv&quot;, header = T) Load up the lavaan library and run some CFA’s! library(lavaan) 9.2 PART I: Two-Factor CFA, Fixed Loading 9.2.1 Fixed Loading, AKA Marker Variable method. Let’s write up the model syntax for the measurement model with two factors: fixedIndTwoFacSyntax &lt;- &quot; #Factor Specification posAffect =~ glad + happy + cheerful satisfaction =~ satisfied + content + comfortable &quot; Here we named the fitted object ‘fixedIndTwoFacRun’ to see our output: fixedIndTwoFacRun = lavaan::sem(model = fixedIndTwoFacSyntax, data = cfaData, fixed.x=FALSE) Get a summary using summary() function, add standardized=T to request standardized parameter estimates: summary(fixedIndTwoFacRun, standardized=T) ## lavaan 0.6-12 ended normally after 24 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 13 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 2.957 ## Degrees of freedom 8 ## P-value (Chi-square) 0.937 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.694 0.706 ## happy 1.067 0.055 19.294 0.000 0.740 0.758 ## cheerful 1.112 0.057 19.458 0.000 0.772 0.785 ## satisfaction =~ ## satisfied 1.000 0.773 0.767 ## content 1.068 0.052 20.525 0.000 0.826 0.762 ## comfortable 0.918 0.045 20.336 0.000 0.709 0.746 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect ~~ ## satisfaction 0.262 0.025 10.284 0.000 0.488 0.488 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.484 0.029 16.647 0.000 0.484 0.501 ## .happy 0.405 0.028 14.389 0.000 0.405 0.425 ## .cheerful 0.371 0.029 13.004 0.000 0.371 0.384 ## .satisfied 0.419 0.029 14.326 0.000 0.419 0.412 ## .content 0.491 0.034 14.542 0.000 0.491 0.419 ## .comfortable 0.400 0.026 15.315 0.000 0.400 0.443 ## posAffect 0.482 0.042 11.439 0.000 1.000 1.000 ## satisfaction 0.597 0.047 12.686 0.000 1.000 1.000 summary(fixedIndTwoFacRun, standardized = T) ## lavaan 0.6-12 ended normally after 24 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 13 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 2.957 ## Degrees of freedom 8 ## P-value (Chi-square) 0.937 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.694 0.706 ## happy 1.067 0.055 19.294 0.000 0.740 0.758 ## cheerful 1.112 0.057 19.458 0.000 0.772 0.785 ## satisfaction =~ ## satisfied 1.000 0.773 0.767 ## content 1.068 0.052 20.525 0.000 0.826 0.762 ## comfortable 0.918 0.045 20.336 0.000 0.709 0.746 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect ~~ ## satisfaction 0.262 0.025 10.284 0.000 0.488 0.488 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.484 0.029 16.647 0.000 0.484 0.501 ## .happy 0.405 0.028 14.389 0.000 0.405 0.425 ## .cheerful 0.371 0.029 13.004 0.000 0.371 0.384 ## .satisfied 0.419 0.029 14.326 0.000 0.419 0.412 ## .content 0.491 0.034 14.542 0.000 0.491 0.419 ## .comfortable 0.400 0.026 15.315 0.000 0.400 0.443 ## posAffect 0.482 0.042 11.439 0.000 1.000 1.000 ## satisfaction 0.597 0.047 12.686 0.000 1.000 1.000 Here is the fun part. Plot the fitted model using semPaths() function from the semPlot package: library(semPlot) semPaths(fixedIndTwoFacRun) semPaths(fixedIndTwoFacRun, what = &#39;est&#39;) # under &quot;Estimate&quot; df = 8 (why?) Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all posAffect =~ glad 1.000 0.694 0.706 happy 1.067 0.055 19.294 0.000 0.740 0.758 cheerful 1.112 0.057 19.458 0.000 0.772 0.785 satisfaction =~ satisfied 1.000 0.773 0.767 content 1.068 0.052 20.525 0.000 0.826 0.762 comfortable 0.918 0.045 20.336 0.000 0.709 0.746 What does this mean? 1 unit change in posAffect (factor) produces: 1-unit change in “glad” (marker indicator) 1.067-unit change in “happy” (1.067 times greater than the effect on “glad”) 1.112-unit change in “cheerful” (1.112 times greater than the effect on “glad”) 1 unit change in satisfaction (factor) produces: 1-unit change in “satisfied” (marker indicator) 1.068-unit change in “content” (1.068 times greater than the effect on “satisfied”) 0.918-unit change in “comfortable” (0.918 times greater than the effect on “satisfied”) Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .glad 0.484 0.029 16.647 0.000 0.484 0.501 .happy 0.405 0.028 14.389 0.000 0.405 0.425 .cheerful 0.371 0.029 13.004 0.000 0.371 0.384 .satisfied 0.419 0.029 14.326 0.000 0.419 0.412 .content 0.491 0.034 14.542 0.000 0.491 0.419 .comfortable 0.400 0.026 15.315 0.000 0.400 0.443 The leftover unique factor variances remain substantial Meaning that none of the indicators is a perfect measure of posAffect or satisfaction but they all contribute significantly to the measurement of latent variables (the standardized loadings above larger than 0.6) Followed by two factor variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all posAffect 0.482 0.042 11.439 0.000 1.000 1.000 satisfaction 0.597 0.047 12.686 0.000 1.000 1.000 which were freely estimated using Fixed Loading scaling method. Both posAffect and satisfaction are variable across participants (sig* according to p-values) posAffect seems to be more stable than satisfaction (0.482&lt;0.597). Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all posAffect ~~ satisfaction 0.262 0.025 10.284 0.000 0.488 0.488 covariance between posAffect and satisfaction is 0.262 standardized covariance (correlation) between posAffect and satisfaction is 0.488 (sig*) 0.262/sqrt(0.482)/sqrt(0.597) = 0.488 posAffect is positively correlated with satisfaction. Participants who have a high level posAffect tend to have a high level of satisfaction. 9.2.2 How well does this model fit to the data? The model-implied variance of glad is the sum of: Tracing 1: lam1*psi1*lam1 = 1.0*0.482*1.0 = 0.482 Tracing 2: sig2_u1 = sig2_glad = 0.484 which is 0.966. What is the sample variance of glad? var(cfaData$glad) = 0.9664733, which is super close to the model-implied one! meaning our CFA model is doing a pretty good job at explaining the sample variance of glad. The proportion of variance of glad explained by posAffect is: Tracing 1 / (Tracing 1+Tracing 2) = 49.9% Unexplained: 50.1% Exercise What about the model-implied variance of comfortable? Tracing 1: 0.918*.597*.918 = .503 Tracing 2: .400 Total .903 Proportion of variance explained: .503/.903 Sample var: var(cfaData$comfortable) = 0.904 Exercise What about the model-implied covariance between glad and comfortable? (e.g., cov(y1, y6)) Tracing 1: 1*.262*.918 = .241 Sample cov: cov(cfaData\\(glad, cfaData\\)comfortable) = .230 Overall speaking, how is our model doing at explaining the sample covariance matrix? Remember the fitted() function we used during path analysis R labs? Apply fitted() function to the fitted object fixedIndTwoFacRun (not on the model syntax fixedIndTwoFacSyntax!!): fitted(fixedIndTwoFacRun) ## $cov ## glad happy cherfl satsfd contnt cmfrtb ## glad 0.966 ## happy 0.514 0.953 ## cheerful 0.536 0.571 0.967 ## satisfied 0.262 0.279 0.291 1.016 ## content 0.280 0.298 0.311 0.638 1.173 ## comfortable 0.240 0.256 0.267 0.548 0.586 0.903 Sigma &lt;- fitted(fixedIndTwoFacRun)$cov fixed variance: fixedVarTwoFacRun = lavaan::sem(model = fixedIndTwoFacSyntax, data = cfaData, fixed.x=FALSE, std.lv = TRUE) Sigma2 &lt;- fitted(fixedVarTwoFacRun)$cov Sigma2 # same as Sigma! ## glad happy cherfl satsfd contnt cmfrtb ## glad 0.966 ## happy 0.514 0.953 ## cheerful 0.536 0.571 0.967 ## satisfied 0.262 0.279 0.291 1.016 ## content 0.280 0.298 0.311 0.638 1.173 ## comfortable 0.240 0.256 0.267 0.548 0.586 0.903 How close is Sigma to S? Rearrange the rows and columns of Sigma (important!) and take the difference between Sigma and S (S = cov(cfaData[,-1])) ## glad cheerful happy satisfied content comfortable ## glad 0.9664733 0.5370642 0.5123945 0.2781506 0.2813310 0.2295491 ## cheerful 0.5370642 0.9678301 0.5724804 0.2820439 0.3168253 0.2611358 ## happy 0.5123945 0.5724804 0.9537015 0.2851151 0.3127966 0.2438086 ## satisfied 0.2781506 0.2820439 0.2851151 1.0169865 0.6352282 0.5511595 ## content 0.2813310 0.3168253 0.3127966 0.6352282 1.1739185 0.5872279 ## comfortable 0.2295491 0.2611358 0.2438086 0.5511595 0.5872279 0.9042505 diff = Sigma[colnames(S), colnames(S)] - S round(diff, 4) ## glad cheerful happy satisfied content comfortable ## glad -0.0010 -0.0014 0.0013 -0.0164 -0.0017 0.0107 ## cheerful -0.0014 -0.0010 -0.0011 0.0091 -0.0058 0.0060 ## happy 0.0013 -0.0011 -0.0010 -0.0059 -0.0145 0.0124 ## satisfied -0.0164 0.0091 -0.0059 -0.0010 0.0030 -0.0030 ## content -0.0017 -0.0058 -0.0145 0.0030 -0.0012 -0.0016 ## comfortable 0.0107 0.0060 0.0124 -0.0030 -0.0016 -0.0009 print(paste0(&quot;The difference between S and Sigma ranged between &quot;, round(min(diff),4), &quot; and &quot;, round(max(diff),4), &quot;.&quot;)) ## [1] &quot;The difference between S and Sigma ranged between -0.0164 and 0.0124.&quot; This is the closest Sigma can get to S. Any other set of parameter estimates would yield bigger differences with S. Have you wondered about how we obtain the parameter estimates in the output? Estimation down the road… 9.2.3 Fundamental Equation of SEM Just some bonus stuff… You can inspect the fitted object using inspect() and save the object as InspFit1. InspFit1 &lt;- inspect(fixedIndTwoFacRun, what = &quot;est&quot;) Extract Lambda matrix from InspFit1. Lambda &lt;- InspFit1$lambda Lambda ## psAffc stsfct ## glad 1.000 0.000 ## happy 1.067 0.000 ## cheerful 1.112 0.000 ## satisfied 0.000 1.000 ## content 0.000 1.068 ## comfortable 0.000 0.918 Extract Psi matrix from InspFit1. Psi &lt;- InspFit1$psi Psi ## psAffc stsfct ## posAffect 0.482 ## satisfaction 0.262 0.597 Extract Theta matrix from InspFit1. Theta &lt;- InspFit1$theta Theta ## glad happy cherfl satsfd contnt cmfrtb ## glad 0.484 ## happy 0.000 0.405 ## cheerful 0.000 0.000 0.371 ## satisfied 0.000 0.000 0.000 0.419 ## content 0.000 0.000 0.000 0.000 0.491 ## comfortable 0.000 0.000 0.000 0.000 0.000 0.400 Use the three matrices above to calculate the model-implied covariance matrix and save is as SIGMA: SIGMA &lt;- Lambda%*%Psi%*%t(Lambda)+Theta SIGMA ## glad happy cherfl satsfd contnt cmfrtb ## glad 0.966 ## happy 0.514 0.953 ## cheerful 0.536 0.571 0.967 ## satisfied 0.262 0.279 0.291 1.016 ## content 0.280 0.298 0.311 0.638 1.173 ## comfortable 0.240 0.256 0.267 0.548 0.586 0.903 A shortcut function to obtain the SIGMA matrix is to use fitted() function, as shown above… all.equal(Sigma, SIGMA) ## [1] TRUE 9.2.4 Interpretion How do I interpret the results? Introduce the scaling method I used; Based on the loadings, acknowledge that the indicators are not perfect measures of the latent factors, but they all contribute significantly to the measurement of latent factors (the standardized loadings above larger than 0.6); Report the the proportions of variance explained on each indicator; Describe the discrepancy between S and Sigma (where the main differences lie, and whether the differences are concerning); Interpret the correlation among the latent factors (size, sign, positive/negative, sig/non-sig). 9.2.5 Standardized solutions: Std.lv vs. Std.all Std.lv: This is the solution you’ll get using Fixed Factor Variance Scaling Method; All factor variances are fixed as 1.0; Factor covariance is the same as factor correlation; All factor loadings are freely estimated so that the model-implied covariance matrix remains the same; All unique factor variances remain unchanged. For example, under Std.lv, the model-implied variance of glad is the sum of: Tracing 1: lam1*psi1*lam1 = 0.694*1.0*0.694 = 0.482 Tracing 2: sig2_u1 = sig2_glad = 0.484 Proportion of variance explained: 0.482/0.966 = 50.1% which is still 0.966. Exercise Why is 0.709 the loading of comfortable under Std.lv? Under Std.lv, the model-implied variance of comfortable is the sum of: Tracing 1: Tracing 2: Proportion of variance explained: Std.all: All factor variances are fixed as 1.0; Factor covariance is the same as factor correlation; All factor loadings and unique factor variances are re-estimated so that the model-implied variances of indicators are all 1.0; For example, under Std.all, the model-implied variance of glad is the sum of: Tracing 1: lam1*psi1*lam1 = 0.706*1.0*0.706 = 0.499 Tracing 2: sig2_u1 = sig2_glad = 0.501 which add to 1.0. Exercise Why is 0.746 the loading of comfortable under Std.all? Under Std.all, the model-implied variance of comfortable is the sum of: Tracing 1: Tracing 2: Why the hassle? The solution is fully standardized so that squaring Std.all loadings is equivalent to the proportion of variance explained: 0.706*0.706 (std.all) 1.0*0.482*1.0/0.966 (fixed loading) 0.694*1.0*0.694/0.966 (fixed variance; std.lv) = 49.9% 9.3 PART II: Two-Factor CFA, Fixed Factor Variance 9.3.1 Fixed Factor Method Keep using the same syntax but assign a new name: fixedFacTwoFacSyntax &lt;- &quot; #Factor Specification posAffect =~ glad + happy + cheerful satisfaction =~ satisfied + content + comfortable &quot; To fix the variance of the latent variable to 1, add std.lv=T to sem() function: fixedFacTwoFacRun = lavaan::sem(model = fixedFacTwoFacSyntax, data = cfaData, fixed.x=FALSE, std.lv=T) Get a summary using summary() function, add standardized=T to request standardized parameter estimates summary(fixedFacTwoFacRun, standardized=T) ## lavaan 0.6-12 ended normally after 16 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 13 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 2.957 ## Degrees of freedom 8 ## P-value (Chi-square) 0.937 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 0.694 0.030 22.878 0.000 0.694 0.706 ## happy 0.740 0.030 24.806 0.000 0.740 0.758 ## cheerful 0.772 0.030 25.798 0.000 0.772 0.785 ## satisfaction =~ ## satisfied 0.773 0.030 25.373 0.000 0.773 0.767 ## content 0.826 0.033 25.207 0.000 0.826 0.762 ## comfortable 0.709 0.029 24.584 0.000 0.709 0.746 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect ~~ ## satisfaction 0.488 0.032 15.097 0.000 0.488 0.488 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.484 0.029 16.647 0.000 0.484 0.501 ## .happy 0.405 0.028 14.389 0.000 0.405 0.425 ## .cheerful 0.371 0.029 13.004 0.000 0.371 0.384 ## .satisfied 0.419 0.029 14.326 0.000 0.419 0.412 ## .content 0.491 0.034 14.542 0.000 0.491 0.419 ## .comfortable 0.400 0.026 15.315 0.000 0.400 0.443 ## posAffect 1.000 1.000 1.000 ## satisfaction 1.000 1.000 1.000 df = 8 # same! Latent Variables: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all posAffect =~ glad 0.694 0.030 22.878 0.000 0.694 0.706 happy 0.740 0.030 24.806 0.000 0.740 0.758 cheerful 0.772 0.030 25.798 0.000 0.772 0.785 satisfaction =~ satisfied 0.773 0.030 25.373 0.000 0.773 0.767 content 0.826 0.033 25.207 0.000 0.826 0.762 comfortable 0.709 0.029 24.584 0.000 0.709 0.746 1-SD change in the factor (posAffect) causes: 0.694-unit change in glad (on its raw scale) 0.740-unit change in happy (on its raw scale) 0.772-unit change in cheerful (on its raw scale) 1-SD change in the factor (satisfaction) causes: 0.773-unit change in satisfied (on its raw scale) 0.826-unit change in content (on its raw scale) 0.709-unit change in comfortable (on its raw scale) Covariances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all posAffect ~~ satisfaction 0.262 0.025 10.284 0.000 0.488 0.488 Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .glad 0.484 0.029 16.647 0.000 0.484 0.501 .happy 0.405 0.028 14.389 0.000 0.405 0.425 .cheerful 0.371 0.029 13.004 0.000 0.371 0.384 .satisfied 0.419 0.029 14.326 0.000 0.419 0.412 .content 0.491 0.034 14.542 0.000 0.491 0.419 .comfortable 0.400 0.026 15.315 0.000 0.400 0.443 remain unchanged. Followed by two factor variances. Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all posAffect 1.000 1.000 1.000 satisfaction 1.000 1.000 1.000 "],["lavaan-lab-7-two-factor-sr-model.html", "Chapter 10 Lavaan Lab 7: Two-factor SR Model 10.1 Data Prep 10.2 PART I: Two-Factor SR, Fixed Loading 10.3 PART II: Two-Factor SR, Fixed Factor Variance 10.4 PART III: Exercise (what fun!): 3-Factor SR Model", " Chapter 10 Lavaan Lab 7: Two-factor SR Model 10.1 Data Prep Again, we use cfaInClassData.csv in this lab Let’s read this dataset in: cfaData&lt;- read.csv(&quot;cfaInclassData.csv&quot;, header = T) Load up the lavaan library: library(lavaan) 10.2 PART I: Two-Factor SR, Fixed Loading 10.2.1 Fixed Loading, AKA Marker Variable method. Let’s write up the model syntax for the structural regression (SR) model with two factors: srSyntax &lt;- &quot; #Factor Specification posAffect =~ glad + cheerful + happy satisfaction =~ satisfied + content + comfortable #Structural Regression! satisfaction ~ posAffect &quot; Here we named the fitted object ‘srRun’ to see our output: srRun = lavaan::sem(model = srSyntax, data = cfaData, fixed.x=FALSE) Get a summary using summary() function, add standardized=T to request standardized parameter estimates: summary(srRun, standardized = T) ## lavaan 0.6-12 ended normally after 22 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 13 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 2.957 ## Degrees of freedom 8 ## P-value (Chi-square) 0.937 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.694 0.706 ## cheerful 1.112 0.057 19.458 0.000 0.772 0.785 ## happy 1.067 0.055 19.294 0.000 0.740 0.758 ## satisfaction =~ ## satisfied 1.000 0.773 0.767 ## content 1.068 0.052 20.525 0.000 0.826 0.762 ## comfortable 0.918 0.045 20.336 0.000 0.709 0.746 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## satisfaction ~ ## posAffect 0.544 0.047 11.490 0.000 0.488 0.488 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.484 0.029 16.647 0.000 0.484 0.501 ## .cheerful 0.371 0.029 13.004 0.000 0.371 0.384 ## .happy 0.405 0.028 14.389 0.000 0.405 0.425 ## .satisfied 0.419 0.029 14.326 0.000 0.419 0.412 ## .content 0.491 0.034 14.542 0.000 0.491 0.419 ## .comfortable 0.400 0.026 15.315 0.000 0.400 0.443 ## posAffect 0.482 0.042 11.439 0.000 1.000 1.000 ## .satisfaction 0.455 0.038 11.869 0.000 0.762 0.762 The above syntax reproduces the SR analysis from the class slides. CFA part under “Latent Variables” remains unchanged “Covariances” section no longer exists satisfaction is also removed under “Variances” Instead, in the Regressions section: Regressions: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all satisfaction ~ posAffect 0.544 0.047 11.490 0.000 0.488 0.488 returns the regression slope of posAffect (b = 0.544). One-unit change in posAffect is leading to one-unit change in satisfaction. Variances: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all .satisfaction 0.455 0.038 11.869 0.000 0.762 0.762 returns the disturbance variance of satisfaction (sigma_(d_2)^2, not psi_2!) 10.3 PART II: Two-Factor SR, Fixed Factor Variance 10.3.1 Fixed Factor Method Here we named the fitted object ‘srRun2’. We add std.lv=T to fix all latent factor variances to 1: srRun2 = lavaan::sem(model = srSyntax, data = cfaData, fixed.x=FALSE, std.lv=T) summary(srRun2, standardized = T) ## lavaan 0.6-12 ended normally after 17 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 13 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 2.957 ## Degrees of freedom 8 ## P-value (Chi-square) 0.937 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 0.694 0.030 22.878 0.000 0.694 0.706 ## cheerful 0.772 0.030 25.798 0.000 0.772 0.785 ## happy 0.740 0.030 24.806 0.000 0.740 0.758 ## satisfaction =~ ## satisfied 0.675 0.028 23.737 0.000 0.773 0.767 ## content 0.721 0.031 23.611 0.000 0.826 0.762 ## comfortable 0.619 0.027 23.124 0.000 0.709 0.746 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## satisfaction ~ ## posAffect 0.559 0.049 11.501 0.000 0.488 0.488 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.484 0.029 16.647 0.000 0.484 0.501 ## .cheerful 0.371 0.029 13.004 0.000 0.371 0.384 ## .happy 0.405 0.028 14.389 0.000 0.405 0.425 ## .satisfied 0.419 0.029 14.326 0.000 0.419 0.412 ## .content 0.491 0.034 14.542 0.000 0.491 0.419 ## .comfortable 0.400 0.026 15.315 0.000 0.400 0.443 ## posAffect 1.000 1.000 1.000 ## .satisfaction 1.000 0.762 0.762 This doesn’t work as expected, why? Therefore, for SR models, we should always go with fixed loading scaling approach. 10.4 PART III: Exercise (what fun!): 3-Factor SR Model Suppose that in a 3-factor SR model: Positive Affect is measured by Happy and Cheerful Satisfaction is measured by Satisfied and Content Pleasure is measured by Glad and Comfortable Satisfaction is predicted by both Positive Affect and Pleasure Can you use cfaData to fit such a model? "],["lavaan-lab-8-estimation-methods.html", "Chapter 11 Lavaan Lab 8: Estimation Methods 11.1 PART I: Hypothetical Example 11.2 PART II: ULS on the Positive Affect Example 11.3 PART III: Calculate ULS test statistic manually 11.4 PART IV: ML vs ULS vs WLS 11.5 PART V: Improper Solutions", " Chapter 11 Lavaan Lab 8: Estimation Methods In this lab, we will learn how to estimate parameters in CFA/SR models. Load up the lavaan library: library(lavaan) 11.1 PART I: Hypothetical Example 11.1.1 One-factor CFA model A made-up sample covariance matrix with n = 200: n = 200 S_3fac = matrix(c(5, 2, 3.5, 2, 3, 2, 3.5, 2, 6), 3, 3, dimnames = list(c(&#39;Y1&#39;, &#39;Y2&#39;, &#39;Y3&#39;), c(&#39;Y1&#39;, &#39;Y2&#39;, &#39;Y3&#39;))) S_3fac ## Y1 Y2 Y3 ## Y1 5.0 2 3.5 ## Y2 2.0 3 2.0 ## Y3 3.5 2 6.0 Fit a one-factor CFA to the sample covariance matrix: one_fac_syntax &lt;- &quot; eta =~ Y1 + Y2 + Y3 &quot; Request Unweighted Least Squares (ULS): one_fac_fit2 &lt;- lavaan::sem(one_fac_syntax, sample.cov = S_3fac, sample.nobs = n, estimator = &quot;ULS&quot;, fixed.x = FALSE) summary(one_fac_fit2, standardized = T) ## lavaan 0.6-12 ended normally after 23 iterations ## ## Estimator ULS ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 200 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Unstructured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## eta =~ ## Y1 1.000 1.871 0.837 ## Y2 0.571 0.023 24.496 0.000 1.069 0.617 ## Y3 1.000 0.050 19.950 0.000 1.871 0.764 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .Y1 1.500 0.202 7.423 0.000 1.500 0.300 ## .Y2 1.857 0.094 19.749 0.000 1.857 0.619 ## .Y3 2.500 0.202 12.372 0.000 2.500 0.417 ## eta 3.500 0.189 18.497 0.000 1.000 1.000 Sigma: fitted(one_fac_fit2)$cov ## Y1 Y2 Y3 ## Y1 5.0 ## Y2 2.0 3.0 ## Y3 3.5 2.0 6.0 Sigma = fitted(one_fac_fit2)$cov diff = Sigma[colnames(S_3fac), colnames(S_3fac)] - S_3fac round(diff,3) ## Y1 Y2 Y3 ## Y1 0 0 0 ## Y2 0 0 0 ## Y3 0 0 0 all zeros. Meaning that Sigma = S. 11.2 PART II: ULS on the Positive Affect Example Let’s read this dataset in: cfaData&lt;- read.csv(&quot;cfaInclassData.csv&quot;, header = T) Fit a two-factor CFA model: fixedIndTwoFacSyntax &lt;- &quot; #Factor Specification posAffect =~ glad + happy + cheerful satisfaction =~ satisfied + content + comfortable &quot; two_fac_fit_uls &lt;- lavaan::sem(fixedIndTwoFacSyntax, data = cfaData, fixed.x = FALSE, estimator = &quot;ULS&quot;) summary(two_fac_fit_uls, standardized = T) ## lavaan 0.6-12 ended normally after 33 iterations ## ## Estimator ULS ## Optimization method NLMINB ## Number of model parameters 13 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 0.837 ## Degrees of freedom 8 ## P-value (Unknown) NA ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Unstructured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.696 0.708 ## happy 1.067 0.066 16.205 0.000 0.743 0.760 ## cheerful 1.105 0.068 16.157 0.000 0.769 0.781 ## satisfaction =~ ## satisfied 1.000 0.775 0.768 ## content 1.073 0.064 16.845 0.000 0.832 0.768 ## comfortable 0.906 0.053 17.124 0.000 0.702 0.738 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect ~~ ## satisfaction 0.264 0.017 15.795 0.000 0.489 0.489 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.482 0.050 9.597 0.000 0.482 0.499 ## .happy 0.402 0.054 7.488 0.000 0.402 0.422 ## .cheerful 0.377 0.056 6.762 0.000 0.377 0.390 ## .satisfied 0.417 0.055 7.615 0.000 0.417 0.410 ## .content 0.482 0.060 8.106 0.000 0.482 0.411 ## .comfortable 0.411 0.049 8.322 0.000 0.411 0.455 ## posAffect 0.484 0.039 12.395 0.000 1.000 1.000 ## satisfaction 0.600 0.045 13.455 0.000 1.000 1.000 Sigma: S = cov(cfaData[,-1]) Sigma = fitted(two_fac_fit_uls)$cov[colnames(S), colnames(S)] diff = Sigma - S round(diff,3) ## glad cheerful happy satisfied content comfortable ## glad 0.000 -0.002 0.004 -0.014 0.002 0.009 ## cheerful -0.002 0.000 -0.002 0.009 -0.004 0.003 ## happy 0.004 -0.002 0.000 -0.003 -0.011 0.011 ## satisfied -0.014 0.009 -0.003 0.000 0.009 -0.007 ## content 0.002 -0.004 -0.011 0.009 0.000 -0.003 ## comfortable 0.009 0.003 0.011 -0.007 -0.003 0.000 print(paste0(&quot;The difference between S and Sigma ranged between &quot;, round(min(diff),4), &quot; and &quot;, round(max(diff),4), &quot;.&quot;)) ## [1] &quot;The difference between S and Sigma ranged between -0.0143 and 0.0113.&quot; Sigma is not the same as S, but close. The sum of squared differences is: sum(diff[lower.tri(diff,diag = T)]^2) ## [1] 0.0008375874 11.3 PART III: Calculate ULS test statistic manually ULS test statistic is calculated as: T_uls = (1000-1)*sum(diff[lower.tri(diff,diag = T)]^2) T_uls ## [1] 0.8367498 One can also obtain vectors of S and Sigma first: s = lav_matrix_vech(S) sigma = lav_matrix_vech(Sigma) and calculate the ULS test statistic: T_uls = (1000-1)*sum((s - sigma)^2) T_uls ## [1] 0.8367498 No p-value for ULS test statistic as there is no known distribution for this test statistic i.e., no suitable method for model fit evaluation 11.4 PART IV: ML vs ULS vs WLS 11.4.1 ML Estimation two_fac_fit_ml &lt;- lavaan::sem(fixedIndTwoFacSyntax, data = cfaData, fixed.x = FALSE, estimator = &quot;ML&quot;) 11.4.2 WLS Estimation two_fac_fit_wls &lt;- lavaan::sem(fixedIndTwoFacSyntax, data = cfaData, fixed.x = FALSE, estimator = &quot;WLS&quot;) 11.4.3 Compare the parameter estimates coefTable = parameterEstimates(two_fac_fit_ml)[,1:3] coefTable = cbind(coefTable, ML = parameterEstimates(two_fac_fit_ml)$est, ULS = parameterEstimates(two_fac_fit_uls)$est, WLS = parameterEstimates(two_fac_fit_wls)$est) coefTable ## lhs op rhs ML ULS WLS ## 1 posAffect =~ glad 1.0000000 1.0000000 1.0000000 ## 2 posAffect =~ happy 1.0665969 1.0674339 1.0645600 ## 3 posAffect =~ cheerful 1.1121981 1.1045607 1.1150756 ## 4 satisfaction =~ satisfied 1.0000000 1.0000000 1.0000000 ## 5 satisfaction =~ content 1.0683189 1.0732316 1.0719435 ## 6 satisfaction =~ comfortable 0.9175967 0.9059151 0.9151164 ## 7 glad ~~ glad 0.4838829 0.4823845 0.4835721 ## 8 happy ~~ happy 0.4048385 0.4021235 0.4052179 ## 9 cheerful ~~ cheerful 0.3711009 0.3772155 0.3643077 ## 10 satisfied ~~ satisfied 0.4185958 0.4165368 0.4182162 ## 11 content ~~ content 0.4909589 0.4823048 0.4902739 ## 12 comfortable ~~ comfortable 0.4003673 0.4114721 0.4034071 ## 13 posAffect ~~ posAffect 0.4816239 0.4840887 0.4812181 ## 14 satisfaction ~~ satisfaction 0.5973737 0.6004497 0.5964489 ## 15 posAffect ~~ satisfaction 0.2617756 0.2638580 0.2619159 11.4.4 Compare the standard errors: seTable = parameterEstimates(two_fac_fit_ml)[,1:3] seTable = cbind(seTable, ML = parameterEstimates(two_fac_fit_ml)$se, ULS = parameterEstimates(two_fac_fit_uls)$se, WLS = parameterEstimates(two_fac_fit_wls)$se) seTable ## lhs op rhs ML ULS WLS ## 1 posAffect =~ glad 0.00000000 0.00000000 0.00000000 ## 2 posAffect =~ happy 0.05528238 0.06587199 0.05328971 ## 3 posAffect =~ cheerful 0.05715937 0.06836322 0.05484498 ## 4 satisfaction =~ satisfied 0.00000000 0.00000000 0.00000000 ## 5 satisfaction =~ content 0.05204954 0.06371370 0.05010033 ## 6 satisfaction =~ comfortable 0.04512163 0.05290389 0.04329804 ## 7 glad ~~ glad 0.02906697 0.05026166 0.02803266 ## 8 happy ~~ happy 0.02813623 0.05369964 0.02823333 ## 9 cheerful ~~ cheerful 0.02853712 0.05578216 0.02731647 ## 10 satisfied ~~ satisfied 0.02921942 0.05470277 0.02802346 ## 11 content ~~ content 0.03376166 0.05950183 0.03304768 ## 12 comfortable ~~ comfortable 0.02614297 0.04944382 0.02655468 ## 13 posAffect ~~ posAffect 0.04210374 0.03905424 0.04141331 ## 14 satisfaction ~~ satisfaction 0.04708792 0.04462502 0.04883263 ## 15 posAffect ~~ satisfaction 0.02545399 0.01670551 0.02718848 11.5 PART V: Improper Solutions Going back to the 1-factor toy example… Suppose we have a new covariance matrix now and a super large sample size (n = 200000000): n = 2000 S_3fac_new = matrix(c(5, 1, 3.5, 1, 3, 2, 3.5, 2, 6), 3, 3, dimnames = list(c(&#39;Y1&#39;, &#39;Y2&#39;, &#39;Y3&#39;), c(&#39;Y1&#39;, &#39;Y2&#39;, &#39;Y3&#39;))) S_3fac_new ## Y1 Y2 Y3 ## Y1 5.0 1 3.5 ## Y2 1.0 3 2.0 ## Y3 3.5 2 6.0 The one-factor syntax is the same: one_fac_syntax &lt;- &quot; eta =~ Y1 + Y2 + Y3 &quot; ML Estimation: one_fac_fit_new &lt;- lavaan::sem(one_fac_syntax, sample.cov = S_3fac_new, sample.nobs = n, estimator = &quot;ML&quot;, fixed.x = FALSE) ## Warning in lav_object_post_check(object): lavaan WARNING: some estimated ov variances are negative lavaan WARNING: some estimated ov variances are negative negative residual variances changing the estimation from ML to ULS doesn’t help summary(one_fac_fit_new, standardized = T) ## lavaan 0.6-12 ended normally after 28 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 2e+08 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## eta =~ ## Y1 1.000 1.323 0.592 ## Y2 0.571 0.000 6181.151 0.000 0.756 0.436 ## Y3 2.000 0.000 4714.045 0.000 2.646 1.080 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .Y1 3.250 0.000 7039.895 0.000 3.250 0.650 ## .Y2 2.429 0.000 9150.327 0.000 2.429 0.810 ## .Y3 -1.000 0.001 -760.286 0.000 -1.000 -0.167 ## eta 1.750 0.001 3486.948 0.000 1.000 1.000 Label and constraint sig3 to be larger than 0: one_fac_syntax_const &lt;- &quot; eta =~ Y1 + Y2 + Y3 Y3~~sig3*Y3 # constraints sig3 &gt; 0 &quot; ML Estimation: one_fac_fit_new2 &lt;- lavaan::sem(one_fac_syntax_const, sample.cov = S_3fac_new, sample.nobs = n, fixed.x = FALSE) ## Warning in lav_object_post_check(object): lavaan WARNING: some estimated ov variances are negative summary(one_fac_fit_new2, standardized = T) ## lavaan 0.6-12 ended normally after 63 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## Number of inequality constraints 1 ## ## Number of observations 2e+08 ## ## Model Test User Model: ## ## Test statistic 806452.705 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## eta =~ ## Y1 1.000 1.429 0.639 ## Y2 0.571 0.000 6357.073 0.000 0.816 0.471 ## Y3 1.714 0.000 11748.540 0.000 2.449 1.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .Y3 (sig3) -0.000 -0.000 -0.000 ## .Y1 2.958 0.000 10000.000 0.000 2.958 0.592 ## .Y2 2.333 0.000 10000.000 0.000 2.333 0.778 ## eta 2.042 0.000 5065.023 0.000 1.000 1.000 ## ## Constraints: ## |Slack| ## sig3 - 0 0.000 fitted(one_fac_fit_new)$cov # perfect ## Y1 Y2 Y3 ## Y1 5.0 ## Y2 1.0 3.0 ## Y3 3.5 2.0 6.0 fitted(one_fac_fit_new2)$cov # compromised, at the cost of model fit ## Y1 Y2 Y3 ## Y1 5.000 ## Y2 1.167 3.000 ## Y3 3.500 2.000 6.000 S_3fac_new ## Y1 Y2 Y3 ## Y1 5.0 1 3.5 ## Y2 1.0 3 2.0 ## Y3 3.5 2 6.0 "],["lavaan-lab-9-model-fit-part-i-test-statistics.html", "Chapter 12 Lavaan Lab 9: Model Fit Part I (Test Statistics) 12.1 PART I: Robust ML on the Positive Affect Example 12.2 PART II: Nested Model Comparison 12.3 PART III: Exercises: More Nested Models", " Chapter 12 Lavaan Lab 9: Model Fit Part I (Test Statistics) In this lab, we will learn: how to calculate and interpret chi-square statistics for SEM models. how to compare nested models using chi-square difference test. Load up the lavaan and semPlot libraries: library(lavaan) library(semPlot) 12.1 PART I: Robust ML on the Positive Affect Example Let’s read this dataset in: cfaData&lt;- read.csv(&quot;cfaInclassData.csv&quot;, header = T) Write out syntax for a two-factor CFA model: fixedIndTwoFacSyntax &lt;- &quot; #Factor Specification posAffect =~ glad + happy + cheerful satisfaction =~ satisfied + content + comfortable &quot; Fit the model regularly: fixedIndTwoFacRun = lavaan::sem(model = fixedIndTwoFacSyntax, data = cfaData, fixed.x=FALSE) fixedIndTwoFacRun ## lavaan 0.6-12 ended normally after 24 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 13 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 2.957 ## Degrees of freedom 8 ## P-value (Chi-square) 0.937 Model chi_sq: misfit defined through the likelihood ratio. T_ML = 2.957 df_ML = 8 pvalue_ML = 0.937 Since pvalue_ML &gt; 0.05, this model does not have significant model misfit. 12.1.1 Mean corrected statistic (T_M) Satorra and Bentler (1994, 2001) proposed two robust corrections for non-normally distributed data: two_fac_fit_M &lt;- lavaan::sem(fixedIndTwoFacSyntax, data = cfaData, fixed.x=FALSE, estimator = &quot;MLM&quot;) two_fac_fit_M ## lavaan 0.6-12 ended normally after 24 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 13 ## ## Number of observations 1000 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 2.957 2.891 ## Degrees of freedom 8 8 ## P-value (Chi-square) 0.937 0.941 ## Scaling correction factor 1.023 ## Satorra-Bentler correction T_M = 2.891 df_M = 8 pvalue_M = 0.941 12.1.2 Mean and variance adjusted statistic (T_MV) estimator = “MLMVS” returns the Mean- and variance adjusted statistic with an updated degrees of freedom (recommended) estimator = “MLMV” returns another version of Mean- and variance adjusted statistic but does not change the degrees of freedom two_fac_fit_MV &lt;- lavaan::sem(fixedIndTwoFacSyntax, data = cfaData, fixed.x=FALSE, estimator = &quot;MLMVS&quot;) #summary(two_fac_fit_MV, standardized = T) two_fac_fit_MV ## lavaan 0.6-12 ended normally after 24 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 13 ## ## Number of observations 1000 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 2.957 2.842 ## Degrees of freedom 8 7.864 ## P-value (Chi-square) 0.937 0.939 ## Scaling correction factor 1.041 ## mean and variance adjusted correction T_MV = 2.842 df_MV = 7.864 pvalue_MV = 0.939 Please see a complete list of estimators here: http://lavaan.ugent.be/tutorial/est.html 12.1.3 Yuan-Bentler test statistic (T_MLR) Just like T_M and T_MV, T_MLR also corrects for nonnormality. Since MLR works for both complete and incomplete data, T_MLR is more popular in practice: two_fac_fit_MLR &lt;- lavaan::sem(fixedIndTwoFacSyntax, data = cfaData, fixed.x=FALSE, estimator = &quot;MLR&quot;) two_fac_fit_MLR ## lavaan 0.6-12 ended normally after 24 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 13 ## ## Number of observations 1000 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 2.957 2.897 ## Degrees of freedom 8 8 ## P-value (Chi-square) 0.937 0.941 ## Scaling correction factor 1.021 ## Yuan-Bentler correction (Mplus variant) T_MLR = 2.897 df_MLR = 8 pvalue_MLR = 0.941 12.1.4 Small sample correction - F test F_ratio = 2.957/8 referred to an F(df, N-1) distribution: ?pf p_val_F = 1-pf(F_ratio, 8, 1000-1) p_val_F ## [1] 0.936751 which is very similar to the three p-values given above given a large sample size in this study N = 1000. 12.2 PART II: Nested Model Comparison We can compare the fit of the two-factor model to that of a one-factor model because the one-factor model is nested in the two-factor model. 12.2.1 One-factor model OneFacSyntax &lt;- &quot; #Factor Specification eta1 =~ glad + happy + cheerful + satisfied + content + comfortable &quot; one_fac_fit = lavaan::sem(model = OneFacSyntax, data = cfaData, fixed.x=FALSE) request standardized = T to check standardized loadings - item reliability summary(one_fac_fit, standardized = T) ## lavaan 0.6-12 ended normally after 31 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 12 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 592.661 ## Degrees of freedom 9 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## eta1 =~ ## glad 1.000 0.505 0.514 ## happy 1.048 0.086 12.192 0.000 0.529 0.542 ## cheerful 1.070 0.087 12.299 0.000 0.540 0.550 ## satisfied 1.422 0.101 14.117 0.000 0.718 0.712 ## content 1.526 0.108 14.108 0.000 0.770 0.711 ## comfortable 1.296 0.093 13.903 0.000 0.654 0.688 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.711 0.035 20.410 0.000 0.711 0.736 ## .happy 0.673 0.033 20.096 0.000 0.673 0.706 ## .cheerful 0.675 0.034 20.001 0.000 0.675 0.698 ## .satisfied 0.500 0.030 16.648 0.000 0.500 0.493 ## .content 0.579 0.035 16.684 0.000 0.579 0.494 ## .comfortable 0.475 0.027 17.380 0.000 0.475 0.526 ## eta1 0.255 0.033 7.841 0.000 1.000 1.000 Only the last three standardized loadings are larger than 0.6 The first three indicators are not reliable indicators of the new latent variable eta1 Model Test User Model: Test statistic 592.661 Degrees of freedom 9 P-value (Chi-square) 0.000 The chi-square statistic is very large and significant for this one-factor model…poor fit 12.2.2 Plotting semPaths(fixedIndTwoFacRun, what = &quot;std&quot;, fade= F) semPaths(one_fac_fit, what = &quot;std&quot;, fade= F) 12.2.3 Comparing Nested Models The one-factor model is nested in the two-factor model. The fit of the one-factor model is worse than the two-factor model, but is it significantly worse? Here we use anova() function to perform chi-square difference test Note that the order of the models in anova() doesn’t matter anova(one_fac_fit, fixedIndTwoFacRun) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## fixedIndTwoFacRun 8 14992 15056 2.9575 ## one_fac_fit 9 15580 15639 592.6611 589.7 1 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Chi-Squared Difference Test Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) fixedIndTwoFacRun 8 14992 15056 2.9575 one_fac_fit 9 15580 15639 592.6611 589.7 1 &lt; 2.2e-16 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 The model on top is the base model and the model at the bottom is the restricted model. The restricted model always fits worse than the base model. Chisq diff = 589.7; Df diff = 1; p-value &lt; 0.001 Chisq diff is sig: one-factor fits significantly worse than the two-factor model and we should endorse two-factor model 12.3 PART III: Exercises: More Nested Models Your turn now, Have fun! 12.3.1 Exercises: Compare the base model (fixedIndTwoFacRun) to (Model 2) 2-factor CFA model with orthogonal latent variables (Model 3) 2-factor CFA model with a cross-loading from posAffect to satisfied (Model 4) 2-factor CFA model with a correlation between unique factors u1 and u4 12.3.2 Model 2: Orthogonal Factors OrthFacSyntax &lt;- &quot; #Factor Specification posAffect =~ glad + happy + cheerful satisfaction =~ satisfied + content + comfortable #Orthogonal Factors: no covariance posAffect ~~ 0*satisfaction &quot; Fit Model 2: OrthFac_fit &lt;- lavaan::sem(model = OrthFacSyntax, data = cfaData, fixed.x = F) Plot Model 2: semPaths(OrthFac_fit, what = &quot;std&quot;, fade= F) chi-square difference test: anova(fixedIndTwoFacRun, OrthFac_fit) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## fixedIndTwoFacRun 8 14992 15056 2.9575 ## OrthFac_fit 9 15156 15215 168.4731 165.52 1 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Chisq diff = 165.52; Df diff = 1; p-value &lt; 0.001 Chisq diff is sig: the two-factor model with orthogonal latent variables fits significantly worse than the model with correlated latent variables. We should endorse the base two-factor model with correlated latent variables. 12.3.3 Model 3: Cross loading CrossLoadingSyntax &lt;- &quot; #Factor Specification # cross loading: satisfied load on both latent variables # try to avoid using satisfied as the marker variable posAffect =~ glad + satisfied + happy + cheerful satisfaction =~ satisfied + content + comfortable &quot; Fit Model 3: CrossLoading_fit &lt;- lavaan::sem(model = CrossLoadingSyntax, data = cfaData, fixed.x = F) Plot Model 3: semPaths(CrossLoading_fit, what = &quot;std&quot;, fade= F) chi-square difference test: anova(fixedIndTwoFacRun, CrossLoading_fit) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## CrossLoading_fit 7 14994 15063 2.9050 ## fixedIndTwoFacRun 8 14992 15056 2.9575 0.052477 1 0.8188 Chisq diff = 0.052477; Df diff = 1; p-value = 0.8188 Chisq diff is not sig: the two-factor model without the cross-loading is NOT significantly worse than the model with the cross-loading. The cross-loading is not necessary. We should endorse the base two-factor model without the cross-loading. 12.3.4 Model 4: Correlated Unique Factors CorrUniSyntax &lt;-&quot; #Factor Specification posAffect =~ glad + happy + cheerful satisfaction =~ satisfied + content + comfortable # correlated error glad ~~ satisfied &quot; Fit Model 4: CorrUni_fit &lt;- lavaan::sem(model= CorrUniSyntax, data = cfaData, fixed.x = F) Plot Model 4: semPaths(CorrUni_fit, what = &quot;std&quot;, fade= F) chi-square difference test: anova(fixedIndTwoFacRun, CorrUni_fit) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## CorrUni_fit 7 14993 15062 1.6343 ## fixedIndTwoFacRun 8 14992 15056 2.9575 1.3232 1 0.25 Chisq diff = 1.3232; Df diff = 1; p-value = 0.25 Chisq diff is not sig: the two-factor model without the correlated unique factors is NOT significantly worse than the model with the correlated unique factors. The correlation between u1 and u4 is not necessary. We should endorse the base two-factor model without the correlated unique factors. "],["lavaan-lab-10-model-fit-part-ii-fit-indices.html", "Chapter 13 Lavaan Lab 10: Model Fit Part II (Fit Indices) 13.1 PART I: Fit Indices 13.2 PART II: Exercise", " Chapter 13 Lavaan Lab 10: Model Fit Part II (Fit Indices) In this lab, we will learn: how to calculate and interpret global fit indices for SEM models. how to compare non-nested models using AIC and BIC. Load up the lavaan library: library(lavaan) 13.1 PART I: Fit Indices Let’s read this dataset in: cfaData&lt;- read.csv(&quot;cfaInclassData.csv&quot;, header = T) Write out syntax for a two-factor CFA model: fixedIndTwoFacSyntax &lt;- &quot; #Factor Specification posAffect =~ glad + happy + cheerful satisfaction =~ satisfied + content + comfortable &quot; Fit the model regularly: fixedIndTwoFacRun = lavaan::sem(model = fixedIndTwoFacSyntax, data = cfaData, fixed.x=FALSE, estimator = &#39;MLMV&#39;) Request fit indices by adding fit.measures = T in the summary() function: summary(fixedIndTwoFacRun, standardized = T, fit.measures = T) ## lavaan 0.6-12 ended normally after 24 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 13 ## ## Number of observations 1000 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 2.957 2.935 ## Degrees of freedom 8 8 ## P-value (Chi-square) 0.937 0.938 ## Scaling correction factor 1.032 ## Shift parameter 0.068 ## simple second-order correction ## ## Model Test Baseline Model: ## ## Test statistic 2020.010 849.820 ## Degrees of freedom 15 15 ## P-value 0.000 0.000 ## Scaling correction factor 2.397 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 1.000 ## Tucker-Lewis Index (TLI) 1.005 1.011 ## ## Robust Comparative Fit Index (CFI) NA ## Robust Tucker-Lewis Index (TLI) NA ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -7483.272 -7483.272 ## Loglikelihood unrestricted model (H1) -7481.793 -7481.793 ## ## Akaike (AIC) 14992.544 14992.544 ## Bayesian (BIC) 15056.345 15056.345 ## Sample-size adjusted Bayesian (BIC) 15015.056 15015.056 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 0.000 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.009 0.008 ## P-value RMSEA &lt;= 0.05 1.000 1.000 ## ## Robust RMSEA NA ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.007 0.007 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.694 0.706 ## happy 1.067 0.054 19.782 0.000 0.740 0.758 ## cheerful 1.112 0.055 20.147 0.000 0.772 0.785 ## satisfaction =~ ## satisfied 1.000 0.773 0.767 ## content 1.068 0.050 21.399 0.000 0.826 0.762 ## comfortable 0.918 0.044 21.056 0.000 0.709 0.746 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect ~~ ## satisfaction 0.262 0.027 9.539 0.000 0.488 0.488 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.484 0.028 17.146 0.000 0.484 0.501 ## .happy 0.405 0.028 14.260 0.000 0.405 0.425 ## .cheerful 0.371 0.028 13.371 0.000 0.371 0.384 ## .satisfied 0.419 0.028 14.828 0.000 0.419 0.412 ## .content 0.491 0.033 14.729 0.000 0.491 0.419 ## .comfortable 0.400 0.027 14.876 0.000 0.400 0.443 ## posAffect 0.482 0.042 11.544 0.000 1.000 1.000 ## satisfaction 0.597 0.049 12.189 0.000 1.000 1.000 13.1.1 RMSEA Root Mean Square Error of Approximation: RMSEA 0.000 90 Percent confidence interval - lower 0.000 90 Percent confidence interval - upper 0.009 P-value RMSEA &lt;= 0.05 1.000 reproducing RMSEA: T = 2.957 df = 8 N = 1000 RMSEA = sqrt(max(T-df,0)/(N-1)/df) = sqrt(max(2.957-8,0)/(1000-1)/8) = 0 13.1.2 SRMR Standardized Root Mean Square Residual: SRMR 0.007 reproducing SRMR: S = cov(cfaData[,-1]) colnames = colnames(S) SIGMA = fitted(fixedIndTwoFacRun)$cov[colnames, colnames] p = ncol(S) # use cov2cor() function to convert diff to a correlation matrix and standardize the residuals: resd = cov2cor(S) - cov2cor(SIGMA) # keep only the nonduplicated elements: resd2 = lav_matrix_vech(resd) sqrt(sum(resd2^2)/(p*(p+1)/2)) ## [1] 0.006847801 A small average standardized residual…looks good 13.1.3 Null Model MO Model Test Baseline Model: Test statistic 2020.010 Degrees of freedom 15 P-value 0.000 This is the chi_sq for the baseline model used in the CFI/TLI/comparative fit measures. We know what this means now! chisquare of the null model: 2020.010 df of the null model: 15 reproducing M0: baselineM0 &lt;- &quot; glad ~~ glad happy ~~ happy cheerful ~~ cheerful satisfied ~~ satisfied content ~~ content comfortable ~~ comfortable &quot; base_fit &lt;- lavaan::sem(baselineM0, data = cfaData, fixed.x = FALSE) base_fit ## lavaan 0.6-12 ended normally after 12 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 1000 ## ## Model Test User Model: ## ## Test statistic 2020.010 ## Degrees of freedom 15 ## P-value (Chi-square) 0.000 13.1.4 CFI/TLI User Model versus Baseline Model: Comparative Fit Index (CFI) 1.000 Tucker-Lewis Index (TLI) 1.005 100% improvement over the null(baseline) model … great fit Here TLI is larger than 1 because this is a rare situation with chisquare=2.957&lt;df=8 numerator of TLI = (2020.010/15-2.957/8) = 134.2977 denominator of TLI = (2020.010/15-1) = 133.6673 TLI = 134.2977 / 133.6673 = 1.005 A TLI that is larger than 1 is no different from TLI = 1 great overall fit. 13.1.5 Loglikelihood Loglikelihood and Information Criteria: Loglikelihood user model (H0) -7483.272 Loglikelihood unrestricted model (H1) -7481.793 H0 is the loglikelihood of your model (user model … lavaan is kind to clarify this) H1 is the saturated model loglikelihood. 13.1.6 AIC/BIC Akaike (AIC) 14992.544 Bayesian (BIC) 15056.345 Sample-size adjusted Bayesian (BIC) 15015.056 Penalized -2 LogL If these are lower than some other model -&gt; prefer this model. If these are higher than some other model -&gt; prefer the other model. reproducing AIC/BIC: logLik = -7483.272 q = 13 # (4 loadings + 6 unique factor variances + 3 factor var/covs) N = 1000 (AIC = -2*logLik + 2*q) ## [1] 14992.54 (BIC = -2*logLik + log(N)*q) ## [1] 15056.34 13.2 PART II: Exercise For this portion, we will run the CFA analyses on a new simulated dataset based on Todd Little’s positive affect example. Read in the new dataset: affectData_new &lt;- read.csv(&quot;ChiStatSimDat.csv&quot;, header = T) Examine the dataset: head(affectData_new) ## glad cheerful happy satisfied content comfortable ## 1 -0.88164396 0.08934146 -0.02412456 -0.570588909 0.3525594 0.59309666 ## 2 -0.05250507 0.68355268 0.74157736 0.146592063 0.5393570 1.12686970 ## 3 1.87921754 0.84984042 2.87784843 0.279073394 1.1723201 0.26855073 ## 4 -0.22371928 0.10069522 -0.19760745 -0.063127288 -1.1782493 -0.59893739 ## 5 0.17341853 0.26949455 -0.49326121 0.237083760 -1.0066170 -0.04426791 ## 6 -1.03348018 0.04250862 -1.00909832 0.009657695 0.4366756 0.05996283 Examine the covariance matrix: cov(affectData_new) ## glad cheerful happy satisfied content comfortable ## glad 1.4049090 0.9262102 1.3859992 1.1088378 1.200695 0.8715664 ## cheerful 0.9262102 1.1043581 1.0344850 0.8850550 1.053458 0.9513925 ## happy 1.3859992 1.0344850 1.7737439 1.2253205 1.233110 0.9317963 ## satisfied 1.1088378 0.8850550 1.2253205 1.6365917 1.496492 0.8626294 ## content 1.2006950 1.0534583 1.2331099 1.4964919 1.872718 1.0693608 ## comfortable 0.8715664 0.9513925 0.9317963 0.8626294 1.069361 1.3162770 all positive! (Remember that indicators need to be all positively correlated for CFA models?) 13.2.1 PART I: Plot the distributions of all indicators library(PerformanceAnalytics) chart.Correlation(affectData_new) ## Warning in par(usr): argument 1 does not name a graphical parameter ## Warning in par(usr): argument 1 does not name a graphical parameter ## Warning in par(usr): argument 1 does not name a graphical parameter ## Warning in par(usr): argument 1 does not name a graphical parameter ## Warning in par(usr): argument 1 does not name a graphical parameter ## Warning in par(usr): argument 1 does not name a graphical parameter ## Warning in par(usr): argument 1 does not name a graphical parameter ## Warning in par(usr): argument 1 does not name a graphical parameter ## Warning in par(usr): argument 1 does not name a graphical parameter ## Warning in par(usr): argument 1 does not name a graphical parameter ## Warning in par(usr): argument 1 does not name a graphical parameter ## Warning in par(usr): argument 1 does not name a graphical parameter ## Warning in par(usr): argument 1 does not name a graphical parameter ## Warning in par(usr): argument 1 does not name a graphical parameter ## Warning in par(usr): argument 1 does not name a graphical parameter all indicators look roughly normal 13.2.2 PART II: Write out the model syntax for two-factor model twofa.model &lt;- &quot; #Factor Specification posAffect =~ glad + happy + cheerful satisfaction =~ satisfied + content + comfortable &quot; 13.2.3 PART III: Fit the two-factor model new_fit = lavaan::sem(twofa.model, data = affectData_new, fixed.x=FALSE) summary(new_fit, standardized = T, fit.measures = T) ## lavaan 0.6-12 ended normally after 29 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 13 ## ## Number of observations 200 ## ## Model Test User Model: ## ## Test statistic 113.638 ## Degrees of freedom 8 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 1168.055 ## Degrees of freedom 15 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.908 ## Tucker-Lewis Index (TLI) 0.828 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1413.224 ## Loglikelihood unrestricted model (H1) -1356.405 ## ## Akaike (AIC) 2852.449 ## Bayesian (BIC) 2895.327 ## Sample-size adjusted Bayesian (BIC) 2854.141 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.257 ## 90 Percent confidence interval - lower 0.216 ## 90 Percent confidence interval - upper 0.300 ## P-value RMSEA &lt;= 0.05 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.070 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 1.115 0.943 ## happy 1.095 0.048 22.882 0.000 1.221 0.919 ## cheerful 0.763 0.046 16.739 0.000 0.851 0.812 ## satisfaction =~ ## satisfied 1.000 1.152 0.902 ## content 1.110 0.054 20.462 0.000 1.278 0.936 ## comfortable 0.715 0.057 12.548 0.000 0.823 0.719 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect ~~ ## satisfaction 1.104 0.131 8.425 0.000 0.859 0.859 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.154 0.031 4.966 0.000 0.154 0.110 ## .happy 0.274 0.043 6.398 0.000 0.274 0.155 ## .cheerful 0.374 0.042 8.851 0.000 0.374 0.341 ## .satisfied 0.302 0.047 6.492 0.000 0.302 0.186 ## .content 0.230 0.048 4.742 0.000 0.230 0.123 ## .comfortable 0.632 0.068 9.253 0.000 0.632 0.482 ## posAffect 1.244 0.142 8.790 0.000 1.000 1.000 ## satisfaction 1.326 0.164 8.093 0.000 1.000 1.000 13.2.4 PART IV: Interpret the chisquare statistic and fit indices Model Test User Model: Test statistic 113.638 Degrees of freedom 8 P-value (Chi-square) 0.000 113.638 is much larger than df=8 and the p-value is 0.000&lt;0.05, This chisquare is too large and the model is a poor fit. Root Mean Square Error of Approximation: RMSEA 0.257 90 Percent confidence interval - lower 0.216 90 Percent confidence interval - upper 0.300 P-value RMSEA &lt;= 0.05 0.000 Standardized Root Mean Square Residual: SRMR 0.070 User Model versus Baseline Model: Comparative Fit Index (CFI) 0.908 Tucker-Lewis Index (TLI) 0.828 RMSEA = 0.257 &gt;&gt; 0.1. The lower bound of the confidence interval is also larger than 0.1. This indicates a poor fit. P-value RMSEA &lt;= 0.05: sig - close fit null hypothesis rejected. SRMR = 0.07 &lt; 0.08, meaning that the average standardized residual between S and Sigma is no larger than 0.08, but SRMR is known to be lenient (i.e., low SRMR =/= good models, but high SRMR = bad models). 90.8% improvement over the null model … marginal fit "],["lavaan-lab-11-model-local-fitting-and-model-modifications.html", "Chapter 14 Lavaan Lab 11: Model Local Fitting and Model Modifications 14.1 PART I: Local Fit with Residuals 14.2 PART II: Modification Indices", " Chapter 14 Lavaan Lab 11: Model Local Fitting and Model Modifications In this lab, we will learn: how to examine SEM local fit using residuals how to modify SEM models for improved fit using modification indices Load up the lavaan library: library(lavaan) 14.1 PART I: Local Fit with Residuals Let’s read in the new dataset ChiStatSimDat.csv: cfaData&lt;- read.csv(&quot;ChiStatSimDat.csv&quot;, header = T) Write out syntax for a two-factor CFA model: fixedIndTwoFacSyntax &lt;- &quot; #Factor Specification posAffect =~ glad + happy + cheerful satisfaction =~ satisfied + content + comfortable &quot; Fit the two-factor model: fixedIndTwoFacRun = lavaan::sem(model = fixedIndTwoFacSyntax, data = cfaData, fixed.x=FALSE) # , estimator = &#39;MLMV&#39; summary(fixedIndTwoFacRun, standardized = T, fit.measures = T) ## lavaan 0.6-12 ended normally after 29 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 13 ## ## Number of observations 200 ## ## Model Test User Model: ## ## Test statistic 113.638 ## Degrees of freedom 8 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 1168.055 ## Degrees of freedom 15 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.908 ## Tucker-Lewis Index (TLI) 0.828 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1413.224 ## Loglikelihood unrestricted model (H1) -1356.405 ## ## Akaike (AIC) 2852.449 ## Bayesian (BIC) 2895.327 ## Sample-size adjusted Bayesian (BIC) 2854.141 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.257 ## 90 Percent confidence interval - lower 0.216 ## 90 Percent confidence interval - upper 0.300 ## P-value RMSEA &lt;= 0.05 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.070 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 1.115 0.943 ## happy 1.095 0.048 22.882 0.000 1.221 0.919 ## cheerful 0.763 0.046 16.739 0.000 0.851 0.812 ## satisfaction =~ ## satisfied 1.000 1.152 0.902 ## content 1.110 0.054 20.462 0.000 1.278 0.936 ## comfortable 0.715 0.057 12.548 0.000 0.823 0.719 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect ~~ ## satisfaction 1.104 0.131 8.425 0.000 0.859 0.859 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.154 0.031 4.966 0.000 0.154 0.110 ## .happy 0.274 0.043 6.398 0.000 0.274 0.155 ## .cheerful 0.374 0.042 8.851 0.000 0.374 0.341 ## .satisfied 0.302 0.047 6.492 0.000 0.302 0.186 ## .content 0.230 0.048 4.742 0.000 0.230 0.123 ## .comfortable 0.632 0.068 9.253 0.000 0.632 0.482 ## posAffect 1.244 0.142 8.790 0.000 1.000 1.000 ## satisfaction 1.326 0.164 8.093 0.000 1.000 1.000 14.1.1 Unstandardized residuals resid(fixedIndTwoFacRun)$cov ## glad happy cherfl satsfd contnt cmfrtb ## glad 0.000 ## happy 0.017 0.000 ## cheerful -0.028 -0.010 0.000 ## satisfied -0.001 0.011 0.038 0.000 ## content -0.030 -0.114 0.113 0.017 0.000 ## comfortable 0.078 0.063 0.344 -0.090 0.012 0.000 What does this mean? What is the metric? 14.1.2 Standardized residuals resid(fixedIndTwoFacRun, type = &quot;standardized&quot;)$cov ## glad happy cherfl satsfd contnt cmfrtb ## glad 0.000 ## happy 3.950 0.000 ## cheerful -3.732 -0.764 0.000 ## satisfied -0.035 0.431 1.189 0.000 ## content -2.164 -5.979 3.164 3.410 0.000 ## comfortable 2.079 1.383 7.821 -4.755 0.762 0.000 14.1.3 Normalized residuals resid(fixedIndTwoFacRun, type = &quot;normalized&quot;)$cov ## glad happy cherfl satsfd contnt cmfrtb ## glad 0.000 ## happy 0.117 0.000 ## cheerful -0.254 -0.081 0.000 ## satisfied -0.004 0.073 0.337 0.000 ## content -0.214 -0.738 0.903 0.105 0.000 ## comfortable 0.686 0.502 3.187 -0.751 0.087 0.000 Different residuals, same story The covariance residual between cheerful and comfortable is the largest and positive The model under-predicts this covariance Fix! 14.2 PART II: Modification Indices modindices(fixedIndTwoFacRun) Filter output and only show rows with a modification index value equal or higher than 1: modindices(fixedIndTwoFacRun, minimum.value = 10) ## lhs op rhs mi epc sepc.lv sepc.all sepc.nox ## 17 posAffect =~ content 17.031 -0.718 -0.801 -0.587 -0.587 ## 18 posAffect =~ comfortable 14.926 0.500 0.558 0.487 0.487 ## 20 satisfaction =~ happy 10.239 -0.390 -0.450 -0.338 -0.338 ## 21 satisfaction =~ cheerful 20.960 0.456 0.526 0.501 0.501 ## 22 glad ~~ happy 20.960 0.256 0.256 1.245 1.245 ## 23 glad ~~ cheerful 10.239 -0.106 -0.106 -0.443 -0.443 ## 29 happy ~~ content 18.588 -0.132 -0.132 -0.525 -0.525 ## 33 cheerful ~~ comfortable 45.215 0.253 0.253 0.521 0.521 ## 34 satisfied ~~ content 14.926 0.303 0.303 1.151 1.151 ## 35 satisfied ~~ comfortable 17.030 -0.181 -0.181 -0.414 -0.414 Sort the output using the values of the modification index values. Higher values appear first: modindices(fixedIndTwoFacRun, minimum.value = 10, sort = TRUE) ## lhs op rhs mi epc sepc.lv sepc.all sepc.nox ## 33 cheerful ~~ comfortable 45.215 0.253 0.253 0.521 0.521 ## 21 satisfaction =~ cheerful 20.960 0.456 0.526 0.501 0.501 ## 22 glad ~~ happy 20.960 0.256 0.256 1.245 1.245 ## 29 happy ~~ content 18.588 -0.132 -0.132 -0.525 -0.525 ## 17 posAffect =~ content 17.031 -0.718 -0.801 -0.587 -0.587 ## 35 satisfied ~~ comfortable 17.030 -0.181 -0.181 -0.414 -0.414 ## 18 posAffect =~ comfortable 14.926 0.500 0.558 0.487 0.487 ## 34 satisfied ~~ content 14.926 0.303 0.303 1.151 1.151 ## 20 satisfaction =~ happy 10.239 -0.390 -0.450 -0.338 -0.338 ## 23 glad ~~ cheerful 10.239 -0.106 -0.106 -0.443 -0.443 op ~~ : a correlation between two unique factors op =~ : cross-loading This indicates that the parameters lavaan detects for you to free up are all residual covariances. 14.2.1 Modified Model 1: mod1 &lt;- &quot; posAffect =~ glad + happy + cheerful satisfaction =~ satisfied + content + comfortable #residal covariance cheerful ~~ comfortable &quot; mod1_fit &lt;- lavaan::sem(mod1, data = cfaData, std.lv = TRUE, fixed.x=FALSE) summary(mod1_fit, fit.measures = T, standardized = T) ## lavaan 0.6-12 ended normally after 29 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 14 ## ## Number of observations 200 ## ## Model Test User Model: ## ## Test statistic 60.048 ## Degrees of freedom 7 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 1168.055 ## Degrees of freedom 15 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.954 ## Tucker-Lewis Index (TLI) 0.901 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1386.429 ## Loglikelihood unrestricted model (H1) -1356.405 ## ## Akaike (AIC) 2800.859 ## Bayesian (BIC) 2847.035 ## Sample-size adjusted Bayesian (BIC) 2802.682 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.195 ## 90 Percent confidence interval - lower 0.151 ## 90 Percent confidence interval - upper 0.241 ## P-value RMSEA &lt;= 0.05 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.058 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.111 0.064 17.475 0.000 1.111 0.940 ## happy 1.221 0.073 16.802 0.000 1.221 0.919 ## cheerful 0.801 0.060 13.392 0.000 0.801 0.790 ## satisfaction =~ ## satisfied 1.161 0.071 16.389 0.000 1.161 0.910 ## content 1.262 0.075 16.835 0.000 1.262 0.925 ## comfortable 0.750 0.069 10.821 0.000 0.750 0.679 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .cheerful ~~ ## .comfortable 0.270 0.043 6.230 0.000 0.270 0.536 ## posAffect ~~ ## satisfaction 0.876 0.022 39.298 0.000 0.876 0.876 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.163 0.031 5.200 0.000 0.163 0.116 ## .happy 0.274 0.043 6.406 0.000 0.274 0.155 ## .cheerful 0.386 0.043 8.990 0.000 0.386 0.375 ## .satisfied 0.281 0.046 6.106 0.000 0.281 0.172 ## .content 0.271 0.050 5.365 0.000 0.271 0.145 ## .comfortable 0.657 0.070 9.380 0.000 0.657 0.539 ## posAffect 1.000 1.000 1.000 ## satisfaction 1.000 1.000 1.000 Model comparison: anova(mod1_fit, fixedIndTwoFacRun) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## mod1_fit 7 2800.9 2847.0 60.048 ## fixedIndTwoFacRun 8 2852.4 2895.3 113.638 53.59 1 2.47e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Keep modifying mod1: modindices(mod1_fit, minimum.value = 10, sort = TRUE) ## lhs op rhs mi epc sepc.lv sepc.all sepc.nox ## 22 satisfaction =~ cheerful 27.353 0.612 0.612 0.604 0.604 ## 23 glad ~~ happy 27.353 0.275 0.275 1.304 1.304 ## 34 satisfied ~~ content 23.194 0.369 0.369 1.340 1.340 ## 19 posAffect =~ comfortable 23.194 0.711 0.711 0.644 0.644 ## 30 happy ~~ content 22.974 -0.157 -0.157 -0.576 -0.576 ## 33 cheerful ~~ content 16.477 0.113 0.113 0.348 0.348 ## 24 glad ~~ cheerful 11.873 -0.098 -0.098 -0.392 -0.392 ## 21 satisfaction =~ happy 11.872 -0.506 -0.506 -0.381 -0.381 ## 35 satisfied ~~ comfortable 11.662 -0.127 -0.127 -0.297 -0.297 ## 18 posAffect =~ content 11.661 -0.695 -0.695 -0.509 -0.509 14.2.2 Modified Model 2_1: mod2_1 &lt;- &quot; # cross loading posAffect =~ glad + happy + cheerful satisfaction =~ cheerful + satisfied + content + comfortable # cheerful also loads on satisfaction #residal covariance cheerful ~~ comfortable &quot; mod2_1_fit &lt;- lavaan::sem(mod2_1, data = cfaData, std.lv = TRUE, fixed.x=FALSE) summary(mod2_1_fit, fit.measures = T, standardized = T) ## lavaan 0.6-12 ended normally after 27 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 15 ## ## Number of observations 200 ## ## Model Test User Model: ## ## Test statistic 34.265 ## Degrees of freedom 6 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 1168.055 ## Degrees of freedom 15 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.975 ## Tucker-Lewis Index (TLI) 0.939 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1373.538 ## Loglikelihood unrestricted model (H1) -1356.405 ## ## Akaike (AIC) 2777.076 ## Bayesian (BIC) 2826.551 ## Sample-size adjusted Bayesian (BIC) 2779.029 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.153 ## 90 Percent confidence interval - lower 0.106 ## 90 Percent confidence interval - upper 0.205 ## P-value RMSEA &lt;= 0.05 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.031 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.128 0.063 17.786 0.000 1.128 0.954 ## happy 1.223 0.073 16.752 0.000 1.223 0.921 ## cheerful 0.389 0.084 4.624 0.000 0.389 0.374 ## satisfaction =~ ## cheerful 0.483 0.089 5.400 0.000 0.483 0.465 ## satisfied 1.151 0.071 16.168 0.000 1.151 0.902 ## content 1.284 0.074 17.339 0.000 1.284 0.941 ## comfortable 0.815 0.072 11.361 0.000 0.815 0.712 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .cheerful ~~ ## .comfortable 0.262 0.043 6.055 0.000 0.262 0.529 ## posAffect ~~ ## satisfaction 0.835 0.027 30.624 0.000 0.835 0.835 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.126 0.035 3.618 0.000 0.126 0.090 ## .happy 0.269 0.047 5.747 0.000 0.269 0.152 ## .cheerful 0.380 0.041 9.270 0.000 0.380 0.353 ## .satisfied 0.303 0.047 6.458 0.000 0.303 0.186 ## .content 0.215 0.049 4.409 0.000 0.215 0.115 ## .comfortable 0.646 0.070 9.287 0.000 0.646 0.493 ## posAffect 1.000 1.000 1.000 ## satisfaction 1.000 1.000 1.000 Model comparison: anova(mod2_1_fit, mod1_fit) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## mod2_1_fit 6 2777.1 2826.6 34.265 ## mod1_fit 7 2800.9 2847.0 60.048 25.783 1 3.821e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 14.2.3 Modified Model 2_2: mod2_2 &lt;- &quot; posAffect =~ glad + happy + cheerful satisfaction =~ satisfied + content + comfortable #residal covariance cheerful ~~ comfortable glad ~~ happy &quot; mod2_2_fit &lt;- lavaan::sem(mod2_2, data = cfaData, std.lv = TRUE, fixed.x=FALSE) summary(mod2_2_fit, fit.measures = T, standardized = T) ## lavaan 0.6-12 ended normally after 31 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 15 ## ## Number of observations 200 ## ## Model Test User Model: ## ## Test statistic 34.265 ## Degrees of freedom 6 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 1168.055 ## Degrees of freedom 15 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.975 ## Tucker-Lewis Index (TLI) 0.939 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1373.538 ## Loglikelihood unrestricted model (H1) -1356.405 ## ## Akaike (AIC) 2777.076 ## Bayesian (BIC) 2826.551 ## Sample-size adjusted Bayesian (BIC) 2779.029 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.153 ## 90 Percent confidence interval - lower 0.106 ## 90 Percent confidence interval - upper 0.205 ## P-value RMSEA &lt;= 0.05 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.031 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.020 0.069 14.784 0.000 1.020 0.863 ## happy 1.107 0.079 13.947 0.000 1.107 0.833 ## cheerful 0.875 0.061 14.323 0.000 0.875 0.843 ## satisfaction =~ ## satisfied 1.151 0.071 16.168 0.000 1.151 0.902 ## content 1.284 0.074 17.339 0.000 1.284 0.941 ## comfortable 0.815 0.072 11.361 0.000 0.815 0.712 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .cheerful ~~ ## .comfortable 0.262 0.043 6.055 0.000 0.262 0.584 ## .glad ~~ ## .happy 0.250 0.055 4.563 0.000 0.250 0.569 ## posAffect ~~ ## satisfaction 0.923 0.020 46.285 0.000 0.923 0.923 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.356 0.053 6.679 0.000 0.356 0.255 ## .happy 0.540 0.074 7.249 0.000 0.540 0.306 ## .cheerful 0.312 0.043 7.197 0.000 0.312 0.290 ## .satisfied 0.303 0.047 6.458 0.000 0.303 0.186 ## .content 0.215 0.049 4.409 0.000 0.215 0.115 ## .comfortable 0.646 0.070 9.287 0.000 0.646 0.493 ## posAffect 1.000 1.000 1.000 ## satisfaction 1.000 1.000 1.000 Model comparison: anova(mod2_2_fit, mod1_fit) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## mod2_2_fit 6 2777.1 2826.6 34.265 ## mod1_fit 7 2800.9 2847.0 60.048 25.783 1 3.821e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Keep modifying 2_2: modindices(mod2_2_fit, minimum.value = 10, sort = TRUE) ## lhs op rhs mi epc sepc.lv sepc.all sepc.nox ## 30 happy ~~ content 14.77 -0.117 -0.117 -0.344 -0.344 14.2.4 Modified Model 3: mod3 &lt;- &quot; posAffect =~ glad + happy + cheerful satisfaction =~ satisfied + content + comfortable #residal covariance cheerful ~~ comfortable glad ~~ happy content ~~ happy glad ~~ content &quot; mod3_fit &lt;- lavaan::sem(mod3, data = cfaData, std.lv = TRUE, fixed.x = FALSE) ## Warning in lav_object_post_check(object): lavaan WARNING: the covariance matrix of the residuals of the observed ## variables (theta) is not positive definite; ## use lavInspect(fit, &quot;theta&quot;) to investigate. summary(mod3_fit, fit.measures = T, standardized = T) ## lavaan 0.6-12 ended normally after 35 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 17 ## ## Number of observations 200 ## ## Model Test User Model: ## ## Test statistic 4.950 ## Degrees of freedom 4 ## P-value (Chi-square) 0.292 ## ## Model Test Baseline Model: ## ## Test statistic 1168.055 ## Degrees of freedom 15 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.999 ## Tucker-Lewis Index (TLI) 0.997 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1358.880 ## Loglikelihood unrestricted model (H1) -1356.405 ## ## Akaike (AIC) 2751.761 ## Bayesian (BIC) 2807.832 ## Sample-size adjusted Bayesian (BIC) 2753.975 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.034 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.117 ## P-value RMSEA &lt;= 0.05 0.524 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.013 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.071 0.069 15.446 0.000 1.071 0.908 ## happy 1.190 0.079 15.143 0.000 1.190 0.898 ## cheerful 0.837 0.063 13.355 0.000 0.837 0.802 ## satisfaction =~ ## satisfied 1.101 0.074 14.921 0.000 1.101 0.863 ## content 1.341 0.073 18.279 0.000 1.341 0.983 ## comfortable 0.805 0.071 11.327 0.000 0.805 0.703 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .cheerful ~~ ## .comfortable 0.299 0.046 6.433 0.000 0.299 0.590 ## .glad ~~ ## .happy 0.098 0.063 1.563 0.118 0.098 0.339 ## .happy ~~ ## .content -0.277 0.055 -4.998 0.000 -0.277 -1.896 ## .glad ~~ ## .content -0.166 0.050 -3.344 0.001 -0.166 -1.341 ## posAffect ~~ ## satisfaction 0.946 0.020 47.894 0.000 0.946 0.946 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.246 0.062 3.962 0.000 0.246 0.176 ## .happy 0.341 0.080 4.267 0.000 0.341 0.194 ## .cheerful 0.389 0.048 8.016 0.000 0.389 0.357 ## .satisfied 0.416 0.058 7.200 0.000 0.416 0.256 ## .content 0.062 0.065 0.966 0.334 0.062 0.034 ## .comfortable 0.662 0.069 9.660 0.000 0.662 0.505 ## posAffect 1.000 1.000 1.000 ## satisfaction 1.000 1.000 1.000 Model comparison: anova(mod3_fit, mod2_2_fit) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## mod3_fit 4 2751.8 2807.8 4.9499 ## mod2_2_fit 6 2777.1 2826.6 34.2652 29.315 2 4.308e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Keep modifying mod3: modindices(mod3_fit, minimum.value = 1, sort = TRUE) ## Warning in lav_start_check_cov(lavpartable = lavpartable, start = START): lavaan WARNING: starting values imply a correlation larger than 1; ## variables involved are: happy content ## Warning in lav_start_check_cov(lavpartable = lavpartable, start = START): lavaan WARNING: starting values imply a correlation larger than 1; ## variables involved are: glad content ## [1] lhs op rhs mi epc sepc.lv sepc.all sepc.nox ## &lt;0 rows&gt; (or 0-length row.names) No suggestions could decrease the model chisquare by more than 10. "],["lavaan-lab-12-sem-for-missing-data.html", "Chapter 15 Lavaan Lab 12: SEM for Missing Data 15.1 PART I: Generate some missing data 15.2 PART II: Visualization of missing data patterns (nice-to-have) 15.3 PART III: Build a CFA model with missing data 15.4 PART IV: Addressing missing data", " Chapter 15 Lavaan Lab 12: SEM for Missing Data In this lab, we’ll use an example dataset HolzingerSwineford1939 in the package lavaan. Hence, lavaan must be installed. Load up the lavaan library: library(lavaan) Use data() to load HolzingerSwineford1939: data(HolzingerSwineford1939) head(HolzingerSwineford1939,3) ## id sex ageyr agemo school grade x1 x2 x3 x4 x5 x6 x7 x8 x9 ## 1 1 1 13 1 Pasteur 7 3.333333 7.75 0.375 2.333333 5.75 1.2857143 3.391304 5.75 6.361111 ## 2 2 2 13 7 Pasteur 7 5.333333 5.25 2.125 1.666667 3.00 1.2857143 3.782609 6.25 7.916667 ## 3 3 2 13 1 Pasteur 7 4.500000 5.25 1.875 1.000000 1.75 0.4285714 3.260870 3.90 4.416667 tail(HolzingerSwineford1939,3) ## id sex ageyr agemo school grade x1 x2 x3 x4 x5 x6 x7 x8 x9 ## 299 348 2 14 3 Grant-White 8 4.666667 5.50 1.875 3.666667 5.75 4.285714 4.000000 6.00 7.611111 ## 300 349 1 14 2 Grant-White 8 4.333333 6.75 0.500 3.666667 4.50 2.000000 5.086957 6.20 4.388889 ## 301 351 1 13 5 Grant-White NA 4.333333 6.00 3.375 3.666667 5.75 3.142857 4.086957 6.95 5.166667 ?HolzingerSwineford1939 The classic Holzinger and Swineford (1939) dataset consists of mental ability test scores of seventh- and eighth-grade children from two different schools (Pasteur and Grant-White). In the original dataset (available in the MBESS package), there are scores for 26 tests. However, a smaller subset with 9 variables is more widely used in the literature (for example in Joreskog’s 1969 paper, which also uses the 145 subjects from the Grant-White school only). 15.1 PART I: Generate some missing data HolzingerSwineford1939 has complete dataset on all nine indicators x1-x9. In this example, we will create some missingness in x5 and x9. For a commented analysis, check vignettes of the R package lslx. First, missingness on x5 depends on x1: lowest 20% of x1 miss x5 values data_miss &lt;- lavaan::HolzingerSwineford1939 data_miss$x5 &lt;- ifelse(data_miss$x1 &lt;= quantile(data_miss$x1, .2), NA, data_miss$x5) Second, missingness on x9 depends on age: lowest 10% of age group miss x9 values Note that age is created by ageyr and agemo. Since ageyr and agemo are not the variables that we are interested, the two variables are treated as auxiliary in the later analysis. data_miss$age &lt;- data_miss$ageyr + data_miss$agemo/12 data_miss$x9 &lt;- ifelse(data_miss$age &lt;= quantile(data_miss$age, .1), NA, data_miss$x9) head(data_miss) ## id sex ageyr agemo school grade x1 x2 x3 x4 x5 x6 x7 x8 x9 age ## 1 1 1 13 1 Pasteur 7 3.333333 7.75 0.375 2.333333 NA 1.2857143 3.391304 5.75 6.361111 13.08333 ## 2 2 2 13 7 Pasteur 7 5.333333 5.25 2.125 1.666667 3.00 1.2857143 3.782609 6.25 7.916667 13.58333 ## 3 3 2 13 1 Pasteur 7 4.500000 5.25 1.875 1.000000 1.75 0.4285714 3.260870 3.90 4.416667 13.08333 ## 4 4 1 13 2 Pasteur 7 5.333333 7.75 3.000 2.666667 4.50 2.4285714 3.000000 5.30 4.861111 13.16667 ## 5 5 2 12 2 Pasteur 7 4.833333 4.75 0.875 2.666667 4.00 2.5714286 3.695652 6.30 NA 12.16667 ## 6 6 2 14 1 Pasteur 7 5.333333 5.00 2.250 1.000000 3.00 0.8571429 4.347826 6.65 7.500000 14.08333 use the function is.na() to return a matrix of missing data indicators (missing: true, complete: false) na.eval = is.na(data_miss) head(na.eval[,7:15], 3) ## x1 x2 x3 x4 x5 x6 x7 x8 x9 ## [1,] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [2,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE missing counts for each column (variable) colSums(na.eval) ## id sex ageyr agemo school grade x1 x2 x3 x4 x5 x6 x7 x8 x9 age ## 0 0 0 0 0 1 0 0 0 0 65 0 0 0 33 0 65 values are missing on x5 33 values are missing on x9 MCAR, MAR, OR MNAR? Ans: MAR for both x5 and x9 A small tip: if you want a complete version of the dataset, use function na.omit() data.complete = na.omit(data_miss) dim(data.complete) # [1] 208 16 ## [1] 208 16 15.2 PART II: Visualization of missing data patterns (nice-to-have) To visualize and handle missingness, we need mice package: #install.packages(&#39;mice&#39;, dependencies=TRUE) library(&quot;mice&quot;) Display missing-data patterns: md.pattern(data_miss) ## id sex ageyr agemo school x1 x2 x3 x4 x6 x7 x8 age grade x9 x5 ## 208 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 ## 59 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 ## 27 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 ## 6 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 2 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 ## 0 0 0 0 0 0 0 0 0 0 0 0 0 1 33 65 99 Three variables with missing values on the right side: grade x9 x5: five rows: five patterns: 208 cases with complete responses (0 variable missing) 59 cases with only x5 missing (1 variable missing) 27 cases with only x9 missing (1 variable missing) 6 cases with both x5 and x9 missing (2 variables missing) 1 case with grade missing (1 variable missing) 15.3 PART III: Build a CFA model with missing data Write out syntax for a three-factor CFA model: HS.model &lt;- &#39; visual =~ x1 + x2 + x3 textual =~ x4 + x5 + x6 speed =~ x7 + x8 + x9 &#39; Left untreated, the default in sem() is listwise deletion: fit.listwise &lt;- lavaan::sem(HS.model, data = data_miss, fixed.x = FALSE) summary(fit.listwise, fit.measures = TRUE) ## lavaan 0.6-12 ended normally after 35 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 21 ## ## Used Total ## Number of observations 209 301 ## ## Model Test User Model: ## ## Test statistic 57.615 ## Degrees of freedom 24 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 602.992 ## Degrees of freedom 36 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.941 ## Tucker-Lewis Index (TLI) 0.911 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -2531.199 ## Loglikelihood unrestricted model (H1) -2502.391 ## ## Akaike (AIC) 5104.397 ## Bayesian (BIC) 5174.586 ## Sample-size adjusted Bayesian (BIC) 5108.047 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.082 ## 90 Percent confidence interval - lower 0.055 ## 90 Percent confidence interval - upper 0.109 ## P-value RMSEA &lt;= 0.05 0.028 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.068 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## visual =~ ## x1 1.000 ## x2 0.616 0.176 3.501 0.000 ## x3 0.728 0.188 3.870 0.000 ## textual =~ ## x4 1.000 ## x5 1.170 0.081 14.521 0.000 ## x6 0.955 0.068 14.114 0.000 ## speed =~ ## x7 1.000 ## x8 1.063 0.191 5.582 0.000 ## x9 0.853 0.153 5.586 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## visual ~~ ## textual 0.308 0.065 4.700 0.000 ## speed 0.149 0.050 2.983 0.003 ## textual ~~ ## speed 0.169 0.062 2.715 0.007 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .x1 0.277 0.103 2.685 0.007 ## .x2 1.151 0.122 9.420 0.000 ## .x3 1.018 0.115 8.816 0.000 ## .x4 0.438 0.059 7.378 0.000 ## .x5 0.369 0.066 5.573 0.000 ## .x6 0.327 0.049 6.663 0.000 ## .x7 0.716 0.101 7.079 0.000 ## .x8 0.518 0.095 5.427 0.000 ## .x9 0.590 0.079 7.492 0.000 ## visual 0.448 0.119 3.759 0.000 ## textual 0.960 0.136 7.049 0.000 ## speed 0.441 0.115 3.831 0.000 15.4 PART IV: Addressing missing data 15.4.1 FIML fit.fiml &lt;- lavaan::sem(HS.model, data = data_miss, missing = &#39;fiml&#39;, fixed.x = FALSE) summary(fit.fiml, fit.measures = TRUE) ## lavaan 0.6-12 ended normally after 52 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 30 ## ## Number of observations 301 ## Number of missing patterns 4 ## ## Model Test User Model: ## ## Test statistic 71.947 ## Degrees of freedom 24 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 842.783 ## Degrees of freedom 36 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.941 ## Tucker-Lewis Index (TLI) 0.911 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -3609.653 ## Loglikelihood unrestricted model (H1) -3573.679 ## ## Akaike (AIC) 7279.306 ## Bayesian (BIC) 7390.519 ## Sample-size adjusted Bayesian (BIC) 7295.376 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.081 ## 90 Percent confidence interval - lower 0.060 ## 90 Percent confidence interval - upper 0.103 ## P-value RMSEA &lt;= 0.05 0.009 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.060 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Observed ## Observed information based on Hessian ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## visual =~ ## x1 1.000 ## x2 0.549 0.110 5.001 0.000 ## x3 0.720 0.117 6.166 0.000 ## textual =~ ## x4 1.000 ## x5 1.146 0.071 16.114 0.000 ## x6 0.949 0.058 16.259 0.000 ## speed =~ ## x7 1.000 ## x8 1.175 0.154 7.632 0.000 ## x9 0.982 0.160 6.143 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## visual ~~ ## textual 0.420 0.080 5.256 0.000 ## speed 0.248 0.056 4.416 0.000 ## textual ~~ ## speed 0.161 0.050 3.201 0.001 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .x1 4.936 0.067 73.473 0.000 ## .x2 6.088 0.068 89.855 0.000 ## .x3 2.250 0.065 34.579 0.000 ## .x4 3.061 0.067 45.694 0.000 ## .x5 4.316 0.078 55.567 0.000 ## .x6 2.186 0.063 34.667 0.000 ## .x7 4.186 0.063 66.766 0.000 ## .x8 5.527 0.058 94.854 0.000 ## .x9 5.381 0.061 88.408 0.000 ## visual 0.000 ## textual 0.000 ## speed 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .x1 0.540 0.121 4.458 0.000 ## .x2 1.135 0.105 10.845 0.000 ## .x3 0.851 0.095 8.948 0.000 ## .x4 0.397 0.050 8.015 0.000 ## .x5 0.378 0.060 6.301 0.000 ## .x6 0.337 0.044 7.696 0.000 ## .x7 0.773 0.083 9.363 0.000 ## .x8 0.455 0.086 5.278 0.000 ## .x9 0.624 0.085 7.358 0.000 ## visual 0.819 0.152 5.391 0.000 ## textual 0.953 0.112 8.533 0.000 ## speed 0.410 0.090 4.539 0.000 15.4.2 Multiple Imputation To perform MI with lavaan, we turn to the R-package semTools which offers many functions that extends the basic sem() function. #install.packages(&#39;semTools&#39;, dependencies=TRUE) library(&quot;semTools&quot;) Mice also utilizes information from auxiliary variables. Since we don’t know which ones are auxiliary variables, let’s include sex, age and grade and generate imputed datasets. Again, MI consists of three steps: Imputation Step Analysis Step Pooling Step out1 &lt;- cfa.mi(HS.model, data=data_miss[,c(&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;, &quot;x4&quot;, &quot;x5&quot;, &quot;x6&quot;, &quot;x7&quot;, &quot;x8&quot;, &quot;x9&quot;, &quot;sex&quot;, &quot;age&quot;, &quot;grade&quot;)], fixed.x = FALSE, m = 20, miPackage=&quot;mice&quot;, seed = 12345) summary(out1) ## lavaan.mi object based on 20 imputed data sets. ## See class?lavaan.mi help page for available methods. ## ## Convergence information: ## The model converged on 20 imputed data sets ## ## Rubin&#39;s (1987) rules were used to pool point and SE estimates across 20 imputed data sets, and to calculate degrees of freedom for each parameter&#39;s t test and CI. ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err t-value df P(&gt;|t|) ## visual =~ ## x1 1.000 ## x2 0.551 0.104 5.313 Inf 0.000 ## x3 0.722 0.114 6.354 Inf 0.000 ## textual =~ ## x4 1.000 ## x5 1.148 0.068 16.758 309.417 0.000 ## x6 0.946 0.058 16.333 1421.833 0.000 ## speed =~ ## x7 1.000 ## x8 1.171 0.173 6.779 Inf 0.000 ## x9 0.996 0.147 6.778 941.261 0.000 ## ## Covariances: ## Estimate Std.Err t-value df P(&gt;|t|) ## visual ~~ ## textual 0.408 0.076 5.377 6795.170 0.000 ## speed 0.254 0.059 4.304 Inf 0.000 ## textual ~~ ## speed 0.165 0.051 3.219 Inf 0.001 ## ## Variances: ## Estimate Std.Err t-value df P(&gt;|t|) ## .x1 0.542 0.120 4.518 Inf 0.000 ## .x2 1.134 0.106 10.720 Inf 0.000 ## .x3 0.849 0.095 8.979 Inf 0.000 ## .x4 0.392 0.049 7.992 724.963 0.000 ## .x5 0.400 0.058 6.889 286.554 0.000 ## .x6 0.338 0.043 7.837 677.723 0.000 ## .x7 0.780 0.085 9.128 Inf 0.000 ## .x8 0.469 0.080 5.841 9887.286 0.000 ## .x9 0.616 0.074 8.344 850.258 0.000 ## visual 0.816 0.153 5.332 Inf 0.000 ## textual 0.959 0.115 8.318 Inf 0.000 ## speed 0.403 0.093 4.343 Inf 0.000 "],["lavaan-lab-13-sem-for-nonnormal-and-categorical-data.html", "Chapter 16 Lavaan Lab 13: SEM for Nonnormal and Categorical Data 16.1 PART I: Nonnormality Diagnosis 16.2 PART II: Robust corrections 16.3 PART III: Categorical Data Analysis in Lavaan 16.4 PART IV: What if you have it all?", " Chapter 16 Lavaan Lab 13: SEM for Nonnormal and Categorical Data Load up the lavaan library: library(lavaan) 16.1 PART I: Nonnormality Diagnosis Let’s first load the simulated non-normal data and look at the normality/nonnormality of the items: nnorm_dat &lt;- read.csv(&quot;nonnormal.csv&quot;, header = T) head(nnorm_dat) ## odd1 odd2 odd3 odd4 odd5 odd6 odd7 odd8 ## 1 1.455398 1.1968135 -0.52181159 -1.9651704 -4.2353119 1.0959168 1.1434283 -0.4393833 ## 2 6.517492 2.7962582 0.56780460 4.6117031 5.0850284 1.4194502 1.3925536 5.2085814 ## 3 2.047266 -2.1148975 -1.35879880 0.6664211 3.1903653 2.0040147 0.5134188 0.2114613 ## 4 8.251226 4.9818940 10.29910350 7.5773213 0.2595174 1.8953510 -2.5800594 -2.0036514 ## 5 3.553213 4.7921231 2.44286008 1.4219472 0.5007659 1.2665319 0.4181851 1.1586593 ## 6 2.728876 -0.3436493 0.07283766 -1.8742132 -3.0228869 0.5607386 -0.7374122 -0.3249854 par(mfrow = c(2, 2)) #opens graph window with 2 rows 2 columns hist(nnorm_dat$odd5) hist(nnorm_dat$odd6) hist(nnorm_dat$odd7) hist(nnorm_dat$odd8) Use describe() function from the psych package to get univariate descriptives: #install.packages(&quot;psych&quot;) library(psych) describe(nnorm_dat) ## vars n mean sd median trimmed mad min max range skew kurtosis se ## odd1 1 1000 1.23 2.19 1.18 1.17 1.49 -9.50 18.41 27.91 0.87 7.77 0.07 ## odd2 2 1000 0.91 2.18 0.87 0.89 1.65 -12.49 17.81 30.30 0.45 7.07 0.07 ## odd3 3 1000 1.05 2.21 1.02 1.03 1.41 -18.10 22.44 40.55 0.58 18.62 0.07 ## odd4 4 1000 0.65 2.19 0.55 0.62 1.64 -18.04 12.43 30.47 -0.39 8.35 0.07 ## odd5 5 1000 0.46 2.24 0.56 0.53 1.66 -16.83 7.79 24.62 -0.99 6.17 0.07 ## odd6 6 1000 0.54 2.32 0.63 0.62 1.59 -24.87 12.98 37.84 -2.37 24.73 0.07 ## odd7 7 1000 0.26 1.91 0.23 0.28 1.32 -12.30 16.58 28.87 0.07 12.23 0.06 ## odd8 8 1000 0.28 1.89 0.24 0.28 1.33 -8.26 12.64 20.90 0.39 6.23 0.06 Use mardia() from the psych package to test multivariate normality: par(mfrow = c(1, 1)) #opens graph window mardia(nnorm_dat) ## Call: mardia(x = nnorm_dat) ## ## Mardia tests of multivariate skew and kurtosis ## Use describe(x) the to get univariate tests ## n.obs = 1000 num.vars = 8 ## b1p = 40.96 skew = 6827.1 with probability &lt;= 0 ## small sample skew = 6852.15 with probability &lt;= 0 ## b2p = 323.37 kurtosis = 304.21 with probability &lt;= 0 In any case, these data are clearly far from normal, so … 16.2 PART II: Robust corrections Write out syntax for a one-factor CFA model: cfaSyn &lt;- &quot; odd =~ odd1 + odd2 + odd3 + odd4 + odd5 + odd6 + odd7 + odd8 &quot; Fit the one-factor model: mlrFit &lt;- lavaan::sem(cfaSyn, data = nnorm_dat, fixed.x = FALSE, estimator = &quot;mlr&quot;) summary(mlrFit, fit.measure = T) ## lavaan 0.6-12 ended normally after 34 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 16 ## ## Number of observations 1000 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 430.290 125.915 ## Degrees of freedom 20 20 ## P-value (Chi-square) 0.000 0.000 ## Scaling correction factor 3.417 ## Yuan-Bentler correction (Mplus variant) ## ## Model Test Baseline Model: ## ## Test statistic 1692.974 454.468 ## Degrees of freedom 28 28 ## P-value 0.000 0.000 ## Scaling correction factor 3.725 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.754 0.752 ## Tucker-Lewis Index (TLI) 0.655 0.652 ## ## Robust Comparative Fit Index (CFI) 0.772 ## Robust Tucker-Lewis Index (TLI) 0.681 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -16785.305 -16785.305 ## Scaling correction factor 5.604 ## for the MLR correction ## Loglikelihood unrestricted model (H1) -16570.161 -16570.161 ## Scaling correction factor 4.389 ## for the MLR correction ## ## Akaike (AIC) 33602.611 33602.611 ## Bayesian (BIC) 33681.135 33681.135 ## Sample-size adjusted Bayesian (BIC) 33630.318 33630.318 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.143 0.073 ## 90 Percent confidence interval - lower 0.132 0.066 ## 90 Percent confidence interval - upper 0.155 0.079 ## P-value RMSEA &lt;= 0.05 0.000 0.000 ## ## Robust RMSEA 0.135 ## 90 Percent confidence interval - lower 0.113 ## 90 Percent confidence interval - upper 0.157 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.077 0.077 ## ## Parameter Estimates: ## ## Standard errors Sandwich ## Information bread Observed ## Observed information based on Hessian ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## odd =~ ## odd1 1.000 ## odd2 1.095 0.146 7.499 0.000 ## odd3 1.035 0.172 6.000 0.000 ## odd4 0.946 0.150 6.317 0.000 ## odd5 0.778 0.198 3.920 0.000 ## odd6 0.939 0.178 5.282 0.000 ## odd7 0.810 0.158 5.135 0.000 ## odd8 0.890 0.144 6.198 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .odd1 3.312 0.486 6.814 0.000 ## .odd2 3.003 0.295 10.197 0.000 ## .odd3 3.310 0.410 8.074 0.000 ## .odd4 3.490 0.482 7.234 0.000 ## .odd5 4.105 0.418 9.811 0.000 ## .odd6 4.092 0.645 6.345 0.000 ## .odd7 2.665 0.437 6.099 0.000 ## .odd8 2.394 0.232 10.313 0.000 ## odd 1.469 0.355 4.138 0.000 16.3 PART III: Categorical Data Analysis in Lavaan Let’s load the simulated data in which ODD items are ordinal: odd &lt;- read.csv(&quot;oddData.csv&quot;, header = T) head(odd) ## id odd1 odd2 odd3 odd4 odd5 odd6 odd7 odd8 ## 1 4 1 1 1 1 1 0 0 0 ## 2 7 2 0 1 0 0 0 1 0 ## 3 12 2 0 1 0 1 0 0 0 ## 4 14 1 1 2 1 1 1 0 1 ## 5 37 1 1 1 0 0 0 0 0 ## 6 39 1 1 1 0 0 1 1 0 Write out syntax for a one-factor CFA model: oddOneFac = &#39; #Specify Overall Odd Factor odd =~ odd1 + odd2 + odd3 + odd4 + odd5 + odd6 + odd7 + odd8 &#39; Fit the one-factor model: label ordinal variables using ordered argument: ordered = c( #NAMES OF ORDINAL INDICATORS#) oneFacFit &lt;- lavaan::sem(oddOneFac, data = odd, ordered=c(&#39;odd1&#39;,&#39;odd2&#39;,&#39;odd3&#39;,&#39;odd4&#39;,&#39;odd5&#39;,&#39;odd6&#39;,&#39;odd7&#39;,&#39;odd8&#39;), fixed.x = FALSE, parameterization = &#39;theta&#39;) # or delta #declare these as ordered variable summary(oneFacFit, fit.measures = T) ## lavaan 0.6-12 ended normally after 34 iterations ## ## Estimator DWLS ## Optimization method NLMINB ## Number of model parameters 24 ## ## Number of observations 221 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 44.892 63.690 ## Degrees of freedom 20 20 ## P-value (Chi-square) 0.001 0.000 ## Scaling correction factor 0.732 ## Shift parameter 2.377 ## simple second-order correction ## ## Model Test Baseline Model: ## ## Test statistic 888.285 643.487 ## Degrees of freedom 28 28 ## P-value 0.000 0.000 ## Scaling correction factor 1.398 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.971 0.929 ## Tucker-Lewis Index (TLI) 0.959 0.901 ## ## Robust Comparative Fit Index (CFI) NA ## Robust Tucker-Lewis Index (TLI) NA ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.075 0.100 ## 90 Percent confidence interval - lower 0.046 0.073 ## 90 Percent confidence interval - upper 0.105 0.128 ## P-value RMSEA &lt;= 0.05 0.076 0.002 ## ## Robust RMSEA NA ## 90 Percent confidence interval - lower NA ## 90 Percent confidence interval - upper NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.085 0.085 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Unstructured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## odd =~ ## odd1 1.000 ## odd2 0.850 0.180 4.725 0.000 ## odd3 0.759 0.152 4.981 0.000 ## odd4 0.602 0.138 4.373 0.000 ## odd5 0.404 0.099 4.079 0.000 ## odd6 0.607 0.128 4.735 0.000 ## odd7 0.848 0.190 4.456 0.000 ## odd8 0.871 0.186 4.694 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .odd1 0.000 ## .odd2 0.000 ## .odd3 0.000 ## .odd4 0.000 ## .odd5 0.000 ## .odd6 0.000 ## .odd7 0.000 ## .odd8 0.000 ## odd 0.000 ## ## Thresholds: ## Estimate Std.Err z-value P(&gt;|z|) ## odd1|t1 -1.884 0.203 -9.262 0.000 ## odd1|t2 0.613 0.136 4.500 0.000 ## odd2|t1 -0.970 0.136 -7.122 0.000 ## odd2|t2 1.138 0.147 7.721 0.000 ## odd3|t1 -1.355 0.153 -8.869 0.000 ## odd3|t2 1.018 0.134 7.587 0.000 ## odd4|t1 0.076 0.103 0.737 0.461 ## odd4|t2 1.699 0.167 10.168 0.000 ## odd5|t1 0.031 0.093 0.335 0.737 ## odd5|t2 1.417 0.129 10.962 0.000 ## odd6|t1 -0.174 0.103 -1.680 0.093 ## odd6|t2 1.568 0.153 10.276 0.000 ## odd7|t1 0.632 0.134 4.711 0.000 ## odd7|t2 2.692 0.325 8.287 0.000 ## odd8|t1 0.981 0.151 6.510 0.000 ## odd8|t2 2.547 0.283 9.003 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .odd1 1.000 ## .odd2 1.000 ## .odd3 1.000 ## .odd4 1.000 ## .odd5 1.000 ## .odd6 1.000 ## .odd7 1.000 ## .odd8 1.000 ## odd 1.330 0.382 3.483 0.000 ## ## Scales y*: ## Estimate Std.Err z-value P(&gt;|z|) ## odd1 0.655 ## odd2 0.714 ## odd3 0.753 ## odd4 0.822 ## odd5 0.906 ## odd6 0.819 ## odd7 0.715 ## odd8 0.705 16.4 PART IV: What if you have it all? Unfortunately you cannot use missing = ‘fiml’ for categorical data: FitMessy &lt;- lavaan::sem(oddOneFac, data = odd, ordered=c(&#39;odd1&#39;,&#39;odd2&#39;,&#39;odd3&#39;,&#39;odd4&#39;,&#39;odd5&#39;,&#39;odd6&#39;,&#39;odd7&#39;,&#39;odd8&#39;), fixed.x = FALSE, estimator = &quot;DWLS&quot;, #missing = &#39;fiml&#39; ) FitMessy ## lavaan 0.6-12 ended normally after 16 iterations ## ## Estimator DWLS ## Optimization method NLMINB ## Number of model parameters 24 ## ## Number of observations 221 ## ## Model Test User Model: ## ## Test statistic 44.892 ## Degrees of freedom 20 ## P-value (Chi-square) 0.001 #summary(FitMessy, fit.measures = T) But you cannot use missing = ‘fiml’ together with MLR for nonnormal data: FitMessy &lt;- lavaan::sem(oddOneFac, data = nnorm_dat, #ordered=c(&#39;odd1&#39;,&#39;odd2&#39;,&#39;odd3&#39;,&#39;odd4&#39;,&#39;odd5&#39;,&#39;odd6&#39;,&#39;odd7&#39;,&#39;odd8&#39;), fixed.x = FALSE, estimator = &quot;mlr&quot;, missing = &#39;fiml&#39;) FitMessy ## lavaan 0.6-12 ended normally after 34 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 24 ## ## Number of observations 1000 ## Number of missing patterns 1 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 430.290 125.915 ## Degrees of freedom 20 20 ## P-value (Chi-square) 0.000 0.000 ## Scaling correction factor 3.417 ## Yuan-Bentler correction (Mplus variant) #summary(FitMessy, fit.measures = T) "],["lavaan-lab-14-measurement-invariance.html", "Chapter 17 Lavaan Lab 14: Measurement Invariance 17.1 PART I: Multi-Group Analyses, Done Incorrectly 17.2 PART II: Testing Measurement Invariance 17.3 PART III: Shortcut to performing MI 17.4 PART IV: Multi-Group CFA Modeling, done right", " Chapter 17 Lavaan Lab 14: Measurement Invariance For this lab, we will run the MG-CFA analyses in class using simulated data based on Todd Little’s positive affect example. Load up the lavaan library: library(lavaan) and the dataset: affectData &lt;- read.csv(&quot;cfaInclassData.csv&quot;, header = T) For demonstration purposes, let’s first simulate a grouping variable called school: set.seed(555) affectData$school = sample(c(&#39;public&#39;, &#39;private&#39;), nrow(affectData), replace = T) table(affectData$school) ## ## private public ## 513 487 head(affectData) ## ID glad cheerful happy satisfied content comfortable school ## 1 1 0.13521092 0.5413297 -0.1041445 -0.5777446 0.8645383 0.02935020 private ## 2 2 -0.29116043 0.2434081 0.6671535 2.0763730 -0.7382832 1.05439183 public ## 3 3 0.71975913 0.2218277 0.4722337 2.1685984 -0.2727574 0.09053090 private ## 4 4 0.44432030 0.9295414 0.8574083 -1.0575363 -1.3841364 -0.07940091 private ## 5 5 2.84476524 3.1710123 3.5145040 1.5725274 2.3406754 1.59866763 public ## 6 6 -0.03317526 -0.8434011 -0.1485924 -0.5469343 -1.5750953 -0.69629828 public The goal of testing measurement invariance (MI) is to make sure that the scale that measures positive affect and satisfaction functions in the same way between public and private schools. 17.1 PART I: Multi-Group Analyses, Done Incorrectly Syntax for an SR model (it doesn’t matter whether the model is for CFA or SR, the test of MI only applies to the CFA part): srSyntax &lt;- &quot; posAffect =~ glad + happy + cheerful satisfaction =~ satisfied + content + comfortable # Structural Regression: beta satisfaction ~ posAffect &quot; MGsrRunWRONG &lt;- lavaan::sem(srSyntax, data = affectData, fixed.x=FALSE, group = &quot;school&quot;, # group indicator estimator = &quot;MLR&quot;) # use MLR as a go-to estimation method summary(MGsrRunWRONG, standardized = T, fit.measures = T) ## lavaan 0.6-12 ended normally after 31 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 38 ## ## Number of observations per group: ## private 513 ## public 487 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 11.920 11.709 ## Degrees of freedom 16 16 ## P-value (Chi-square) 0.749 0.764 ## Scaling correction factor 1.018 ## Yuan-Bentler correction (Mplus variant) ## Test statistic for each group: ## private 4.201 4.126 ## public 7.719 7.583 ## ## Model Test Baseline Model: ## ## Test statistic 2038.064 2039.295 ## Degrees of freedom 30 30 ## P-value 0.000 0.000 ## Scaling correction factor 0.999 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 1.000 ## Tucker-Lewis Index (TLI) 1.004 1.004 ## ## Robust Comparative Fit Index (CFI) 1.000 ## Robust Tucker-Lewis Index (TLI) 1.004 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -7473.002 -7473.002 ## Scaling correction factor 1.001 ## for the MLR correction ## Loglikelihood unrestricted model (H1) -7467.042 -7467.042 ## Scaling correction factor 1.006 ## for the MLR correction ## ## Akaike (AIC) 15022.004 15022.004 ## Bayesian (BIC) 15208.498 15208.498 ## Sample-size adjusted Bayesian (BIC) 15087.808 15087.808 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 0.000 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.030 0.029 ## P-value RMSEA &lt;= 0.05 0.998 0.999 ## ## Robust RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.029 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.013 0.013 ## ## Parameter Estimates: ## ## Standard errors Sandwich ## Information bread Observed ## Observed information based on Hessian ## ## ## Group 1 [private]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.740 0.754 ## happy 0.975 0.065 14.962 0.000 0.722 0.743 ## cheerful 1.103 0.068 16.264 0.000 0.816 0.807 ## satisfaction =~ ## satisfied 1.000 0.792 0.779 ## content 1.065 0.071 14.999 0.000 0.843 0.761 ## comfortable 0.874 0.059 14.910 0.000 0.692 0.741 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## satisfaction ~ ## posAffect 0.520 0.065 8.033 0.000 0.486 0.486 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.023 0.043 0.530 0.596 0.023 0.023 ## .happy 0.012 0.043 0.270 0.787 0.012 0.012 ## .cheerful 0.027 0.045 0.594 0.553 0.027 0.026 ## .satisfied -0.102 0.045 -2.269 0.023 -0.102 -0.100 ## .content -0.086 0.049 -1.754 0.079 -0.086 -0.077 ## .comfortable -0.061 0.041 -1.481 0.138 -0.061 -0.065 ## posAffect 0.000 0.000 0.000 ## .satisfaction 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.417 0.037 11.257 0.000 0.417 0.432 ## .happy 0.422 0.038 11.130 0.000 0.422 0.448 ## .cheerful 0.357 0.037 9.643 0.000 0.357 0.349 ## .satisfied 0.406 0.039 10.484 0.000 0.406 0.393 ## .content 0.519 0.047 11.070 0.000 0.519 0.422 ## .comfortable 0.393 0.036 10.765 0.000 0.393 0.451 ## posAffect 0.548 0.059 9.213 0.000 1.000 1.000 ## .satisfaction 0.478 0.057 8.380 0.000 0.763 0.763 ## ## ## Group 2 [public]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.643 0.655 ## happy 1.182 0.091 13.041 0.000 0.761 0.775 ## cheerful 1.130 0.091 12.455 0.000 0.727 0.764 ## satisfaction =~ ## satisfied 1.000 0.749 0.753 ## content 1.072 0.072 14.806 0.000 0.803 0.763 ## comfortable 0.972 0.065 14.904 0.000 0.729 0.753 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## satisfaction ~ ## posAffect 0.568 0.074 7.725 0.000 0.488 0.488 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad -0.022 0.045 -0.492 0.623 -0.022 -0.022 ## .happy 0.005 0.044 0.114 0.910 0.005 0.005 ## .cheerful -0.030 0.043 -0.697 0.486 -0.030 -0.032 ## .satisfied 0.018 0.045 0.399 0.690 0.018 0.018 ## .content 0.012 0.048 0.251 0.802 0.012 0.011 ## .comfortable -0.026 0.044 -0.597 0.551 -0.026 -0.027 ## posAffect 0.000 0.000 0.000 ## .satisfaction 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.552 0.043 12.930 0.000 0.552 0.571 ## .happy 0.385 0.043 8.958 0.000 0.385 0.400 ## .cheerful 0.377 0.041 9.304 0.000 0.377 0.416 ## .satisfied 0.429 0.042 10.338 0.000 0.429 0.433 ## .content 0.462 0.048 9.576 0.000 0.462 0.417 ## .comfortable 0.405 0.040 10.259 0.000 0.405 0.433 ## posAffect 0.414 0.058 7.152 0.000 1.000 1.000 ## .satisfaction 0.428 0.053 8.110 0.000 0.762 0.762 library(semPlot) semPaths(MGsrRunWRONG, what=&#39;est&#39;, nCharNodes = 0, nCharEdges = 0, # don&#39;t limit variable name lengths edge.label.cex=0.6, curvePivot = TRUE, curve = 1.5, # pull covariances&#39; curves out a little fade=FALSE) 17.2 PART II: Testing Measurement Invariance 17.2.1 step 1: Configural invariance If you simply use the MGsrRunWRONG synatx above, you are just testing configural invariance: configuralFit &lt;- lavaan::sem(srSyntax, data = affectData, fixed.x=FALSE, group = &quot;school&quot;, # group indicator estimator = &quot;MLR&quot;) # use MLR as a go-to estimation method #summary(configuralFit, standardized = T, fit.measures = T) Configural invariance was established due to satisfying model fit; 17.2.2 step 2: Metric (weak) invariance To test metric invariance, you could manually constraint all factor loadings to be the same using tricks like “posAffect =~ c(lam1, lam1)*glad” but there is a shortcut using “group.equal” argument: metricFit &lt;- lavaan::sem(srSyntax, data = affectData, fixed.x=FALSE, estimator = &#39;MLR&#39;, group = &quot;school&quot;, group.equal = c(&quot;loadings&quot;)) so that all factor loadings are fixed to be the same across groups More group equality constraints can be added, like “intercepts”, “means”, “residuals”, “residual.covariances”, “lv.variances”, “lv.covariances”, “regressions” summary(metricFit, standardized = T, fit.measures = T) ## lavaan 0.6-12 ended normally after 25 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 38 ## Number of equality constraints 4 ## ## Number of observations per group: ## private 513 ## public 487 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 17.640 17.639 ## Degrees of freedom 20 20 ## P-value (Chi-square) 0.611 0.611 ## Scaling correction factor 1.000 ## Yuan-Bentler correction (Mplus variant) ## Test statistic for each group: ## private 6.665 6.665 ## public 10.975 10.974 ## ## Model Test Baseline Model: ## ## Test statistic 2038.064 2039.295 ## Degrees of freedom 30 30 ## P-value 0.000 0.000 ## Scaling correction factor 0.999 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 1.000 ## Tucker-Lewis Index (TLI) 1.002 1.002 ## ## Robust Comparative Fit Index (CFI) 1.000 ## Robust Tucker-Lewis Index (TLI) 1.002 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -7475.862 -7475.862 ## Scaling correction factor 0.904 ## for the MLR correction ## Loglikelihood unrestricted model (H1) -7467.042 -7467.042 ## Scaling correction factor 1.006 ## for the MLR correction ## ## Akaike (AIC) 15019.724 15019.724 ## Bayesian (BIC) 15186.588 15186.588 ## Sample-size adjusted Bayesian (BIC) 15078.602 15078.602 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 0.000 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.033 0.033 ## P-value RMSEA &lt;= 0.05 0.998 0.998 ## ## Robust RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.033 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.019 0.019 ## ## Parameter Estimates: ## ## Standard errors Sandwich ## Information bread Observed ## Observed information based on Hessian ## ## ## Group 1 [private]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.718 0.740 ## happy (.p2.) 1.057 0.054 19.585 0.000 0.759 0.767 ## cheerfl (.p3.) 1.113 0.055 20.393 0.000 0.799 0.796 ## satisfaction =~ ## satisfd 1.000 0.778 0.770 ## content (.p5.) 1.069 0.051 21.078 0.000 0.832 0.754 ## cmfrtbl (.p6.) 0.919 0.044 21.033 0.000 0.715 0.757 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## satisfaction ~ ## posAffect 0.529 0.063 8.333 0.000 0.488 0.488 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.023 0.043 0.530 0.596 0.023 0.024 ## .happy 0.012 0.043 0.270 0.787 0.012 0.012 ## .cheerful 0.027 0.045 0.594 0.553 0.027 0.026 ## .satisfied -0.102 0.045 -2.269 0.023 -0.102 -0.101 ## .content -0.086 0.049 -1.754 0.079 -0.086 -0.078 ## .comfortable -0.061 0.041 -1.481 0.138 -0.061 -0.065 ## posAffect 0.000 0.000 0.000 ## .satisfaction 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.427 0.036 11.943 0.000 0.427 0.453 ## .happy 0.403 0.037 10.793 0.000 0.403 0.411 ## .cheerful 0.370 0.035 10.705 0.000 0.370 0.367 ## .satisfied 0.415 0.036 11.539 0.000 0.415 0.407 ## .content 0.525 0.044 11.903 0.000 0.525 0.431 ## .comfortable 0.381 0.035 10.927 0.000 0.381 0.427 ## posAffect 0.516 0.053 9.799 0.000 1.000 1.000 ## .satisfaction 0.461 0.051 9.082 0.000 0.762 0.762 ## ## ## Group 2 [public]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.673 0.675 ## happy (.p2.) 1.057 0.054 19.585 0.000 0.711 0.740 ## cheerfl (.p3.) 1.113 0.055 20.393 0.000 0.748 0.780 ## satisfaction =~ ## satisfd 1.000 0.763 0.761 ## content (.p5.) 1.069 0.051 21.078 0.000 0.816 0.771 ## cmfrtbl (.p6.) 0.919 0.044 21.033 0.000 0.701 0.734 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## satisfaction ~ ## posAffect 0.557 0.067 8.342 0.000 0.492 0.492 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad -0.022 0.045 -0.492 0.623 -0.022 -0.022 ## .happy 0.005 0.044 0.114 0.910 0.005 0.005 ## .cheerful -0.030 0.043 -0.697 0.486 -0.030 -0.031 ## .satisfied 0.018 0.045 0.399 0.690 0.018 0.018 ## .content 0.012 0.048 0.251 0.802 0.012 0.011 ## .comfortable -0.026 0.044 -0.597 0.551 -0.026 -0.027 ## posAffect 0.000 0.000 0.000 ## .satisfaction 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.541 0.042 12.899 0.000 0.541 0.545 ## .happy 0.418 0.040 10.351 0.000 0.418 0.452 ## .cheerful 0.360 0.038 9.399 0.000 0.360 0.392 ## .satisfied 0.423 0.040 10.551 0.000 0.423 0.421 ## .content 0.454 0.045 10.053 0.000 0.454 0.406 ## .comfortable 0.420 0.038 11.052 0.000 0.420 0.461 ## posAffect 0.453 0.049 9.180 0.000 1.000 1.000 ## .satisfaction 0.441 0.048 9.107 0.000 0.758 0.758 Again, metric invariance was established due to satisfying model fit; To test whether the equal factor loading assumption caused damage to model fit, we compare metricFit to configuralFit: Model comparison: anova(configuralFit, metricFit) ## Scaled Chi-Squared Difference Test (method = &quot;satorra.bentler.2001&quot;) ## ## lavaan NOTE: ## The &quot;Chisq&quot; column contains standard test statistics, not the ## robust test that should be reported per model. A robust difference ## test is a function of two standard (not robust) statistics. ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## configuralFit 16 15022 15208 11.92 ## metricFit 20 15020 15187 17.64 6.1624 4 0.1873 The test was not significant, meaning the increase in chi-square (due to the assumption of equal factor loadings) was not substantial enough to worsen the model fit; Note that this test suffers from the same problem as the chi-square test (too sensitive to model misfit) 17.2.3 step 3: Scalar (strong) Invariance In this step, both factor loadings and measurement intercepts (of course, including factor structure) are constrained to be equal between the groups: scalarFit &lt;- lavaan::sem(srSyntax, data = affectData, fixed.x=FALSE, estimator = &#39;MLR&#39;, group = &quot;school&quot;, group.equal = c(&quot;loadings&quot;, &quot;intercepts&quot;)) summary(scalarFit, standardized = T, fit.measures = T) ## lavaan 0.6-12 ended normally after 32 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 40 ## Number of equality constraints 10 ## ## Number of observations per group: ## private 513 ## public 487 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 20.364 20.360 ## Degrees of freedom 24 24 ## P-value (Chi-square) 0.676 0.676 ## Scaling correction factor 1.000 ## Yuan-Bentler correction (Mplus variant) ## Test statistic for each group: ## private 7.764 7.762 ## public 12.600 12.598 ## ## Model Test Baseline Model: ## ## Test statistic 2038.064 2039.295 ## Degrees of freedom 30 30 ## P-value 0.000 0.000 ## Scaling correction factor 0.999 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 1.000 ## Tucker-Lewis Index (TLI) 1.002 1.002 ## ## Robust Comparative Fit Index (CFI) 1.000 ## Robust Tucker-Lewis Index (TLI) 1.002 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -7477.224 -7477.224 ## Scaling correction factor 0.758 ## for the MLR correction ## Loglikelihood unrestricted model (H1) -7467.042 -7467.042 ## Scaling correction factor 1.006 ## for the MLR correction ## ## Akaike (AIC) 15014.448 15014.448 ## Bayesian (BIC) 15161.681 15161.681 ## Sample-size adjusted Bayesian (BIC) 15066.399 15066.399 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 0.000 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.029 0.029 ## P-value RMSEA &lt;= 0.05 0.999 0.999 ## ## Robust RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.029 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.020 0.020 ## ## Parameter Estimates: ## ## Standard errors Sandwich ## Information bread Observed ## Observed information based on Hessian ## ## ## Group 1 [private]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.718 0.740 ## happy (.p2.) 1.056 0.054 19.522 0.000 0.759 0.767 ## cheerfl (.p3.) 1.113 0.055 20.413 0.000 0.800 0.796 ## satisfaction =~ ## satisfd 1.000 0.780 0.771 ## content (.p5.) 1.068 0.051 21.059 0.000 0.832 0.754 ## cmfrtbl (.p6.) 0.915 0.043 21.140 0.000 0.713 0.755 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## satisfaction ~ ## posAffect 0.530 0.063 8.350 0.000 0.488 0.488 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad (.16.) 0.019 0.040 0.462 0.644 0.019 0.019 ## .happy (.17.) 0.026 0.040 0.644 0.519 0.026 0.026 ## .cheerfl (.18.) 0.018 0.042 0.415 0.678 0.018 0.017 ## .satisfd (.19.) -0.085 0.042 -2.031 0.042 -0.085 -0.084 ## .content (.20.) -0.082 0.045 -1.820 0.069 -0.082 -0.075 ## .cmfrtbl (.21.) -0.081 0.038 -2.108 0.035 -0.081 -0.086 ## psAffct 0.000 0.000 0.000 ## .stsfctn 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.427 0.036 11.937 0.000 0.427 0.453 ## .happy 0.403 0.037 10.809 0.000 0.403 0.412 ## .cheerful 0.370 0.035 10.680 0.000 0.370 0.367 ## .satisfied 0.414 0.036 11.505 0.000 0.414 0.405 ## .content 0.525 0.044 11.872 0.000 0.525 0.431 ## .comfortable 0.383 0.035 11.042 0.000 0.383 0.430 ## posAffect 0.516 0.053 9.801 0.000 1.000 1.000 ## .satisfaction 0.463 0.051 9.090 0.000 0.762 0.762 ## ## ## Group 2 [public]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.673 0.675 ## happy (.p2.) 1.056 0.054 19.522 0.000 0.710 0.739 ## cheerfl (.p3.) 1.113 0.055 20.413 0.000 0.749 0.780 ## satisfaction =~ ## satisfd 1.000 0.764 0.762 ## content (.p5.) 1.068 0.051 21.059 0.000 0.816 0.771 ## cmfrtbl (.p6.) 0.915 0.043 21.140 0.000 0.699 0.732 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## satisfaction ~ ## posAffect 0.558 0.067 8.350 0.000 0.492 0.492 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad (.16.) 0.019 0.040 0.462 0.644 0.019 0.019 ## .happy (.17.) 0.026 0.040 0.644 0.519 0.026 0.027 ## .cheerfl (.18.) 0.018 0.042 0.415 0.678 0.018 0.018 ## .satisfd (.19.) -0.085 0.042 -2.031 0.042 -0.085 -0.085 ## .content (.20.) -0.082 0.045 -1.820 0.069 -0.082 -0.078 ## .cmfrtbl (.21.) -0.081 0.038 -2.108 0.035 -0.081 -0.085 ## psAffct -0.035 0.049 -0.697 0.486 -0.051 -0.051 ## .stsfctn 0.104 0.051 2.053 0.040 0.137 0.137 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.541 0.042 12.891 0.000 0.541 0.545 ## .happy 0.419 0.041 10.332 0.000 0.419 0.453 ## .cheerful 0.360 0.038 9.378 0.000 0.360 0.391 ## .satisfied 0.422 0.040 10.516 0.000 0.422 0.420 ## .content 0.454 0.045 10.032 0.000 0.454 0.405 ## .comfortable 0.422 0.038 11.095 0.000 0.422 0.463 ## posAffect 0.453 0.049 9.174 0.000 1.000 1.000 ## .satisfaction 0.443 0.049 9.101 0.000 0.758 0.758 Model comparison: anova(metricFit, scalarFit) ## Scaled Chi-Squared Difference Test (method = &quot;satorra.bentler.2001&quot;) ## ## lavaan NOTE: ## The &quot;Chisq&quot; column contains standard test statistics, not the ## robust test that should be reported per model. A robust difference ## test is a function of two standard (not robust) statistics. ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## metricFit 20 15020 15187 17.640 ## scalarFit 24 15014 15162 20.364 2.7216 4 0.6054 The test was not significant, meaning the increase in chi-square (due to the assumption of equal measurement intercepts) was not substantial enough to worsen the model fit; Scalar invariance was established; 17.2.4 step 4: (Optional) Residual variance (strict) invariance resVarFit &lt;- lavaan::sem(srSyntax, data = affectData, fixed.x=FALSE, estimator = &#39;MLR&#39;, group = &quot;school&quot;, group.equal = c(&quot;loadings&quot;, &quot;intercepts&quot;, &quot;residuals&quot;)) summary(resVarFit, standardized = T, fit.measures = T) ## lavaan 0.6-12 ended normally after 32 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 40 ## Number of equality constraints 16 ## ## Number of observations per group: ## private 513 ## public 487 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 26.885 26.947 ## Degrees of freedom 30 30 ## P-value (Chi-square) 0.629 0.626 ## Scaling correction factor 0.998 ## Yuan-Bentler correction (Mplus variant) ## Test statistic for each group: ## private 11.454 11.481 ## public 15.430 15.466 ## ## Model Test Baseline Model: ## ## Test statistic 2038.064 2039.295 ## Degrees of freedom 30 30 ## P-value 0.000 0.000 ## Scaling correction factor 0.999 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 1.000 ## Tucker-Lewis Index (TLI) 1.002 1.002 ## ## Robust Comparative Fit Index (CFI) 1.000 ## Robust Tucker-Lewis Index (TLI) 1.002 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -7480.484 -7480.484 ## Scaling correction factor 0.610 ## for the MLR correction ## Loglikelihood unrestricted model (H1) -7467.042 -7467.042 ## Scaling correction factor 1.006 ## for the MLR correction ## ## Akaike (AIC) 15008.968 15008.968 ## Bayesian (BIC) 15126.754 15126.754 ## Sample-size adjusted Bayesian (BIC) 15050.529 15050.529 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 0.000 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.029 0.029 ## P-value RMSEA &lt;= 0.05 1.000 1.000 ## ## Robust RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.029 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.023 0.023 ## ## Parameter Estimates: ## ## Standard errors Sandwich ## Information bread Observed ## Observed information based on Hessian ## ## ## Group 1 [private]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.713 0.716 ## happy (.p2.) 1.063 0.054 19.648 0.000 0.758 0.765 ## cheerfl (.p3.) 1.114 0.055 20.246 0.000 0.794 0.794 ## satisfaction =~ ## satisfd 1.000 0.781 0.770 ## content (.p5.) 1.068 0.051 21.048 0.000 0.833 0.766 ## cmfrtbl (.p6.) 0.915 0.043 21.139 0.000 0.714 0.748 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## satisfaction ~ ## posAffect 0.537 0.064 8.445 0.000 0.491 0.491 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad (.16.) 0.018 0.040 0.443 0.658 0.018 0.018 ## .happy (.17.) 0.026 0.040 0.650 0.516 0.026 0.026 ## .cheerfl (.18.) 0.018 0.042 0.415 0.678 0.018 0.018 ## .satisfd (.19.) -0.085 0.042 -2.026 0.043 -0.085 -0.084 ## .content (.20.) -0.083 0.045 -1.825 0.068 -0.083 -0.076 ## .cmfrtbl (.21.) -0.082 0.038 -2.136 0.033 -0.082 -0.086 ## psAffct 0.000 0.000 0.000 ## .stsfctn 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad (.p8.) 0.483 0.028 17.136 0.000 0.483 0.487 ## .happy (.p9.) 0.408 0.029 14.229 0.000 0.408 0.415 ## .cheerfl (.10.) 0.369 0.028 13.197 0.000 0.369 0.369 ## .satisfd (.11.) 0.417 0.028 14.667 0.000 0.417 0.406 ## .content (.12.) 0.490 0.034 14.545 0.000 0.490 0.414 ## .cmfrtbl (.13.) 0.402 0.027 15.048 0.000 0.402 0.441 ## psAffct 0.509 0.051 9.913 0.000 1.000 1.000 ## .stsfctn 0.463 0.050 9.171 0.000 0.759 0.759 ## ## ## Group 2 [public]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.673 0.696 ## happy (.p2.) 1.063 0.054 19.648 0.000 0.716 0.746 ## cheerfl (.p3.) 1.114 0.055 20.246 0.000 0.750 0.777 ## satisfaction =~ ## satisfd 1.000 0.764 0.764 ## content (.p5.) 1.068 0.051 21.048 0.000 0.816 0.759 ## cmfrtbl (.p6.) 0.915 0.043 21.139 0.000 0.699 0.741 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## satisfaction ~ ## posAffect 0.556 0.067 8.327 0.000 0.490 0.490 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad (.16.) 0.018 0.040 0.443 0.658 0.018 0.018 ## .happy (.17.) 0.026 0.040 0.650 0.516 0.026 0.027 ## .cheerfl (.18.) 0.018 0.042 0.415 0.678 0.018 0.018 ## .satisfd (.19.) -0.085 0.042 -2.026 0.043 -0.085 -0.085 ## .content (.20.) -0.083 0.045 -1.825 0.068 -0.083 -0.077 ## .cmfrtbl (.21.) -0.082 0.038 -2.136 0.033 -0.082 -0.087 ## psAffct -0.034 0.049 -0.693 0.488 -0.051 -0.051 ## .stsfctn 0.104 0.051 2.049 0.040 0.136 0.136 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad (.p8.) 0.483 0.028 17.136 0.000 0.483 0.516 ## .happy (.p9.) 0.408 0.029 14.229 0.000 0.408 0.443 ## .cheerfl (.10.) 0.369 0.028 13.197 0.000 0.369 0.396 ## .satisfd (.11.) 0.417 0.028 14.667 0.000 0.417 0.417 ## .content (.12.) 0.490 0.034 14.545 0.000 0.490 0.424 ## .cmfrtbl (.13.) 0.402 0.027 15.048 0.000 0.402 0.451 ## psAffct 0.454 0.050 9.153 0.000 1.000 1.000 ## .stsfctn 0.444 0.049 9.141 0.000 0.760 0.760 Model comparison: anova(resVarFit, scalarFit) ## Scaled Chi-Squared Difference Test (method = &quot;satorra.bentler.2001&quot;) ## ## lavaan NOTE: ## The &quot;Chisq&quot; column contains standard test statistics, not the ## robust test that should be reported per model. A robust difference ## test is a function of two standard (not robust) statistics. ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## scalarFit 24 15014 15162 20.364 ## resVarFit 30 15009 15127 26.885 6.6022 6 0.3592 The test was not significant, meaning the increase in chi-square (due to the assumption of equal residual variances) was not substantial enough to worsen the model fit; Strict invariance was established. 17.3 PART III: Shortcut to performing MI 17.3.1 measurementInvariance() There is a shortcut function in package ‘semTools’ that performs invariance testing in one place, but unfortunately it will soon retire… library(semTools) measurementInvariance(model = srSyntax, data = affectData, fixed.x=FALSE, estimator = &#39;MLR&#39;, group = &quot;school&quot;) ## Warning: The measurementInvariance function is deprecated, and it will cease to be included in future versions of semTools. See help(&#39;semTools-deprecated) for details. ## ## Measurement invariance models: ## ## Model 1 : fit.configural ## Model 2 : fit.loadings ## Model 3 : fit.intercepts ## Model 4 : fit.means ## ## Scaled Chi-Squared Difference Test (method = &quot;satorra.bentler.2001&quot;) ## ## lavaan NOTE: ## The &quot;Chisq&quot; column contains standard test statistics, not the ## robust test that should be reported per model. A robust difference ## test is a function of two standard (not robust) statistics. ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## fit.configural 16 15022 15208 11.920 ## fit.loadings 20 15020 15187 17.640 6.1624 4 0.1873 ## fit.intercepts 24 15014 15162 20.364 2.7216 4 0.6054 ## fit.means 26 15015 15152 24.826 4.4961 2 0.1056 ## ## ## Fit measures: ## ## cfi.scaled rmsea.scaled cfi.scaled.delta rmsea.scaled.delta ## fit.configural 1 0 NA NA ## fit.loadings 1 0 0 0 ## fit.intercepts 1 0 0 0 ## fit.means 1 0 0 0 17.3.2 measEq.syntax() To use measEq.syntax() from semTools, we need to use a model syntax for CFA model instead of SR model: cfaSyntax &lt;- &quot; posAffect =~ glad + happy + cheerful satisfaction =~ satisfied + content + comfortable &quot; test.seq &lt;- list(weak = c(&quot;loadings&quot;), strong = c(&quot;intercepts&quot;), strict = c(&quot;residuals&quot;)) meq.list &lt;- list() for (i in 0:length(test.seq)) { if (i == 0L) { meq.label &lt;- &quot;configural&quot; group.equal &lt;- &quot;&quot; } else { meq.label &lt;- names(test.seq)[i] group.equal &lt;- unlist(test.seq[1:i]) } meq.list[[meq.label]] &lt;- measEq.syntax(configural.model = cfaSyntax, data = affectData, fixed.x = TRUE, estimator = &#39;MLR&#39;, group = &quot;school&quot;, group.equal = group.equal, return.fit = TRUE) } summary(compareFit(meq.list)) ## ################### Nested Model Comparison ######################### ## Scaled Chi-Squared Difference Test (method = &quot;satorra.bentler.2001&quot;) ## ## lavaan NOTE: ## The &quot;Chisq&quot; column contains standard test statistics, not the ## robust test that should be reported per model. A robust difference ## test is a function of two standard (not robust) statistics. ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## meq.list.configural 16 15022 15208 11.920 ## meq.list.weak 20 15020 15187 17.640 6.1624 4 0.1873 ## meq.list.strong 24 15014 15162 20.364 2.7216 4 0.6054 ## meq.list.strict 30 15009 15127 26.885 6.6022 6 0.3592 ## ## ####################### Model Fit Indices ########################### ## chisq.scaled df.scaled pvalue.scaled rmsea.robust cfi.robust tli.robust srmr aic bic ## meq.list.configural 11.709† 16 .764 .000† 1.000† 1.004† .013† 15022.004 15208.498 ## meq.list.weak 17.639 20 .611 .000† 1.000† 1.002 .019 15019.724 15186.588 ## meq.list.strong 20.360 24 .676 .000† 1.000† 1.002 .020 15014.448 15161.681 ## meq.list.strict 26.947 30 .626 .000† 1.000† 1.002 .023 15008.968† 15126.754† ## ## ################## Differences in Fit Indices ####################### ## df.scaled rmsea.robust cfi.robust tli.robust srmr aic bic ## meq.list.weak - meq.list.configural 4 0 0 -0.002 0.006 -2.280 -21.911 ## meq.list.strong - meq.list.weak 4 0 0 0.001 0.001 -5.276 -24.907 ## meq.list.strict - meq.list.strong 6 0 0 -0.001 0.003 -5.480 -34.926 According to Putnick &amp; Bornstein (2016): Some researchers have shifted from a focus on absolute fit in terms of \\(\\chi^2\\) to a focus on alternative fit indices because \\(\\chi^2\\) is overly sensitive to small, unimportant deviations from a “perfect” model in large samples (Chen, 2007; Cheung &amp; Rensvold, 2002; French &amp; Finch, 2006; Meade, Johnson, &amp; Braddy, 2008). Cheung and Rensvold’s (2002) criterion of a -.01 change in CFI for nested models is commonly used, but other researchers have suggested different criteria for CFI (Meade et al., 2008; Rutkowski &amp; Svetina, 2014). This means in the last section of “Differences in Fit Indices”, a positive ‘cfi.robust’ or a negative ‘cfi.robust’&gt;= -.01 helps establish measurement noninvariance. 17.4 PART IV: Multi-Group CFA Modeling, done right To compare the structural model parameters, at least scalar (strong) invariance is required; Since strict invariance was also satisfied, we will use resVarFit for MG-SR Modeling in this example: 17.4.1 Statistical Test of Equal Factor Means: equalMeanfit &lt;- lavaan::sem(cfaSyntax, affectData, fixed.x = FALSE, estimator = &#39;MLR&#39;, group = &quot;school&quot;, group.equal = c(&quot;loadings&quot;, &quot;intercepts&quot;, &quot;residuals&quot;, &quot;means&quot;)) summary(equalMeanfit, standardized = T, fit.measures = T) ## lavaan 0.6-12 ended normally after 28 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 38 ## Number of equality constraints 16 ## ## Number of observations per group: ## private 513 ## public 487 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 31.333 31.415 ## Degrees of freedom 32 32 ## P-value (Chi-square) 0.500 0.496 ## Scaling correction factor 0.997 ## Yuan-Bentler correction (Mplus variant) ## Test statistic for each group: ## private 13.747 13.784 ## public 17.585 17.632 ## ## Model Test Baseline Model: ## ## Test statistic 2038.064 2039.295 ## Degrees of freedom 30 30 ## P-value 0.000 0.000 ## Scaling correction factor 0.999 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 1.000 ## Tucker-Lewis Index (TLI) 1.000 1.000 ## ## Robust Comparative Fit Index (CFI) 1.000 ## Robust Tucker-Lewis Index (TLI) 1.000 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -7482.708 -7482.708 ## Scaling correction factor 0.590 ## for the MLR correction ## Loglikelihood unrestricted model (H1) -7467.042 -7467.042 ## Scaling correction factor 1.006 ## for the MLR correction ## ## Akaike (AIC) 15009.416 15009.416 ## Bayesian (BIC) 15117.387 15117.387 ## Sample-size adjusted Bayesian (BIC) 15047.514 15047.514 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 0.000 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.032 0.032 ## P-value RMSEA &lt;= 0.05 1.000 1.000 ## ## Robust RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.032 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.028 0.028 ## ## Parameter Estimates: ## ## Standard errors Sandwich ## Information bread Observed ## Observed information based on Hessian ## ## ## Group 1 [private]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.713 0.716 ## happy (.p2.) 1.064 0.054 19.659 0.000 0.759 0.766 ## cheerfl (.p3.) 1.113 0.055 20.234 0.000 0.794 0.794 ## satisfaction =~ ## satisfd 1.000 0.781 0.770 ## content (.p5.) 1.069 0.051 21.128 0.000 0.834 0.766 ## cmfrtbl (.p6.) 0.917 0.043 21.128 0.000 0.716 0.749 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect ~~ ## satisfaction 0.272 0.038 7.166 0.000 0.489 0.489 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad (.16.) 0.000 0.031 0.007 0.994 0.000 0.000 ## .happy (.17.) 0.007 0.031 0.241 0.810 0.007 0.008 ## .cheerfl (.18.) -0.002 0.031 -0.065 0.948 -0.002 -0.002 ## .satisfd (.19.) -0.043 0.032 -1.360 0.174 -0.043 -0.043 ## .content (.20.) -0.038 0.034 -1.111 0.267 -0.038 -0.035 ## .cmfrtbl (.21.) -0.044 0.030 -1.459 0.144 -0.044 -0.046 ## psAffct 0.000 0.000 0.000 ## stsfctn 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad (.p7.) 0.484 0.028 17.125 0.000 0.484 0.487 ## .happy (.p8.) 0.407 0.029 14.219 0.000 0.407 0.414 ## .cheerfl (.p9.) 0.369 0.028 13.207 0.000 0.369 0.369 ## .satisfd (.10.) 0.419 0.028 14.741 0.000 0.419 0.407 ## .content (.11.) 0.491 0.034 14.588 0.000 0.491 0.413 ## .cmfrtbl (.12.) 0.401 0.027 14.918 0.000 0.401 0.439 ## psAffct 0.509 0.051 9.894 0.000 1.000 1.000 ## stsfctn 0.609 0.064 9.549 0.000 1.000 1.000 ## ## ## Group 2 [public]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.674 0.696 ## happy (.p2.) 1.064 0.054 19.659 0.000 0.717 0.747 ## cheerfl (.p3.) 1.113 0.055 20.234 0.000 0.750 0.777 ## satisfaction =~ ## satisfd 1.000 0.765 0.763 ## content (.p5.) 1.069 0.051 21.128 0.000 0.817 0.759 ## cmfrtbl (.p6.) 0.917 0.043 21.128 0.000 0.701 0.742 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect ~~ ## satisfaction 0.251 0.035 7.161 0.000 0.488 0.488 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad (.16.) 0.000 0.031 0.007 0.994 0.000 0.000 ## .happy (.17.) 0.007 0.031 0.241 0.810 0.007 0.008 ## .cheerfl (.18.) -0.002 0.031 -0.065 0.948 -0.002 -0.002 ## .satisfd (.19.) -0.043 0.032 -1.360 0.174 -0.043 -0.043 ## .content (.20.) -0.038 0.034 -1.111 0.267 -0.038 -0.035 ## .cmfrtbl (.21.) -0.044 0.030 -1.459 0.144 -0.044 -0.046 ## psAffct 0.000 0.000 0.000 ## stsfctn 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad (.p7.) 0.484 0.028 17.125 0.000 0.484 0.516 ## .happy (.p8.) 0.407 0.029 14.219 0.000 0.407 0.442 ## .cheerfl (.p9.) 0.369 0.028 13.207 0.000 0.369 0.396 ## .satisfd (.10.) 0.419 0.028 14.741 0.000 0.419 0.417 ## .content (.11.) 0.491 0.034 14.588 0.000 0.491 0.424 ## .cmfrtbl (.12.) 0.401 0.027 14.918 0.000 0.401 0.449 ## psAffct 0.454 0.050 9.164 0.000 1.000 1.000 ## stsfctn 0.585 0.057 10.290 0.000 1.000 1.000 anova(resVarFit, equalMeanfit) ## Scaled Chi-Squared Difference Test (method = &quot;satorra.bentler.2001&quot;) ## ## lavaan NOTE: ## The &quot;Chisq&quot; column contains standard test statistics, not the ## robust test that should be reported per model. A robust difference ## test is a function of two standard (not robust) statistics. ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## resVarFit 30 15009 15127 26.885 ## equalMeanfit 32 15009 15117 31.333 4.4815 2 0.1064 The anova test was not significant, meaning the increase in chi-square (due to the constraint of equal latent means) was not substantial enough to worsen the model fit; It says the levels of positive affect and satisfaction in public schools were essentially the same as those in private schools. 17.4.2 Statistical Test of Equal Regression Coefficients: Note that we used srSyntax here because we need to define the regression coefficient between PA and satisfaction: equalBetafit &lt;- lavaan::sem(srSyntax, affectData, fixed.x = FALSE, estimator = &#39;MLR&#39;, group = &quot;school&quot;, group.equal = c(&quot;loadings&quot;, &quot;intercepts&quot;, &quot;residuals&quot;, &quot;regressions&quot;)) summary(equalBetafit, standardized = T, fit.measures = T) ## lavaan 0.6-12 ended normally after 30 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 40 ## Number of equality constraints 17 ## ## Number of observations per group: ## private 513 ## public 487 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 26.937 26.888 ## Degrees of freedom 31 31 ## P-value (Chi-square) 0.675 0.678 ## Scaling correction factor 1.002 ## Yuan-Bentler correction (Mplus variant) ## Test statistic for each group: ## private 11.496 11.476 ## public 15.441 15.413 ## ## Model Test Baseline Model: ## ## Test statistic 2038.064 2039.295 ## Degrees of freedom 30 30 ## P-value 0.000 0.000 ## Scaling correction factor 0.999 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 1.000 ## Tucker-Lewis Index (TLI) 1.002 1.002 ## ## Robust Comparative Fit Index (CFI) 1.000 ## Robust Tucker-Lewis Index (TLI) 1.002 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -7480.510 -7480.510 ## Scaling correction factor 0.582 ## for the MLR correction ## Loglikelihood unrestricted model (H1) -7467.042 -7467.042 ## Scaling correction factor 1.006 ## for the MLR correction ## ## Akaike (AIC) 15007.021 15007.021 ## Bayesian (BIC) 15119.899 15119.899 ## Sample-size adjusted Bayesian (BIC) 15046.850 15046.850 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 0.000 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.027 0.027 ## P-value RMSEA &lt;= 0.05 1.000 1.000 ## ## Robust RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.027 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.024 0.024 ## ## Parameter Estimates: ## ## Standard errors Sandwich ## Information bread Observed ## Observed information based on Hessian ## ## ## Group 1 [private]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.712 0.716 ## happy (.p2.) 1.064 0.054 19.668 0.000 0.758 0.765 ## cheerfl (.p3.) 1.114 0.055 20.253 0.000 0.794 0.794 ## satisfaction =~ ## satisfd 1.000 0.783 0.771 ## content (.p5.) 1.067 0.051 21.045 0.000 0.836 0.767 ## cmfrtbl (.p6.) 0.915 0.043 21.157 0.000 0.716 0.749 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## satisfaction ~ ## psAffct (.p7.) 0.546 0.048 11.266 0.000 0.497 0.497 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad (.16.) 0.018 0.040 0.443 0.658 0.018 0.018 ## .happy (.17.) 0.026 0.040 0.650 0.516 0.026 0.026 ## .cheerfl (.18.) 0.018 0.042 0.415 0.678 0.018 0.018 ## .satisfd (.19.) -0.085 0.042 -2.026 0.043 -0.085 -0.084 ## .content (.20.) -0.083 0.045 -1.825 0.068 -0.083 -0.076 ## .cmfrtbl (.21.) -0.082 0.038 -2.137 0.033 -0.082 -0.086 ## psAffct 0.000 0.000 0.000 ## .stsfctn 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad (.p8.) 0.483 0.028 17.135 0.000 0.483 0.488 ## .happy (.p9.) 0.407 0.029 14.260 0.000 0.407 0.415 ## .cheerfl (.10.) 0.369 0.028 13.259 0.000 0.369 0.370 ## .satisfd (.11.) 0.417 0.028 14.675 0.000 0.417 0.405 ## .content (.12.) 0.490 0.034 14.542 0.000 0.490 0.412 ## .cmfrtbl (.13.) 0.402 0.027 15.067 0.000 0.402 0.439 ## psAffct 0.508 0.051 9.911 0.000 1.000 1.000 ## .stsfctn 0.462 0.050 9.171 0.000 0.753 0.753 ## ## ## Group 2 [public]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.674 0.696 ## happy (.p2.) 1.064 0.054 19.668 0.000 0.717 0.747 ## cheerfl (.p3.) 1.114 0.055 20.253 0.000 0.751 0.777 ## satisfaction =~ ## satisfd 1.000 0.762 0.763 ## content (.p5.) 1.067 0.051 21.045 0.000 0.813 0.758 ## cmfrtbl (.p6.) 0.915 0.043 21.157 0.000 0.697 0.740 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## satisfaction ~ ## psAffct (.p7.) 0.546 0.048 11.266 0.000 0.483 0.483 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad (.16.) 0.018 0.040 0.443 0.658 0.018 0.018 ## .happy (.17.) 0.026 0.040 0.650 0.516 0.026 0.027 ## .cheerfl (.18.) 0.018 0.042 0.415 0.678 0.018 0.018 ## .satisfd (.19.) -0.085 0.042 -2.026 0.043 -0.085 -0.085 ## .content (.20.) -0.083 0.045 -1.825 0.068 -0.083 -0.077 ## .cmfrtbl (.21.) -0.082 0.038 -2.137 0.033 -0.082 -0.087 ## psAffct -0.034 0.049 -0.693 0.488 -0.051 -0.051 ## .stsfctn 0.104 0.051 2.046 0.041 0.136 0.136 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad (.p8.) 0.483 0.028 17.135 0.000 0.483 0.515 ## .happy (.p9.) 0.407 0.029 14.260 0.000 0.407 0.442 ## .cheerfl (.10.) 0.369 0.028 13.259 0.000 0.369 0.396 ## .satisfd (.11.) 0.417 0.028 14.675 0.000 0.417 0.418 ## .content (.12.) 0.490 0.034 14.542 0.000 0.490 0.426 ## .cmfrtbl (.13.) 0.402 0.027 15.067 0.000 0.402 0.453 ## psAffct 0.454 0.049 9.228 0.000 1.000 1.000 ## .stsfctn 0.445 0.048 9.293 0.000 0.767 0.767 anova(resVarFit, equalBetafit) ## Scaled Chi-Squared Difference Test (method = &quot;satorra.bentler.2001&quot;) ## ## lavaan NOTE: ## The &quot;Chisq&quot; column contains standard test statistics, not the ## robust test that should be reported per model. A robust difference ## test is a function of two standard (not robust) statistics. ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## resVarFit 30 15009 15127 26.885 ## equalBetafit 31 15007 15120 26.937 0.046657 1 0.829 The test was not significant, meaning the increase in chi-square (due to the constraint of equal regression coefficients) was not substantial enough to worsen the model fit; It says the effect of positive affect on satisfaction in public schools was essentially the same as that in private schools. "],["lavaan-lab-15-mimic-longitudinal-invariance.html", "Chapter 18 Lavaan Lab 15: MIMIC &amp; Longitudinal Invariance 18.1 PART I: Partial Invariance 18.2 PART II: MIMIC 18.3 PART III: Longitudinal Invariance 18.4 PART IV: Exercises: MIMIC 18.5 PART V: Exercises: Longitudinal Invariance", " Chapter 18 Lavaan Lab 15: MIMIC &amp; Longitudinal Invariance For this lab, we will run Partial Invariance Test and MIMIC Models using simulated data based on Todd Little’s positive affect example. We will also test longitudinal measurement invariance using a longitudinal dataset from semTools Load up the lavaan library: library(lavaan) and the dataset: affectData &lt;- read.csv(&quot;cfaInclassData.csv&quot;, header = T) For demonstration purposes, let’s first simulate a grouping variable called school: set.seed(555) affectData$school = sample(c(&#39;public&#39;, &#39;private&#39;), nrow(affectData), replace = T) 18.1 PART I: Partial Invariance Suppose that you do not need: the loading of content on satisfaction the intercept of content to be equal across groups, you can use group.partial= to relax them: “satisfaction=~content”: factor loading of content on satisfaction “content~1”: intercept of indicator content srSyntax &lt;- &quot; posAffect =~ glad + happy + cheerful satisfaction =~ satisfied + content + comfortable # Structural Regression: beta satisfaction ~ posAffect &quot; PartialInvFit &lt;- lavaan::sem(srSyntax, data = affectData, fixed.x=FALSE, estimator = &#39;MLR&#39;, group = &quot;school&quot;, group.equal = c(&quot;loadings&quot;, &quot;intercepts&quot;, &quot;residuals&quot;), group.partial = c(&quot;satisfaction=~content&quot;, &quot;content~1&quot;)) summary(PartialInvFit, standardized = T, fit.measures = T) ## lavaan 0.6-12 ended normally after 33 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 40 ## Number of equality constraints 14 ## ## Number of observations per group: ## private 513 ## public 487 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 25.622 25.620 ## Degrees of freedom 28 28 ## P-value (Chi-square) 0.594 0.594 ## Scaling correction factor 1.000 ## Yuan-Bentler correction (Mplus variant) ## Test statistic for each group: ## private 10.914 10.913 ## public 14.708 14.707 ## ## Model Test Baseline Model: ## ## Test statistic 2038.064 2039.295 ## Degrees of freedom 30 30 ## P-value 0.000 0.000 ## Scaling correction factor 0.999 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 1.000 ## Tucker-Lewis Index (TLI) 1.001 1.001 ## ## Robust Comparative Fit Index (CFI) 1.000 ## Robust Tucker-Lewis Index (TLI) 1.001 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -7479.853 -7479.853 ## Scaling correction factor 0.658 ## for the MLR correction ## Loglikelihood unrestricted model (H1) -7467.042 -7467.042 ## Scaling correction factor 1.006 ## for the MLR correction ## ## Akaike (AIC) 15011.706 15011.706 ## Bayesian (BIC) 15139.308 15139.308 ## Sample-size adjusted Bayesian (BIC) 15056.730 15056.730 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 0.000 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.031 0.031 ## P-value RMSEA &lt;= 0.05 1.000 1.000 ## ## Robust RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.031 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.021 0.021 ## ## Parameter Estimates: ## ## Standard errors Sandwich ## Information bread Observed ## Observed information based on Hessian ## ## ## Group 1 [private]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.713 0.716 ## happy (.p2.) 1.063 0.054 19.642 0.000 0.758 0.765 ## cheerfl (.p3.) 1.114 0.055 20.243 0.000 0.794 0.794 ## satisfaction =~ ## satisfd 1.000 0.769 0.765 ## content 1.111 0.068 16.370 0.000 0.855 0.774 ## cmfrtbl (.p6.) 0.917 0.044 20.985 0.000 0.705 0.744 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## satisfaction ~ ## posAffect 0.529 0.064 8.286 0.000 0.491 0.491 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad (.16.) 0.018 0.040 0.443 0.658 0.018 0.018 ## .happy (.17.) 0.026 0.040 0.650 0.516 0.026 0.026 ## .cheerfl (.18.) 0.018 0.042 0.415 0.678 0.018 0.018 ## .satisfd (.19.) -0.083 0.043 -1.949 0.051 -0.083 -0.083 ## .content -0.086 0.049 -1.754 0.079 -0.086 -0.078 ## .cmfrtbl (.21.) -0.080 0.039 -2.058 0.040 -0.080 -0.085 ## psAffct 0.000 0.000 0.000 ## .stsfctn 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad (.p8.) 0.483 0.028 17.129 0.000 0.483 0.487 ## .happy (.p9.) 0.408 0.029 14.225 0.000 0.408 0.415 ## .cheerfl (.10.) 0.369 0.028 13.205 0.000 0.369 0.369 ## .satisfd (.11.) 0.418 0.029 14.671 0.000 0.418 0.414 ## .content (.12.) 0.489 0.034 14.487 0.000 0.489 0.401 ## .cmfrtbl (.13.) 0.401 0.027 14.951 0.000 0.401 0.446 ## psAffct 0.509 0.051 9.913 0.000 1.000 1.000 ## .stsfctn 0.449 0.052 8.611 0.000 0.759 0.759 ## ## ## Group 2 [public]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.674 0.696 ## happy (.p2.) 1.063 0.054 19.642 0.000 0.716 0.746 ## cheerfl (.p3.) 1.114 0.055 20.243 0.000 0.750 0.777 ## satisfaction =~ ## satisfd 1.000 0.775 0.768 ## content 1.024 0.059 17.424 0.000 0.793 0.750 ## cmfrtbl (.p6.) 0.917 0.044 20.985 0.000 0.710 0.746 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## satisfaction ~ ## posAffect 0.563 0.067 8.357 0.000 0.489 0.489 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad (.16.) 0.018 0.040 0.443 0.658 0.018 0.018 ## .happy (.17.) 0.026 0.040 0.650 0.516 0.026 0.027 ## .cheerfl (.18.) 0.018 0.042 0.415 0.678 0.018 0.018 ## .satisfd (.19.) -0.083 0.043 -1.949 0.051 -0.083 -0.082 ## .content -0.072 0.056 -1.276 0.202 -0.072 -0.068 ## .cmfrtbl (.21.) -0.080 0.039 -2.058 0.040 -0.080 -0.085 ## psAffct -0.034 0.049 -0.693 0.488 -0.051 -0.051 ## .stsfctn 0.101 0.054 1.862 0.063 0.130 0.130 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad (.p8.) 0.483 0.028 17.129 0.000 0.483 0.516 ## .happy (.p9.) 0.408 0.029 14.225 0.000 0.408 0.443 ## .cheerfl (.10.) 0.369 0.028 13.205 0.000 0.369 0.396 ## .satisfd (.11.) 0.418 0.029 14.671 0.000 0.418 0.411 ## .content (.12.) 0.489 0.034 14.487 0.000 0.489 0.437 ## .cmfrtbl (.13.) 0.401 0.027 14.951 0.000 0.401 0.443 ## psAffct 0.454 0.050 9.153 0.000 1.000 1.000 ## .stsfctn 0.456 0.050 9.162 0.000 0.760 0.760 The overall model seems to be fine, so we can safely assume these two parameters can be freed across group; Technically you want to compare PartialInvFit to resVarFit from last lab 18.2 PART II: MIMIC To test whether the grouping variable school affects the loadings (i.e., metric invariance), school has to first interact with PA and predict the indicators: This is easily said than done. To create such an interaction, we first need to create indicators of the latent interaction by multiplying school with each of the indicators of PA: # first convert public/private to 0/1 affectData$school_N = ifelse(affectData$school==&#39;public&#39;, 0, 1) affectData$intPA1 = affectData$school_N * affectData$glad affectData$intPA2 = affectData$school_N * affectData$happy affectData$intPA3 = affectData$school_N * affectData$cheerful 18.2.1 Test Metric Invariance Now that we have our latent interaction indicators ready, we can run our MIMIC analyses by testing: srSyntaxMIMIC0 &lt;- &quot; posAffect =~ glad + happy + cheerful satisfaction =~ satisfied + content + comfortable schoolxPA =~ intPA1 + intPA2 + intPA3 # Structural Regression: beta satisfaction ~ posAffect # Correlated Residuals: intPA1 ~~ glad intPA2 ~~ happy intPA3 ~~ cheerful &quot; MIMICmodel &lt;- lavaan::sem(srSyntaxMIMIC0, data = affectData, fixed.x=FALSE, estimator = &#39;MLR&#39;) library(semPlot) semPaths(MIMICmodel, what=&#39;est&#39;, nCharNodes = 0, nCharEdges = 0, # don&#39;t limit variable name lengths edge.label.cex=0.6, curvePivot = TRUE, curve = 1.5, # pull covariances&#39; curves out a little fade=FALSE) srSyntaxMIMICLoading &lt;- &quot; posAffect =~ glad + happy + cheerful satisfaction =~ satisfied + content + comfortable schoolxPA =~ intPA1 + intPA2 + intPA3 # Structural Regression: beta satisfaction ~ posAffect # Correlated Residuals: intPA1 ~~ glad intPA2 ~~ happy intPA3 ~~ cheerful # Test Metric Invariance glad ~ school + schoolxPA happy ~ school + schoolxPA cheerful ~ school + schoolxPA &quot; Note that you don’t need group=, group.equal=, or group.partial= in the following function (why?): MIMICloading &lt;- lavaan::sem(srSyntaxMIMICLoading, data = affectData, fixed.x=FALSE, estimator = &#39;MLR&#39;) ## Warning in lav_model_vcov(lavmodel = lavmodel, lavsamplestats = lavsamplestats, : lavaan WARNING: ## The variance-covariance matrix of the estimated parameters (vcov) ## does not appear to be positive definite! The smallest eigenvalue ## (= 3.538313e-14) is close to zero. This may be a symptom that the ## model is not identified. summary(MIMICloading, standardized = T, fit.measures = T) ## lavaan 0.6-12 ended normally after 52 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 30 ## ## Number of observations 1000 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 18.095 20.392 ## Degrees of freedom 25 25 ## P-value (Chi-square) 0.838 0.726 ## Scaling correction factor 0.887 ## Yuan-Bentler correction (Mplus variant) ## ## Model Test Baseline Model: ## ## Test statistic 5166.260 4938.018 ## Degrees of freedom 45 45 ## P-value 0.000 0.000 ## Scaling correction factor 1.046 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 1.000 ## Tucker-Lewis Index (TLI) 1.002 1.002 ## ## Robust Comparative Fit Index (CFI) 1.000 ## Robust Tucker-Lewis Index (TLI) 1.001 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -9863.278 -9863.278 ## Scaling correction factor 1.267 ## for the MLR correction ## Loglikelihood unrestricted model (H1) NA NA ## Scaling correction factor 1.094 ## for the MLR correction ## ## Akaike (AIC) 19786.556 19786.556 ## Bayesian (BIC) 19933.789 19933.789 ## Sample-size adjusted Bayesian (BIC) 19838.507 19838.507 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 0.000 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.015 0.020 ## P-value RMSEA &lt;= 0.05 1.000 1.000 ## ## Robust RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.018 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.014 0.014 ## ## Parameter Estimates: ## ## Standard errors Sandwich ## Information bread Observed ## Observed information based on Hessian ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.644 0.655 ## happy 1.198 0.093 12.844 0.000 0.772 0.791 ## cheerful 1.130 0.091 12.420 0.000 0.728 0.740 ## satisfaction =~ ## satisfied 1.000 0.773 0.767 ## content 1.068 0.051 21.114 0.000 0.826 0.763 ## comfortable 0.917 0.043 21.150 0.000 0.709 0.746 ## schoolxPA =~ ## intPA1 1.000 0.531 0.755 ## intPA2 0.968 0.065 14.944 0.000 0.514 0.739 ## intPA3 1.104 0.068 16.157 0.000 0.587 0.809 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## satisfaction ~ ## posAffect 0.583 0.073 7.942 0.000 0.485 0.485 ## glad ~ ## school -0.038 0.043 -0.869 0.385 -0.038 -0.019 ## schoolxPA 0.129 0.147 0.874 0.382 0.068 0.070 ## happy ~ ## school -0.014 0.043 -0.325 0.745 -0.014 -0.007 ## schoolxPA -0.081 0.177 -0.457 0.648 -0.043 -0.044 ## cheerful ~ ## school -0.048 0.042 -1.150 0.250 -0.048 -0.024 ## schoolxPA 0.118 0.173 0.681 0.496 0.063 0.064 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad ~~ ## .intPA1 0.212 0.020 10.526 0.000 0.212 0.662 ## .happy ~~ ## .intPA2 0.219 0.021 10.685 0.000 0.219 0.738 ## .cheerful ~~ ## .intPA3 0.181 0.020 9.130 0.000 0.181 0.703 ## posAffect ~~ ## schoolxPA 0.246 0.046 5.349 0.000 0.720 0.720 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.482 0.028 17.094 0.000 0.482 0.499 ## .happy 0.401 0.029 14.007 0.000 0.401 0.422 ## .cheerful 0.368 0.027 13.445 0.000 0.368 0.380 ## .satisfied 0.418 0.028 14.724 0.000 0.418 0.412 ## .content 0.491 0.034 14.594 0.000 0.491 0.418 ## .comfortable 0.401 0.027 14.960 0.000 0.401 0.444 ## .intPA1 0.213 0.020 10.602 0.000 0.213 0.430 ## .intPA2 0.219 0.021 10.598 0.000 0.219 0.454 ## .intPA3 0.181 0.020 9.136 0.000 0.181 0.345 ## posAffect 0.415 0.085 4.888 0.000 1.000 1.000 ## .satisfaction 0.457 0.039 11.610 0.000 0.764 0.764 ## schoolxPA 0.282 0.032 8.881 0.000 1.000 1.000 ## school 0.250 0.000 607.925 0.000 0.250 1.000 semPaths(MIMICloading, what=&#39;est&#39;, nCharNodes = 0, nCharEdges = 0, # don&#39;t limit variable name lengths edge.label.cex=0.6, curvePivot = TRUE, curve = 1.5, # pull covariances&#39; curves out a little fade=FALSE) Regressions: Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all satisfaction ~ posAffect 0.583 0.073 7.942 0.000 0.485 0.485 glad ~ school -0.038 0.043 -0.869 0.385 -0.038 -0.019 schoolxPA 0.129 0.147 0.874 0.382 0.068 0.070 happy ~ school -0.014 0.043 -0.325 0.745 -0.014 -0.007 schoolxPA -0.081 0.177 -0.457 0.648 -0.043 -0.044 cheerful ~ school -0.048 0.042 -1.150 0.250 -0.048 -0.024 schoolxPA 0.118 0.173 0.681 0.496 0.063 0.064 The coefficient of schoolxPA on all indicators were insignificant. The loadings do not depend on school type. No sign of violation of metric invariance. 18.2.2 Test Scalar Invariance Since metric invariance has been established, we do not need the indicators of the latent interactions, we simply predict each of the indicators using group: srSyntaxMIMICInt &lt;- &quot; posAffect =~ glad + happy + cheerful satisfaction =~ satisfied + content + comfortable # Structural Regression: beta satisfaction ~ posAffect # Test Scalar Invariance glad ~ school happy ~ school cheerful ~ school &quot; MIMICintercept &lt;- lavaan::sem(srSyntaxMIMICInt, data = affectData, fixed.x=FALSE, estimator = &#39;MLR&#39;) semPaths(MIMICintercept, what=&#39;est&#39;, nCharNodes = 0, nCharEdges = 0, # don&#39;t limit variable name lengths edge.label.cex=0.6, curvePivot = TRUE, curve = 1.5, # pull covariances&#39; curves out a little fade=FALSE) summary(MIMICintercept, standardized = T, fit.measures = T) ## lavaan 0.6-12 ended normally after 24 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 17 ## ## Number of observations 1000 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 7.345 7.230 ## Degrees of freedom 11 11 ## P-value (Chi-square) 0.770 0.780 ## Scaling correction factor 1.016 ## Yuan-Bentler correction (Mplus variant) ## ## Model Test Baseline Model: ## ## Test statistic 2027.246 2025.669 ## Degrees of freedom 21 21 ## P-value 0.000 0.000 ## Scaling correction factor 1.001 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 1.000 ## Tucker-Lewis Index (TLI) 1.003 1.004 ## ## Robust Comparative Fit Index (CFI) 1.000 ## Robust Tucker-Lewis Index (TLI) 1.004 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -8207.301 -8207.301 ## Scaling correction factor 0.942 ## for the MLR correction ## Loglikelihood unrestricted model (H1) NA NA ## Scaling correction factor 0.971 ## for the MLR correction ## ## Akaike (AIC) 16448.602 16448.602 ## Bayesian (BIC) 16532.034 16532.034 ## Sample-size adjusted Bayesian (BIC) 16478.041 16478.041 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 0.000 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.023 0.022 ## P-value RMSEA &lt;= 0.05 1.000 1.000 ## ## Robust RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.023 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.017 0.017 ## ## Parameter Estimates: ## ## Standard errors Sandwich ## Information bread Observed ## Observed information based on Hessian ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.694 0.706 ## happy 1.067 0.054 19.752 0.000 0.740 0.758 ## cheerful 1.112 0.055 20.188 0.000 0.772 0.784 ## satisfaction =~ ## satisfied 1.000 0.773 0.767 ## content 1.068 0.051 21.122 0.000 0.826 0.763 ## comfortable 0.917 0.043 21.166 0.000 0.709 0.746 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## satisfaction ~ ## posAffect 0.547 0.049 11.250 0.000 0.491 0.491 ## glad ~ ## school -0.075 0.059 -1.268 0.205 -0.075 -0.038 ## happy ~ ## school -0.039 0.058 -0.664 0.507 -0.039 -0.020 ## cheerful ~ ## school -0.090 0.058 -1.542 0.123 -0.090 -0.046 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.484 0.028 17.143 0.000 0.484 0.501 ## .happy 0.405 0.028 14.280 0.000 0.405 0.425 ## .cheerful 0.371 0.028 13.437 0.000 0.371 0.383 ## .satisfied 0.418 0.028 14.740 0.000 0.418 0.412 ## .content 0.491 0.034 14.615 0.000 0.491 0.419 ## .comfortable 0.401 0.027 14.993 0.000 0.401 0.444 ## posAffect 0.481 0.042 11.547 0.000 1.000 1.000 ## .satisfaction 0.454 0.039 11.629 0.000 0.759 0.759 ## school 0.250 0.000 607.925 0.000 0.250 1.000 The coefficient of school on all indicators were insignificant. The intercepts of indicators do not depend on school type. No sign of violation of scalar invariance. 18.2.3 Test The Hypothesis of Equal Factor Means: cfaSyntaxMIMIC &lt;- &quot; posAffect =~ glad + happy + cheerful satisfaction =~ satisfied + content + comfortable # Test Equal Factor Means posAffect ~ school satisfaction ~ school &quot; MIMICmean &lt;- lavaan::sem(cfaSyntaxMIMIC, data = affectData, fixed.x=FALSE, estimator = &#39;MLR&#39;) summary(MIMICmean, standardized = T, fit.measures = T) ## lavaan 0.6-12 ended normally after 26 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 16 ## ## Number of observations 1000 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 5.758 5.670 ## Degrees of freedom 12 12 ## P-value (Chi-square) 0.928 0.932 ## Scaling correction factor 1.015 ## Yuan-Bentler correction (Mplus variant) ## ## Model Test Baseline Model: ## ## Test statistic 2027.246 2025.669 ## Degrees of freedom 21 21 ## P-value 0.000 0.000 ## Scaling correction factor 1.001 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 1.000 ## Tucker-Lewis Index (TLI) 1.005 1.006 ## ## Robust Comparative Fit Index (CFI) 1.000 ## Robust Tucker-Lewis Index (TLI) 1.006 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -8206.507 -8206.507 ## Scaling correction factor 0.938 ## for the MLR correction ## Loglikelihood unrestricted model (H1) NA NA ## Scaling correction factor 0.971 ## for the MLR correction ## ## Akaike (AIC) 16445.014 16445.014 ## Bayesian (BIC) 16523.538 16523.538 ## Sample-size adjusted Bayesian (BIC) 16472.722 16472.722 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 0.000 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.010 0.009 ## P-value RMSEA &lt;= 0.05 1.000 1.000 ## ## Robust RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.009 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.009 0.009 ## ## Parameter Estimates: ## ## Standard errors Sandwich ## Information bread Observed ## Observed information based on Hessian ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect =~ ## glad 1.000 0.694 0.706 ## happy 1.066 0.054 19.687 0.000 0.740 0.758 ## cheerful 1.113 0.055 20.209 0.000 0.772 0.785 ## satisfaction =~ ## satisfied 1.000 0.774 0.768 ## content 1.067 0.051 21.069 0.000 0.826 0.763 ## comfortable 0.915 0.043 21.175 0.000 0.708 0.745 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## posAffect ~ ## school -0.034 0.049 -0.691 0.489 -0.049 -0.025 ## satisfaction ~ ## school 0.085 0.055 1.559 0.119 0.110 0.055 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .posAffect ~~ ## .satisfaction 0.263 0.028 9.514 0.000 0.490 0.490 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .glad 0.484 0.028 17.137 0.000 0.484 0.501 ## .happy 0.406 0.028 14.268 0.000 0.406 0.426 ## .cheerful 0.370 0.028 13.396 0.000 0.370 0.383 ## .satisfied 0.417 0.028 14.689 0.000 0.417 0.411 ## .content 0.491 0.034 14.582 0.000 0.491 0.418 ## .comfortable 0.402 0.027 15.076 0.000 0.402 0.445 ## .posAffect 0.482 0.042 11.546 0.000 0.999 0.999 ## .satisfaction 0.597 0.049 12.212 0.000 0.997 0.997 ## school 0.250 0.000 607.925 0.000 0.250 1.000 semPaths(MIMICmean, what=&#39;est&#39;, nCharNodes = 0, nCharEdges = 0, # don&#39;t limit variable name lengths edge.label.cex=0.6, curvePivot = TRUE, curve = 1.5, # pull covariances&#39; curves out a little fade=FALSE) The coefficient of school on two latent variables were insignificant. It says the levels of positive affect and satisfaction in public schools were essentially the same as those in private schools. 18.3 PART III: Longitudinal Invariance The following codes were adapted from the examples of measEq.syntax() in semTools: library(semTools) ?measEq.syntax They used a built-in dataset called datCat: head(datCat) ## u1 u2 u3 u4 u5 u6 u7 u8 g ## 1 3 3 2 2 3 2 2 3 male ## 2 3 2 2 2 2 3 3 2 male ## 3 3 2 3 1 4 3 3 3 male ## 4 1 3 2 1 3 2 2 3 male ## 5 3 4 3 4 4 4 5 3 male ## 6 3 4 3 3 4 3 2 1 male A data.frame with 200 observations of 9 variables. A simulated data set with 2 factors with 4 indicators each separated into two groups Let’s ignore the gender groups for now u1-u4 are likert variables measured at time 1 u5-u8 are the same set of likert variables measured at time 2 Both u1-u4 and u5-u8 measure the same latent variable, FU The goal of testing longitudinal invariance is to make sure that the scale that measures positive affect and satisfaction functions in the same way across all time points. 18.3.1 step 1: Configural invariance First define the CFA model that measures the same latent variable (FU) at two time points: mod.cat &lt;- &#39; FU1 =~ u1 + u2 + u3 + u4 FU2 =~ u5 + u6 + u7 + u8 &#39; It’s important to know: You do not want to use sem() to test longitudinal invariance (technically you can, but it’ll be very messy) I recommend using the function measEq.syntax() from semTools package To tell measEq.syntax() that FU1 are FU2 are just the same variables, you need to define a list longFacNames that includes this information The indicators are categorical you’ll need the ordered= argument and parameterization = “theta” The example codes used ID.fac = “std.lv” (fixed variance scaling) so we’ll use this as well return.fit = TRUE fits the model instead of just creating a model syntax ## the 2 factors are actually the same factor (FU) measured twice longFacNames &lt;- list(FU = c(&quot;FU1&quot;,&quot;FU2&quot;)) syntax.config &lt;- measEq.syntax(configural.model = mod.cat, data = datCat, ordered = paste0(&quot;u&quot;, 1:8), parameterization = &quot;theta&quot;, ID.fac = &quot;std.lv&quot;, longFacNames = longFacNames, fixed.x = TRUE, return.fit = TRUE) #cat(as.character(syntax.config)) summary(syntax.config, standardized = T, fit.measures = T) ## lavaan 0.6-12 ended normally after 43 iterations ## ## Estimator DWLS ## Optimization method NLMINB ## Number of model parameters 45 ## ## Number of observations 200 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 11.839 19.695 ## Degrees of freedom 15 15 ## P-value (Chi-square) 0.691 0.184 ## Scaling correction factor 0.665 ## Shift parameter 1.891 ## simple second-order correction ## ## Model Test Baseline Model: ## ## Test statistic 1493.667 953.386 ## Degrees of freedom 28 28 ## P-value 0.000 0.000 ## Scaling correction factor 1.584 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 0.995 ## Tucker-Lewis Index (TLI) 1.004 0.991 ## ## Robust Comparative Fit Index (CFI) NA ## Robust Tucker-Lewis Index (TLI) NA ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 0.040 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.053 0.083 ## P-value RMSEA &lt;= 0.05 0.938 0.603 ## ## Robust RMSEA NA ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.038 0.038 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Unstructured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## FU1 =~ ## u1 (l.1_) 1.264 0.181 6.972 0.000 1.264 0.784 ## u2 (l.2_) 0.941 0.142 6.638 0.000 0.941 0.685 ## u3 (l.3_) 1.053 0.141 7.472 0.000 1.053 0.725 ## u4 (l.4_) 0.833 0.113 7.383 0.000 0.833 0.640 ## FU2 =~ ## u5 (l.5_) 1.286 0.200 6.446 0.000 1.286 0.790 ## u6 (l.6_) 1.254 0.183 6.847 0.000 1.254 0.782 ## u7 (l.7_) 1.142 0.167 6.822 0.000 1.142 0.752 ## u8 (l.8_) 0.919 0.119 7.746 0.000 0.919 0.677 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .u1 ~~ ## .u5 (t.5_) 0.144 0.140 1.029 0.303 0.144 0.144 ## .u2 ~~ ## .u6 (t.6_) -0.023 0.127 -0.185 0.853 -0.023 -0.023 ## .u3 ~~ ## .u7 (t.7_) 0.059 0.128 0.460 0.646 0.059 0.059 ## .u4 ~~ ## .u8 (t.8_) -0.078 0.113 -0.689 0.491 -0.078 -0.078 ## FU1 ~~ ## FU2 (p.2_) 0.550 0.062 8.813 0.000 0.550 0.550 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .u1 (nu.1) 0.000 0.000 0.000 ## .u2 (nu.2) 0.000 0.000 0.000 ## .u3 (nu.3) 0.000 0.000 0.000 ## .u4 (nu.4) 0.000 0.000 0.000 ## .u5 (nu.5) 0.000 0.000 0.000 ## .u6 (nu.6) 0.000 0.000 0.000 ## .u7 (nu.7) 0.000 0.000 0.000 ## .u8 (nu.8) 0.000 0.000 0.000 ## FU1 (al.1) 0.000 0.000 0.000 ## FU2 (al.2) 0.000 0.000 0.000 ## ## Thresholds: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## u1|t1 (u1.1) -2.065 0.230 -8.995 0.000 -2.065 -1.282 ## u1|t2 (u1.2) -0.471 0.147 -3.210 0.001 -0.471 -0.292 ## u1|t3 (u1.3) 1.087 0.175 6.214 0.000 1.087 0.674 ## u1|t4 (u1.4) 2.506 0.277 9.046 0.000 2.506 1.555 ## u2|t1 (u2.1) -1.977 0.202 -9.766 0.000 -1.977 -1.440 ## u2|t2 (u2.2) -0.419 0.125 -3.349 0.001 -0.419 -0.305 ## u2|t3 (u2.3) 0.948 0.146 6.501 0.000 0.948 0.690 ## u2|t4 (u2.4) 2.404 0.265 9.060 0.000 2.404 1.751 ## u3|t1 (u3.1) -1.992 0.208 -9.592 0.000 -1.992 -1.372 ## u3|t2 (u3.2) -0.579 0.135 -4.287 0.000 -0.579 -0.399 ## u3|t3 (u3.3) 1.049 0.156 6.745 0.000 1.049 0.722 ## u3|t4 (u3.4) 2.462 0.267 9.227 0.000 2.462 1.695 ## u4|t1 (u4.1) -1.786 0.177 -10.073 0.000 -1.786 -1.372 ## u4|t2 (u4.2) -0.296 0.116 -2.546 0.011 -0.296 -0.228 ## u4|t3 (u4.3) 0.940 0.132 7.123 0.000 0.940 0.722 ## u4|t4 (u4.4) 2.551 0.262 9.719 0.000 2.551 1.960 ## u5|t1 (u5.1) -2.289 0.272 -8.431 0.000 -2.289 -1.405 ## u5|t2 (u5.2) -0.831 0.166 -5.017 0.000 -0.831 -0.510 ## u5|t3 (u5.3) 0.739 0.157 4.717 0.000 0.739 0.454 ## u5|t4 (u5.4) 2.135 0.279 7.667 0.000 2.135 1.311 ## u6|t1 (u6.1) -2.367 0.261 -9.077 0.000 -2.367 -1.476 ## u6|t2 (u6.2) -0.683 0.156 -4.385 0.000 -0.683 -0.426 ## u6|t3 (u6.3) 0.983 0.166 5.930 0.000 0.983 0.613 ## u6|t4 (u6.4) 2.150 0.259 8.317 0.000 2.150 1.341 ## u7|t1 (u7.1) -2.299 0.267 -8.601 0.000 -2.299 -1.514 ## u7|t2 (u7.2) -0.796 0.149 -5.327 0.000 -0.796 -0.524 ## u7|t3 (u7.3) 0.626 0.143 4.375 0.000 0.626 0.412 ## u7|t4 (u7.4) 1.746 0.200 8.734 0.000 1.746 1.150 ## u8|t1 (u8.1) -1.909 0.178 -10.721 0.000 -1.909 -1.405 ## u8|t2 (u8.2) -0.712 0.130 -5.469 0.000 -0.712 -0.524 ## u8|t3 (u8.3) 0.451 0.124 3.635 0.000 0.451 0.332 ## u8|t4 (u8.4) 1.909 0.184 10.387 0.000 1.909 1.405 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .u1 (t.1_) 1.000 1.000 0.385 ## .u2 (t.2_) 1.000 1.000 0.530 ## .u3 (t.3_) 1.000 1.000 0.474 ## .u4 (t.4_) 1.000 1.000 0.590 ## .u5 (t.5_) 1.000 1.000 0.377 ## .u6 (t.6_) 1.000 1.000 0.389 ## .u7 (t.7_) 1.000 1.000 0.434 ## .u8 (t.8_) 1.000 1.000 0.542 ## FU1 (p.1_) 1.000 1.000 1.000 ## FU2 (p.2_) 1.000 1.000 1.000 ## ## Scales y*: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## u1 0.620 0.620 1.000 ## u2 0.728 0.728 1.000 ## u3 0.689 0.689 1.000 ## u4 0.768 0.768 1.000 ## u5 0.614 0.614 1.000 ## u6 0.624 0.624 1.000 ## u7 0.659 0.659 1.000 ## u8 0.736 0.736 1.000 18.3.2 step 1.5: Threshold invariance (for categorical indicators only) The test of Threshold invariance has to happen before the test of all other parameters; Note that we do not have to use group = argument, longFacNames does the job Use long.equal = c(“thresholds”) to test Threshold invariance syntax.thresh &lt;- measEq.syntax(configural.model = mod.cat, data = datCat, ordered = paste0(&quot;u&quot;, 1:8), parameterization = &quot;theta&quot;, ID.fac = &quot;std.lv&quot;, longFacNames = longFacNames, long.equal = c(&quot;thresholds&quot;), fixed.x = TRUE, return.fit = TRUE) summary(syntax.thresh, standardized = T, fit.measures = T) ## lavaan 0.6-12 ended normally after 58 iterations ## ## Estimator DWLS ## Optimization method NLMINB ## Number of model parameters 53 ## Number of equality constraints 16 ## ## Number of observations 200 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 14.104 24.688 ## Degrees of freedom 23 23 ## P-value (Chi-square) 0.924 0.367 ## Scaling correction factor 0.647 ## Shift parameter 2.878 ## simple second-order correction ## ## Model Test Baseline Model: ## ## Test statistic 1493.667 953.386 ## Degrees of freedom 28 28 ## P-value 0.000 0.000 ## Scaling correction factor 1.584 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 0.998 ## Tucker-Lewis Index (TLI) 1.007 0.998 ## ## Robust Comparative Fit Index (CFI) NA ## Robust Tucker-Lewis Index (TLI) NA ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 0.019 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.020 0.063 ## P-value RMSEA &lt;= 0.05 0.996 0.850 ## ## Robust RMSEA NA ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.038 0.038 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Unstructured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## FU1 =~ ## u1 (l.1_) 1.264 0.181 6.972 0.000 1.264 0.784 ## u2 (l.2_) 0.941 0.142 6.638 0.000 0.941 0.685 ## u3 (l.3_) 1.053 0.141 7.472 0.000 1.053 0.725 ## u4 (l.4_) 0.833 0.113 7.383 0.000 0.833 0.640 ## FU2 =~ ## u5 (l.5_) 1.320 0.179 7.377 0.000 1.320 0.790 ## u6 (l.6_) 1.169 0.141 8.271 0.000 1.169 0.782 ## u7 (l.7_) 1.266 0.157 8.056 0.000 1.266 0.752 ## u8 (l.8_) 1.037 0.140 7.420 0.000 1.037 0.677 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .u1 ~~ ## .u5 (t.5_) 0.147 0.145 1.018 0.309 0.147 0.144 ## .u2 ~~ ## .u6 (t.6_) -0.022 0.118 -0.185 0.853 -0.022 -0.023 ## .u3 ~~ ## .u7 (t.7_) 0.065 0.143 0.459 0.647 0.065 0.059 ## .u4 ~~ ## .u8 (t.8_) -0.088 0.128 -0.685 0.493 -0.088 -0.078 ## FU1 ~~ ## FU2 (p.2_) 0.550 0.062 8.813 0.000 0.550 0.550 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .u1 (nu.1) 0.000 0.000 0.000 ## .u2 (nu.2) 0.000 0.000 0.000 ## .u3 (nu.3) 0.000 0.000 0.000 ## .u4 (nu.4) 0.000 0.000 0.000 ## .u5 (nu.5) 0.336 0.144 2.338 0.019 0.336 0.201 ## .u6 (nu.6) 0.186 0.132 1.411 0.158 0.186 0.124 ## .u7 (nu.7) 0.399 0.144 2.771 0.006 0.399 0.237 ## .u8 (nu.8) 0.443 0.140 3.154 0.002 0.443 0.289 ## FU1 (al.1) 0.000 0.000 0.000 ## FU2 (al.2) 0.000 0.000 0.000 ## ## Thresholds: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## u1|t1 (u1.1) -2.042 0.224 -9.132 0.000 -2.042 -1.267 ## u1|t2 (u1.2) -0.492 0.132 -3.730 0.000 -0.492 -0.306 ## u1|t3 (u1.3) 1.091 0.162 6.740 0.000 1.091 0.677 ## u1|t4 (u1.4) 2.518 0.270 9.335 0.000 2.518 1.562 ## u2|t1 (u2.1) -1.996 0.193 -10.327 0.000 -1.996 -1.454 ## u2|t2 (u2.2) -0.434 0.115 -3.771 0.000 -0.434 -0.316 ## u2|t3 (u2.3) 1.020 0.144 7.090 0.000 1.020 0.743 ## u2|t4 (u2.4) 2.280 0.237 9.620 0.000 2.280 1.660 ## u3|t1 (u3.1) -2.053 0.198 -10.357 0.000 -2.053 -1.414 ## u3|t2 (u3.2) -0.539 0.131 -4.129 0.000 -0.539 -0.371 ## u3|t3 (u3.3) 1.069 0.148 7.236 0.000 1.069 0.736 ## u3|t4 (u3.4) 2.388 0.240 9.940 0.000 2.388 1.644 ## u4|t1 (u4.1) -1.754 0.167 -10.516 0.000 -1.754 -1.348 ## u4|t2 (u4.2) -0.322 0.108 -2.969 0.003 -0.322 -0.247 ## u4|t3 (u4.3) 0.945 0.124 7.600 0.000 0.945 0.726 ## u4|t4 (u4.4) 2.577 0.231 11.167 0.000 2.577 1.980 ## u5|t1 (u1.1) -2.042 0.224 -9.132 0.000 -2.042 -1.221 ## u5|t2 (u1.2) -0.492 0.132 -3.730 0.000 -0.492 -0.295 ## u5|t3 (u1.3) 1.091 0.162 6.740 0.000 1.091 0.652 ## u5|t4 (u1.4) 2.518 0.270 9.335 0.000 2.518 1.506 ## u6|t1 (u2.1) -1.996 0.193 -10.327 0.000 -1.996 -1.335 ## u6|t2 (u2.2) -0.434 0.115 -3.771 0.000 -0.434 -0.290 ## u6|t3 (u2.3) 1.020 0.144 7.090 0.000 1.020 0.682 ## u6|t4 (u2.4) 2.280 0.237 9.620 0.000 2.280 1.524 ## u7|t1 (u3.1) -2.053 0.198 -10.357 0.000 -2.053 -1.220 ## u7|t2 (u3.2) -0.539 0.131 -4.129 0.000 -0.539 -0.321 ## u7|t3 (u3.3) 1.069 0.148 7.236 0.000 1.069 0.635 ## u7|t4 (u3.4) 2.388 0.240 9.940 0.000 2.388 1.419 ## u8|t1 (u4.1) -1.754 0.167 -10.516 0.000 -1.754 -1.146 ## u8|t2 (u4.2) -0.322 0.108 -2.969 0.003 -0.322 -0.210 ## u8|t3 (u4.3) 0.945 0.124 7.600 0.000 0.945 0.617 ## u8|t4 (u4.4) 2.577 0.231 11.167 0.000 2.577 1.683 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .u1 (t.1_) 1.000 1.000 0.385 ## .u2 (t.2_) 1.000 1.000 0.530 ## .u3 (t.3_) 1.000 1.000 0.474 ## .u4 (t.4_) 1.000 1.000 0.590 ## .u5 (t.5_) 1.053 0.274 3.845 0.000 1.053 0.377 ## .u6 (t.6_) 0.870 0.213 4.088 0.000 0.870 0.389 ## .u7 (t.7_) 1.228 0.294 4.181 0.000 1.228 0.434 ## .u8 (t.8_) 1.271 0.248 5.135 0.000 1.271 0.542 ## FU1 (p.1_) 1.000 1.000 1.000 ## FU2 (p.2_) 1.000 1.000 1.000 ## ## Scales y*: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## u1 0.620 0.620 1.000 ## u2 0.728 0.728 1.000 ## u3 0.689 0.689 1.000 ## u4 0.768 0.768 1.000 ## u5 0.598 0.598 1.000 ## u6 0.669 0.669 1.000 ## u7 0.594 0.594 1.000 ## u8 0.653 0.653 1.000 compare their fit to test threshold invariance: anova(syntax.config, syntax.thresh) ## Scaled Chi-Squared Difference Test (method = &quot;satorra.2000&quot;) ## ## lavaan NOTE: ## The &quot;Chisq&quot; column contains standard test statistics, not the ## robust test that should be reported per model. A robust difference ## test is a function of two standard (not robust) statistics. ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## syntax.config 15 11.839 ## syntax.thresh 23 14.104 4.4393 8 0.8155 The test was not significant, meaning the increase in chi-square (due to the assumption of equal thresholds) was not substantial enough to worsen the model fit; Threshold invariance was established; 18.3.3 RECOMMENDED PRACTICE: fit one invariance model at a time A downside of setting return.fit=TRUE is that if the model has trouble converging, you don’t have the opportunity to investigate the syntax, or even to know whether an error resulted from the syntax-generator or from lavaan itself. A downside of automatically fitting an entire set of invariance models (like the old measurementInvariance() function did) is that you might end up testing models that shouldn’t even be fitted because less restrictive models already fail (e.g., don’t test full scalar invariance if metric invariance fails! Establish partial metric invariance first, then test equivalent of intercepts ONLY among the indicators that have invariate loadings.) The recommended sequence is to generate and save each syntax object (i.e., return = FALSE), print it to the screen to verify you are fitting the model you expect to (and potentially learn which identification constraints should be released when equality constraints are imposed), and fit that model to the data, as you would if you had written the syntax yourself. Continuing from the examples above, after establishing invariance of thresholds, we proceed to test equivalence of loadings and intercepts (metric and scalar invariance, respectively) simultaneously across groups and repeated measures. 18.3.4 step 2: Metric (weak) invariance syntax.metric &lt;- measEq.syntax(configural.model = mod.cat, data = datCat, ordered = paste0(&quot;u&quot;, 1:8), parameterization = &quot;theta&quot;, ID.fac = &quot;std.lv&quot;, longFacNames = longFacNames, long.equal = c(&quot;thresholds&quot;,&quot;loadings&quot;), fixed.x = TRUE, return.fit = TRUE) summary(syntax.metric, standardized = T, fit.measures = T) # summarize model features test equivalence of loadings, given equivalence of thresholds: anova(syntax.thresh, syntax.metric) ## Scaled Chi-Squared Difference Test (method = &quot;satorra.2000&quot;) ## ## lavaan NOTE: ## The &quot;Chisq&quot; column contains standard test statistics, not the ## robust test that should be reported per model. A robust difference ## test is a function of two standard (not robust) statistics. ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## syntax.thresh 23 14.104 ## syntax.metric 26 15.526 1.3632 3 0.7142 The test was not significant, meaning the increase in chi-square (due to the assumption of equal loadings) was not substantial enough to worsen the model fit; Metric invariance was established; 18.3.5 step 3: Scalar (strong) Invariance syntax.scalar &lt;- measEq.syntax(configural.model = mod.cat, data = datCat, ordered = paste0(&quot;u&quot;, 1:8), parameterization = &quot;theta&quot;, ID.fac = &quot;std.lv&quot;, longFacNames = longFacNames, long.equal = c(&quot;thresholds&quot;,&quot;loadings&quot;,&quot;intercepts&quot;), fixed.x = TRUE, return.fit = TRUE) summary(syntax.scalar, standardized = T, fit.measures = T) # summarize model features test equivalence of intercepts, given equal thresholds &amp; loadings: anova(syntax.metric, syntax.scalar) ## Scaled Chi-Squared Difference Test (method = &quot;satorra.2000&quot;) ## ## lavaan NOTE: ## The &quot;Chisq&quot; column contains standard test statistics, not the ## robust test that should be reported per model. A robust difference ## test is a function of two standard (not robust) statistics. ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## syntax.metric 26 15.526 ## syntax.scalar 29 19.338 4.0529 3 0.2558 The test was not significant, meaning the increase in chi-square (due to the assumption of equal intercepts) was not substantial enough to worsen the model fit; Scalar invariance was established; 18.3.6 step 4: Residual variance (strict) invariance syntax.strict &lt;- measEq.syntax(configural.model = mod.cat, data = datCat, ordered = paste0(&quot;u&quot;, 1:8), parameterization = &quot;theta&quot;, ID.fac = &quot;std.lv&quot;, longFacNames = longFacNames, long.equal = c(&quot;thresholds&quot;,&quot;loadings&quot;,&quot;intercepts&quot;, &quot;residuals&quot;), fixed.x = TRUE, return.fit = TRUE) summary(syntax.strict, standardized = T, fit.measures = T) # summarize model features test equivalence of intercepts, given equal thresholds &amp; loadings: anova(syntax.scalar, syntax.strict) ## Scaled Chi-Squared Difference Test (method = &quot;satorra.2000&quot;) ## ## lavaan NOTE: ## The &quot;Chisq&quot; column contains standard test statistics, not the ## robust test that should be reported per model. A robust difference ## test is a function of two standard (not robust) statistics. ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## syntax.scalar 29 19.338 ## syntax.strict 33 21.448 2.5846 4 0.6296 The test was not significant, meaning the increase in chi-square (due to the assumption of equal residual variances) was not substantial enough to worsen the model fit; Strict invariance was established; 18.3.7 Shortcut Function For a single table with all results, you can pass the models to summarize to the compareFit() function: summary(compareFit(syntax.config, syntax.thresh, syntax.metric, syntax.scalar, syntax.strict)) ## ################### Nested Model Comparison ######################### ## Scaled Chi-Squared Difference Test (method = &quot;satorra.2000&quot;) ## ## lavaan NOTE: ## The &quot;Chisq&quot; column contains standard test statistics, not the ## robust test that should be reported per model. A robust difference ## test is a function of two standard (not robust) statistics. ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## syntax.config 15 11.839 ## syntax.thresh 23 14.104 4.4393 8 0.8155 ## syntax.metric 26 15.526 1.3632 3 0.7142 ## syntax.scalar 29 19.338 4.0529 3 0.2558 ## syntax.strict 33 21.448 2.5846 4 0.6296 ## ## ####################### Model Fit Indices ########################### ## chisq.scaled df.scaled pvalue.scaled rmsea.scaled cfi.scaled tli.scaled srmr ## syntax.config 19.695† 15 .184 .040 .995 .991 .038† ## syntax.thresh 24.688 23 .367 .019 0.998 0.998 .038 ## syntax.metric 25.098 26 .513 .000† 1.000† 1.001† .038 ## syntax.scalar 29.459 29 .441 .009 1.000 1.000 .039 ## syntax.strict 31.984 33 .518 .000† 1.000† 1.001 .038 ## ## ################## Differences in Fit Indices ####################### ## df.scaled rmsea.scaled cfi.scaled tli.scaled srmr ## syntax.thresh - syntax.config 8 -0.020 0.003 0.007 0.000 ## syntax.metric - syntax.thresh 3 -0.019 0.002 0.003 0.001 ## syntax.scalar - syntax.metric 3 0.009 0.000 -0.002 0.000 ## syntax.strict - syntax.scalar 4 -0.009 0.000 0.001 0.000 18.3.8 NOT RECOMMENDED: fit several invariance models at once Must SIMULTANEOUSLY constrain thresholds, loadings, and intercepts”: test.seq &lt;- list(strong = c(&quot;thresholds&quot;, &quot;loadings&quot;,&quot;intercepts&quot;), strict = c(&quot;residuals&quot;)) meq.list &lt;- list() for (i in 0:length(test.seq)) { if (i == 0L) { meq.label &lt;- &quot;configural&quot; long.equal &lt;- &quot;&quot; } else { meq.label &lt;- names(test.seq)[i] long.equal &lt;- unlist(test.seq[1:i]) } meq.list[[meq.label]] &lt;- measEq.syntax(configural.model = mod.cat, data = datCat, ordered = paste0(&quot;u&quot;, 1:8), parameterization = &quot;theta&quot;, ID.fac = &quot;std.lv&quot;, longFacNames = longFacNames, long.equal = long.equal, fixed.x = TRUE, return.fit = TRUE) } compareFit(meq.list) ## The following lavaan models were compared: ## meq.list.configural ## meq.list.strong ## meq.list.strict ## To view results, assign the compareFit() output to an object and use the summary() method; see the class?FitDiff help page. summary(compareFit(meq.list)) ## ################### Nested Model Comparison ######################### ## Scaled Chi-Squared Difference Test (method = &quot;satorra.2000&quot;) ## ## lavaan NOTE: ## The &quot;Chisq&quot; column contains standard test statistics, not the ## robust test that should be reported per model. A robust difference ## test is a function of two standard (not robust) statistics. ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## meq.list.configural 15 11.839 ## meq.list.strong 29 19.338 10.5780 14 0.7188 ## meq.list.strict 33 21.448 2.5846 4 0.6296 ## ## ####################### Model Fit Indices ########################### ## chisq.scaled df.scaled pvalue.scaled rmsea.scaled cfi.scaled tli.scaled srmr ## meq.list.configural 19.695† 15 .184 .040 .995 .991 .038† ## meq.list.strong 29.459 29 .441 .009 1.000 1.000 .039 ## meq.list.strict 31.984 33 .518 .000† 1.000† 1.001† .038 ## ## ################## Differences in Fit Indices ####################### ## df.scaled rmsea.scaled cfi.scaled tli.scaled srmr ## meq.list.strong - meq.list.configural 14 -0.031 0.005 0.009 0.001 ## meq.list.strict - meq.list.strong 4 -0.009 0.000 0.001 0.000 18.4 PART IV: Exercises: MIMIC In this exercise, you are given a dataset, activefull.txt, to fit the MIMIC model on page 70 of &lt;Week13 MGSEM + Measurement Invariance.pdf&gt;: I’ll get you started: active&lt;-read.table(&#39;activefull.txt&#39;, header=T) V&lt;-c(&#39;ws1&#39;,&#39;ls1&#39;,&#39;lt1&#39;,&#39;gender&#39;) active_sub&lt;-active[,V] head(active_sub) ## ws1 ls1 lt1 gender ## 1 4 6 7 2 ## 2 11 12 7 2 ## 3 7 11 5 2 ## 4 16 15 9 2 ## 5 9 7 5 2 ## 6 20 20 8 2 This subscale measures the latent variable R using three continuous indicators: ‘ws1’,‘ls1’,‘lt1’ You can ignore the mediator edu for now. Using active_sub, can you test the (1) Metric Invariance and (2) Scalar Invariance of this subscale between gender groups? 18.5 PART V: Exercises: Longitudinal Invariance In this exercise, you are given a dataset, myData, that can be downloaded from Mplus website: myData &lt;- read.table(&quot;http://www.statmodel.com/usersguide/chap5/ex5.16.dat&quot;) names(myData) &lt;- c(&quot;u1&quot;,&quot;u2&quot;,&quot;u3&quot;,&quot;u4&quot;,&quot;u5&quot;,&quot;u6&quot;,&quot;x1&quot;,&quot;x2&quot;,&quot;x3&quot;,&quot;g&quot;) myData_sub&lt;-myData[,c(&quot;u1&quot;,&quot;u2&quot;,&quot;u3&quot;,&quot;u4&quot;,&quot;u5&quot;,&quot;u6&quot;)] head(myData_sub) ## u1 u2 u3 u4 u5 u6 ## 1 0 0 0 0 1 1 ## 2 1 1 1 1 1 1 ## 3 1 0 0 1 0 0 ## 4 0 0 0 0 0 0 ## 5 1 1 1 0 0 0 ## 6 0 0 1 0 0 0 myData_sub is a data.frame with 2200 observations of 6 variables. u1-u3 are binary variables measured at time 1 u4-u6 are the same set of binary variables measured at time 2 Both u1-u3 and u4-u6 measure the same latent variable, FU Let’s first define the CFA model that measures the same latent variable (FU) at two time points (you are welcome): bin.mod &lt;- &#39; FU1 =~ u1 + u2 + u3 FU2 =~ u4 + u5 + u6 &#39; Using myData_sub, can you test the (1) Metric Invariance and (2) Scalar Invariance of this subscale between gender groups? test.seq &lt;- list(strong = c(&quot;thresholds&quot;, &quot;loadings&quot;,&quot;intercepts&quot;), strict = c(&quot;residuals&quot;)) meq.list &lt;- list() for (i in 0:length(test.seq)) { if (i == 0L) { meq.label &lt;- &quot;configural&quot; long.equal &lt;- &quot;&quot; } else { meq.label &lt;- names(test.seq)[i] long.equal &lt;- unlist(test.seq[1:i]) } meq.list[[meq.label]] &lt;- measEq.syntax(configural.model = bin.mod, data = myData_sub, ordered = paste0(&quot;u&quot;, 1:6), parameterization = &quot;theta&quot;, ID.fac = &quot;std.lv&quot;, longFacNames = longFacNames, long.equal = long.equal, fixed.x = TRUE, return.fit = TRUE) } compareFit(meq.list) ## The following lavaan models were compared: ## meq.list.configural ## meq.list.strong ## meq.list.strict ## To view results, assign the compareFit() output to an object and use the summary() method; see the class?FitDiff help page. summary(compareFit(meq.list)) ## ################### Nested Model Comparison ######################### ## Scaled Chi-Squared Difference Test (method = &quot;satorra.2000&quot;) ## ## lavaan NOTE: ## The &quot;Chisq&quot; column contains standard test statistics, not the ## robust test that should be reported per model. A robust difference ## test is a function of two standard (not robust) statistics. ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## meq.list.configural 5 0.8964 ## meq.list.strong 6 1.5166 1.785 1 0.1815 ## meq.list.strict 9 46.2898 61.483 3 2.833e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ####################### Model Fit Indices ########################### ## chisq.scaled df.scaled pvalue.scaled rmsea.scaled cfi.scaled tli.scaled srmr ## meq.list.configural 1.937† 5 .858 .000† 1.000† 1.001† .006 ## meq.list.strong 3.229 6 .780 .000† 1.000† 1.000 .006† ## meq.list.strict 73.013 9 .000 .057 0.996 .993 .016 ## ## ################## Differences in Fit Indices ####################### ## df.scaled rmsea.scaled cfi.scaled tli.scaled srmr ## meq.list.strong - meq.list.configural 1 0.000 0.000 0.000 0.000 ## meq.list.strict - meq.list.strong 3 0.057 -0.004 -0.008 0.011 "],["lavaan-lab-16-latent-growth-models.html", "Chapter 19 Lavaan Lab 16: Latent Growth Models 19.1 PART I: Spaghetti Plot 19.2 PART II: Growth Models 19.3 PART III: LGM on Latent Variables", " Chapter 19 Lavaan Lab 16: Latent Growth Models In this lab, we will: run and interpret a series of growth models (no growth, linear, quadratic, latent basis, spline growth); compare nested models and identify the best possible shape for characterizing the growth patterns; add predictors for the growth factors; run growth models on latent variables. Load up the lavaan library: library(lavaan) We will also need ggplot2, semPlot, and semTools. Install them if you haven’t: #install.packages(&quot;ggplot2&quot;) #install.packages(&quot;semPlot&quot;) #install.packages(&quot;semTools&quot;) library(ggplot2) library(semPlot) library(semTools) For this lab, we will work with a simulated dataset # based on an example from McCoach &amp; Kaniskan (2010). The main DV is Oral Reading Fluency (ORF) and is measured over 4 time points (Fall and Spring, 2 consecutive years) N = 277 Elementary students. Let’s read in the dataset: orf &lt;- read.csv(&quot;readingSimData.csv&quot;, header = T) Take a look at the dataset: head(orf) ## id treatmentDummy orf1 orf2 orf3 orf4 ## 1 1 0 133.93092 157.81992 178.74260 151.28058 ## 2 2 0 191.84038 185.91922 192.02037 175.53109 ## 3 3 1 144.80922 189.83059 201.97789 218.63097 ## 4 4 0 22.58577 60.99331 63.06560 56.76576 ## 5 5 0 46.27449 60.32287 35.20089 65.97300 ## 6 6 0 166.39739 194.00734 166.40298 169.77082 sample size: n &lt;- nrow(orf) n #277, just like the McCoach paper. ## [1] 277 sample means and cov matrix orfNames &lt;- paste0(&quot;orf&quot;, 1:4) (samMeans &lt;- round(apply(orf[,orfNames], 2, mean), 3)) ## orf1 orf2 orf3 orf4 ## 108.651 134.685 136.560 163.744 (samCov &lt;- round(cov(orf[,orfNames])*((n-1)/n), 3)) ## orf1 orf2 orf3 orf4 ## orf1 1989.243 1680.066 1718.200 1758.556 ## orf2 1680.066 1817.077 1769.452 1823.106 ## orf3 1718.200 1769.452 2157.290 2118.046 ## orf4 1758.556 1823.106 2118.046 2525.285 19.1 PART I: Spaghetti Plot For more details, check out https://www.r-bloggers.com/my-commonly-done-ggplot2-graphs/ First, let’s use reshape() to convert wide format to long format for plotting: growthDataLong &lt;- reshape(orf, varying = paste0(&quot;orf&quot;, 1:4), sep = &quot;&quot;, direction = &quot;long&quot;) head(growthDataLong) ## id treatmentDummy time orf ## 1.1 1 0 1 133.93092 ## 2.1 2 0 1 191.84038 ## 3.1 3 1 1 144.80922 ## 4.1 4 0 1 22.58577 ## 5.1 5 0 1 46.27449 ## 6.1 6 0 1 166.39739 Plot trajectory of individual with id=1 tspag_id1 = ggplot(growthDataLong[growthDataLong$id==1, ], aes(x=time, y=orf)) + geom_line() + xlab(&quot;Observation Time Point&quot;) + ylab(&quot;Y&quot;) + ylim(-40, 300) + ggtitle(&quot;Spaghetti plot&quot;) + aes(colour = factor(id)) tspag_id1 plot trajectory of everyone tspag = ggplot(growthDataLong, aes(x=time, y=orf)) + geom_line(show.legend = FALSE) + xlab(&quot;Observation Time Point&quot;) + ylab(&quot;Y&quot;) + ylim(-40, 300) + ggtitle(&quot;Spaghetti plot&quot;) + aes(colour = factor(id)) tspag 19.2 PART II: Growth Models 19.2.1 1. No growth model Let’s start by examining the hypothesis of no growth (intercept only) Intercept loads on all variables with fixed loadings of 1.0 Use a*VAR1 to fix the coefficient of VAR1 at a: noGrowthSyn &lt;- &quot; #Specify Latent Intercept I =~ 1*orf1 + 1*orf2 + 1*orf3 + 1*orf4 &quot; noGrowthFit &lt;- growth(noGrowthSyn, data = orf, fixed.x = FALSE) summary(noGrowthFit, fit.measures = T) ## lavaan 0.6-12 ended normally after 80 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 277 ## ## Model Test User Model: ## ## Test statistic 622.276 ## Degrees of freedom 8 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 1367.967 ## Degrees of freedom 6 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.549 ## Tucker-Lewis Index (TLI) 0.662 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -5438.991 ## Loglikelihood unrestricted model (H1) -5127.853 ## ## Akaike (AIC) 10889.982 ## Bayesian (BIC) 10911.726 ## Sample-size adjusted Bayesian (BIC) 10892.701 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.526 ## 90 Percent confidence interval - lower 0.492 ## 90 Percent confidence interval - upper 0.562 ## P-value RMSEA &lt;= 0.05 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.263 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## I =~ ## orf1 1.000 ## orf2 1.000 ## orf3 1.000 ## orf4 1.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .orf1 0.000 ## .orf2 0.000 ## .orf3 0.000 ## .orf4 0.000 ## I 134.926 2.559 52.728 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .orf1 1132.232 102.816 11.012 0.000 ## .orf2 98.891 28.254 3.500 0.000 ## .orf3 312.013 38.482 8.108 0.000 ## .orf4 1412.090 126.348 11.176 0.000 ## I 1746.714 154.599 11.298 0.000 semPaths(noGrowthFit, what=&#39;est&#39;, fade= F) 19.2.2 2. Linear growth model Intercept loads on all variables with fixed loadings of 1.0 Slope loads on all variables with fixed loadings of t = 0, 1, 2, …, t-1 t must start from 0 linearGrowthSyn &lt;- &quot; #Specify Latent Intercept and Slope I =~ 1*orf1 + 1*orf2 + 1*orf3 + 1*orf4 S =~ 0*orf1 + 1*orf2 + 2*orf3 + 3*orf4 &quot; linearGrowthFit &lt;- growth(linearGrowthSyn, data = orf, fixed.x = FALSE) summary(linearGrowthFit, fit.measures = T) ## lavaan 0.6-12 ended normally after 160 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 9 ## ## Number of observations 277 ## ## Model Test User Model: ## ## Test statistic 159.802 ## Degrees of freedom 5 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 1367.967 ## Degrees of freedom 6 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.886 ## Tucker-Lewis Index (TLI) 0.864 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -5207.754 ## Loglikelihood unrestricted model (H1) -5127.853 ## ## Akaike (AIC) 10433.508 ## Bayesian (BIC) 10466.124 ## Sample-size adjusted Bayesian (BIC) 10437.586 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.334 ## 90 Percent confidence interval - lower 0.291 ## 90 Percent confidence interval - upper 0.380 ## P-value RMSEA &lt;= 0.05 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.076 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## I =~ ## orf1 1.000 ## orf2 1.000 ## orf3 1.000 ## orf4 1.000 ## S =~ ## orf1 0.000 ## orf2 1.000 ## orf3 2.000 ## orf4 3.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## I ~~ ## S -4.269 29.309 -0.146 0.884 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .orf1 0.000 ## .orf2 0.000 ## .orf3 0.000 ## .orf4 0.000 ## I 110.504 2.585 42.746 0.000 ## S 17.218 0.628 27.410 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .orf1 209.999 45.503 4.615 0.000 ## .orf2 257.156 30.447 8.446 0.000 ## .orf3 302.851 34.737 8.718 0.000 ## .orf4 168.261 48.428 3.474 0.001 ## I 1698.098 159.067 10.675 0.000 ## S 70.482 11.729 6.009 0.000 semPaths(linearGrowthFit, what=&#39;est&#39;, fade= F) 19.2.3 3. Quadratic growth model Intercept loads on all variables with fixed loadings of 1.0 Slope loads on all variables with fixed loadings of t = 0, 1, 2, …, t-1 Quadratic loads on all variables with fixed loadings of t^2 = 0, 1, 4, …, (t-1)^2 Quadratic has no variance and covariances quadGrowthSyn &lt;- &quot; #int and slope factors I =~ 1*orf1 + 1*orf2 + 1*orf3 + 1*orf4 S =~ 0*orf1 + 1*orf2 + 2*orf3 + 3*orf4 #quadratic factor = slope^2 quadS =~ 0*orf1 + 1*orf2 + 4*orf3 + 9*orf4 &quot; quadGrowthFit &lt;- growth(quadGrowthSyn, data = orf, fixed.x = FALSE) ## Warning in lav_object_post_check(object): lavaan WARNING: some estimated ov variances are negative ## Warning in lav_object_post_check(object): lavaan WARNING: some estimated lv variances are negative If you get the following warning messages: 1: In lav_object_post_check(object) : lavaan WARNING: some estimated ov variances are negative Use var1~~0*var2 to fix the (co)variances at 0 quadGrowthSyn_noQuad &lt;- &quot; #int and slope factors I =~ 1*orf1 + 1*orf2 + 1*orf3 + 1*orf4 S =~ 0*orf1 + 1*orf2 + 2*orf3 + 3*orf4 #quadratic factor = slope^2 quadS =~ 0*orf1 + 1*orf2 + 4*orf3 + 9*orf4 quadS ~~ 0*quadS #restrict quadratic variance to 0 quadS ~~ 0*I #restrict quadratic covariance with I to 0 quadS ~~ 0*S #restrict quadratic covariance with S to 0 &quot; quadGrowthNoquadFit &lt;- growth(quadGrowthSyn_noQuad, data = orf, fixed.x = FALSE) summary(quadGrowthNoquadFit, fit.measures = T) ## lavaan 0.6-12 ended normally after 170 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 10 ## ## Number of observations 277 ## ## Model Test User Model: ## ## Test statistic 159.759 ## Degrees of freedom 4 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 1367.967 ## Degrees of freedom 6 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.886 ## Tucker-Lewis Index (TLI) 0.828 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -5207.733 ## Loglikelihood unrestricted model (H1) -5127.853 ## ## Akaike (AIC) 10435.465 ## Bayesian (BIC) 10471.705 ## Sample-size adjusted Bayesian (BIC) 10439.997 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.375 ## 90 Percent confidence interval - lower 0.326 ## 90 Percent confidence interval - upper 0.426 ## P-value RMSEA &lt;= 0.05 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.077 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## I =~ ## orf1 1.000 ## orf2 1.000 ## orf3 1.000 ## orf4 1.000 ## S =~ ## orf1 0.000 ## orf2 1.000 ## orf3 2.000 ## orf4 3.000 ## quadS =~ ## orf1 0.000 ## orf2 1.000 ## orf3 4.000 ## orf4 9.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## I ~~ ## quadS 0.000 ## S ~~ ## quadS 0.000 ## I ~~ ## S -4.081 29.338 -0.139 0.889 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .orf1 0.000 ## .orf2 0.000 ## .orf3 0.000 ## .orf4 0.000 ## I 110.574 2.619 42.216 0.000 ## S 16.827 1.544 10.897 0.000 ## quadS 0.129 0.460 0.281 0.779 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## quadS 0.000 ## .orf1 210.600 45.691 4.609 0.000 ## .orf2 261.092 30.735 8.495 0.000 ## .orf3 298.679 34.435 8.674 0.000 ## .orf4 167.167 48.238 3.465 0.001 ## I 1697.894 159.144 10.669 0.000 ## S 70.527 11.730 6.013 0.000 semPaths(quadGrowthNoquadFit, what=&#39;est&#39;, fade= F) 19.2.4 4. Latent basis growth model (extension of linear growth model) Intercept loads on all variables with fixed loadings of 1.0 Slope loads on all variables with free loadings between 0 and t-1 latentBasisSyn &lt;- &quot; #Int and slope specification I =~ 1*orf1 + 1*orf2 + 1*orf3 + 1*orf4 #orf2 and orf3 are free in the latent basis specification S =~ 0*orf1 + alpha1*orf2 + alpha2*orf3 + 3*orf4 &quot; latentBasisFit &lt;- growth(latentBasisSyn, data = orf, fixed.x = FALSE) summary(latentBasisFit, fit.measures = T) ## lavaan 0.6-12 ended normally after 180 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 11 ## ## Number of observations 277 ## ## Model Test User Model: ## ## Test statistic 41.591 ## Degrees of freedom 3 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 1367.967 ## Degrees of freedom 6 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.972 ## Tucker-Lewis Index (TLI) 0.943 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -5148.648 ## Loglikelihood unrestricted model (H1) -5127.853 ## ## Akaike (AIC) 10319.297 ## Bayesian (BIC) 10359.161 ## Sample-size adjusted Bayesian (BIC) 10324.281 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.215 ## 90 Percent confidence interval - lower 0.160 ## 90 Percent confidence interval - upper 0.276 ## P-value RMSEA &lt;= 0.05 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.042 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## I =~ ## orf1 1.000 ## orf2 1.000 ## orf3 1.000 ## orf4 1.000 ## S =~ ## orf1 0.000 ## orf2 (alp1) 1.343 0.056 23.780 0.000 ## orf3 (alp2) 1.575 0.058 27.342 0.000 ## orf4 3.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## I ~~ ## S -1.364 30.564 -0.045 0.964 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .orf1 0.000 ## .orf2 0.000 ## .orf3 0.000 ## .orf4 0.000 ## I 108.867 2.633 41.354 0.000 ## S 18.294 0.644 28.418 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .orf1 232.136 45.754 5.074 0.000 ## .orf2 200.709 24.071 8.338 0.000 ## .orf3 215.666 25.245 8.543 0.000 ## .orf4 193.173 49.612 3.894 0.000 ## I 1688.625 159.099 10.614 0.000 ## S 67.573 12.843 5.261 0.000 semPaths(latentBasisFit, what=&#39;est&#39;, fade= F) RMSEA failed us… one approach is to use model modification indices: modindices(latentBasisFit,sort. = T) ## lhs op rhs mi epc sepc.lv sepc.all sepc.nox ## 27 orf3 ~~ orf4 37.677 285.256 285.256 1.398 1.398 ## 17 orf2 ~1 37.648 21.985 21.985 0.491 0.491 ## 18 orf3 ~1 29.423 -20.326 -20.326 -0.447 -0.447 ## 26 orf2 ~~ orf4 29.372 -191.644 -191.644 -0.973 -0.973 ## 24 orf1 ~~ orf4 27.665 701.388 701.388 3.312 3.312 ## 22 orf1 ~~ orf2 17.100 164.959 164.959 0.764 0.764 ## 23 orf1 ~~ orf3 13.434 -113.046 -113.046 -0.505 -0.505 ## 16 orf1 ~1 12.388 -47.984 -47.984 -1.095 -1.095 ## 3 I =~ orf3 10.891 -0.077 -3.175 -0.070 -0.070 ## 2 I =~ orf2 5.875 0.060 2.461 0.055 0.055 ## 4 I =~ orf4 4.587 0.152 6.261 0.126 0.126 ## 1 I =~ orf1 0.801 0.063 2.577 0.059 0.059 ## 25 orf2 ~~ orf3 0.014 -4.660 -4.660 -0.022 -0.022 ## 19 orf4 ~1 0.010 -1.525 -1.525 -0.031 -0.031 Another approach is to use Spline Growth Model. 19.2.5 5. Spline Growth Model splineGrowthSyn &lt;- &quot; #Specify Latent Intercept and Slope I =~ 1*orf1 + 1*orf2 + 1*orf3 + 1*orf4 S =~ 0*orf1 + 1*orf2 + 2*orf3 + 3*orf4 #summer is the spline variable summer =~ 0*orf1 + 0*orf2 + 1*orf3 + 1*orf4 #Summer gets a mean but no variance summer ~ 1 summer ~~ 0*summer #Summer is uncorrelated with I and S summer ~~ 0*I summer ~~ 0*S &quot; splineGrowthFit &lt;- growth(splineGrowthSyn, data = orf, fixed.x = FALSE) summary(splineGrowthFit, fit.measures = T) ## lavaan 0.6-12 ended normally after 167 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 10 ## ## Number of observations 277 ## ## Model Test User Model: ## ## Test statistic 8.029 ## Degrees of freedom 4 ## P-value (Chi-square) 0.091 ## ## Model Test Baseline Model: ## ## Test statistic 1367.967 ## Degrees of freedom 6 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.997 ## Tucker-Lewis Index (TLI) 0.996 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -5131.868 ## Loglikelihood unrestricted model (H1) -5127.853 ## ## Akaike (AIC) 10283.735 ## Bayesian (BIC) 10319.975 ## Sample-size adjusted Bayesian (BIC) 10288.267 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.060 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.121 ## P-value RMSEA &lt;= 0.05 0.320 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.023 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## I =~ ## orf1 1.000 ## orf2 1.000 ## orf3 1.000 ## orf4 1.000 ## S =~ ## orf1 0.000 ## orf2 1.000 ## orf3 2.000 ## orf4 3.000 ## summer =~ ## orf1 0.000 ## orf2 0.000 ## orf3 1.000 ## orf4 1.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## I ~~ ## summer 0.000 ## S ~~ ## summer 0.000 ## I ~~ ## S 3.511 28.391 0.124 0.902 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## summer -24.715 1.804 -13.697 0.000 ## .orf1 0.000 ## .orf2 0.000 ## .orf3 0.000 ## .orf4 0.000 ## I 108.303 2.579 41.998 0.000 ## S 26.616 0.986 26.987 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## summer 0.000 ## .orf1 246.662 40.796 6.046 0.000 ## .orf2 165.717 21.986 7.537 0.000 ## .orf3 183.428 23.958 7.656 0.000 ## .orf4 219.241 42.313 5.181 0.000 ## I 1670.038 155.803 10.719 0.000 ## S 65.687 10.741 6.116 0.000 semPaths(splineGrowthFit, what=&#39;est&#39;, fade= F) 19.2.6 Model Comparison lavTestLRT(noGrowthFit, linearGrowthFit) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## linearGrowthFit 5 10434 10466 159.80 ## noGrowthFit 8 10890 10912 622.28 462.47 3 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Linear Growth Model fits significantly better than No Growth Model lavTestLRT(linearGrowthFit, quadGrowthNoquadFit) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## quadGrowthNoquadFit 4 10436 10472 159.76 ## linearGrowthFit 5 10434 10466 159.80 0.042546 1 0.8366 Linear Growth Model fits almost the same as the Quadratic Growth Model (keep linear model due to parsimony principle) lavTestLRT(linearGrowthFit, latentBasisFit) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## latentBasisFit 3 10319 10359 41.591 ## linearGrowthFit 5 10434 10466 159.802 118.21 2 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Latent Basis Model fits significantly better than Linear Growth Model lavTestLRT(linearGrowthFit, splineGrowthFit) ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff Df diff Pr(&gt;Chisq) ## splineGrowthFit 4 10284 10320 8.0292 ## linearGrowthFit 5 10434 10466 159.8018 151.77 1 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Spline Growth Model also fits significantly better than Linear Growth Model 19.2.7 6. Final Model: Spline Growth Model with a binary treatment predictor splineGrowthTreatmentPredictorSyn &lt;- &quot; #Specify Latent Intercept and Slope I =~ 1*orf1 + 1*orf2 + 1*orf3 + 1*orf4 S =~ 0*orf1 + 1*orf2 + 2*orf3 + 3*orf4 #summer is the spline variable summer =~ 0*orf1 + 0*orf2 + 1*orf3 + 1*orf4 #Summer gets a mean but no variance summer ~~ 0*summer #Summer is uncorrelated with I and S summer ~~ 0*I summer ~~ 0*S #Intercept, Slope, and Summer regressed on (predicted by) treatment I ~ treatmentDummy S ~ treatmentDummy summer ~ treatmentDummy &quot; When including external predictors, we need to turn on fixed.x = T… otherwise you’ll get a warning message and misleading model fit: # do not do this: splineGrowthTreatPredictorFit &lt;- growth(splineGrowthTreatmentPredictorSyn, data = orf, fixed.x = F) # &lt;- Here ## Warning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan WARNING: some observed variances are (at least) a factor 1000 times larger than others; use varTable(fit) to investigate ## Warning in lav_partable_check(lavpartable, categorical = lavoptions$.categorical, : lavaan WARNING: automatically added intercepts are set to zero: ## [treatmentDummy] Instead, turn on fixed.x = T: splineGrowthTreatPredictorFit &lt;- growth(splineGrowthTreatmentPredictorSyn, data = orf, fixed.x = T) # &lt;- Here summary(splineGrowthTreatPredictorFit, fit.measures = T) ## lavaan 0.6-12 ended normally after 216 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 13 ## ## Number of observations 277 ## ## Model Test User Model: ## ## Test statistic 8.288 ## Degrees of freedom 5 ## P-value (Chi-square) 0.141 ## ## Model Test Baseline Model: ## ## Test statistic 1604.488 ## Degrees of freedom 10 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.998 ## Tucker-Lewis Index (TLI) 0.996 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -5013.737 ## Loglikelihood unrestricted model (H1) -5009.593 ## ## Akaike (AIC) 10053.473 ## Bayesian (BIC) 10100.585 ## Sample-size adjusted Bayesian (BIC) 10059.364 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.049 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.105 ## P-value RMSEA &lt;= 0.05 0.443 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.019 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## I =~ ## orf1 1.000 ## orf2 1.000 ## orf3 1.000 ## orf4 1.000 ## S =~ ## orf1 0.000 ## orf2 1.000 ## orf3 2.000 ## orf4 3.000 ## summer =~ ## orf1 0.000 ## orf2 0.000 ## orf3 1.000 ## orf4 1.000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## I ~ ## treatmentDummy 1.181 5.165 0.229 0.819 ## S ~ ## treatmentDummy 14.530 1.725 8.425 0.000 ## summer ~ ## treatmentDummy 2.874 3.650 0.787 0.431 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .I ~~ ## .summer 0.000 ## .S ~~ ## .summer 0.000 ## .I ~~ ## .S -2.783 19.658 -0.142 0.887 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .orf1 0.000 ## .orf2 0.000 ## .orf3 0.000 ## .orf4 0.000 ## .I 107.754 3.565 30.223 0.000 ## .S 19.690 1.191 16.538 0.000 ## .summer -26.082 2.520 -10.352 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .summer 0.000 ## .orf1 232.349 33.038 7.033 0.000 ## .orf2 171.092 20.837 8.211 0.000 ## .orf3 189.397 22.176 8.541 0.000 ## .orf4 206.315 31.878 6.472 0.000 ## .I 1678.279 155.946 10.762 0.000 ## .S 5.773 6.112 0.944 0.345 semPaths(splineGrowthTreatPredictorFit, what=&#39;est&#39;, fade= F) 19.3 PART III: LGM on Latent Variables 19.3.1 Example Please go over the checklist: Make sure the latent variables satisfy longitudinal measurement invariance at the level of scalar invariance or above Use the loadings over time (i.e., metric invariance) No need to correlate the latent factors Add intercepts for all indicators EXCEPT for marker indicators Add correlated residuals for repeated measures of the same indicators Use std.lv = TRUE as the scaling method in growth() For this example, we will use the dataset exLong from package semTools: data(exLong) head(exLong) ## sex y1t1 y2t1 y3t1 y1t2 y2t2 y3t2 y1t3 y2t3 y3t3 ## 1 female 2.7625423 2.2812510 2.49656014 2.9499400 1.12338865 2.1505466 1.912824 1.9625734 2.4403812 ## 2 female 0.2707267 -0.7830365 -0.23554656 0.4631038 0.37536412 2.0283960 2.112440 0.4326280 2.6352259 ## 3 female -0.2604141 0.3146881 1.21590069 0.3528803 0.00986991 1.0709696 1.472736 1.1951005 1.4287358 ## 4 female -1.0227953 -1.3454733 0.04899156 1.9530137 -1.03357363 2.7817132 1.249376 -0.5369589 2.3371304 ## 5 female 0.7385408 -0.6027341 0.42557808 -0.2027779 -0.34937717 1.1865013 1.425101 1.8539630 2.0861627 ## 6 female 0.5864878 -0.4659974 0.10691644 0.7869085 -0.31982170 0.4825864 -1.201912 -2.0090700 -0.8366951 ?exLong The syntax for linear growth model with latent variables: exLinearGrowthsyn &lt;- &quot; # Use the loadings over time (i.e., metric invariance) f_t1 =~ lamb1*y1t1 + lamb2*y2t1 + lamb3*y3t1 f_t2 =~ lamb1*y1t2 + lamb2*y2t2 + lamb3*y3t2 f_t3 =~ lamb1*y1t3 + lamb2*y2t3 + lamb3*y3t3 #Int and slope specification I =~ 1*f_t1 + 1*f_t2 + 1*f_t3 S =~ 0*f_t1 + 1*f_t2 + 2*f_t3 # Add intercepts for all indicators EXCEPT for marker indicators y2t1 ~ 1 y3t1 ~ 1 y2t2 ~ 1 y3t2 ~ 1 y2t3 ~ 1 y3t3 ~ 1 # Add correlated residuals for repeated measures of the same indicators y1t1 ~~ y1t2 y1t1 ~~ y1t3 y1t2 ~~ y1t3 y2t1 ~~ y2t2 y2t1 ~~ y2t3 y2t2 ~~ y2t3 y3t1 ~~ y3t2 y3t1 ~~ y3t3 y3t2 ~~ y3t3 &quot; Use std.lv = TRUE as the scaling method in growth(): exLinearGrowthFit &lt;- growth(exLinearGrowthsyn, data = exLong, fixed.x = FALSE, std.lv = TRUE) ## Warning in lav_model_vcov(lavmodel = lavmodel, lavsamplestats = lavsamplestats, : lavaan WARNING: ## The variance-covariance matrix of the estimated parameters (vcov) ## does not appear to be positive definite! The smallest eigenvalue ## (= 4.804423e-18) is close to zero. This may be a symptom that the ## model is not identified. summary(exLinearGrowthFit, fit.measures = T) ## lavaan 0.6-12 ended normally after 36 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 39 ## Number of equality constraints 6 ## ## Number of observations 200 ## ## Model Test User Model: ## ## Test statistic 72.972 ## Degrees of freedom 21 ## P-value (Chi-square) 0.000 ## ## Model Test Baseline Model: ## ## Test statistic 1126.037 ## Degrees of freedom 36 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.952 ## Tucker-Lewis Index (TLI) 0.918 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -2209.468 ## Loglikelihood unrestricted model (H1) -2172.981 ## ## Akaike (AIC) 4484.935 ## Bayesian (BIC) 4593.780 ## Sample-size adjusted Bayesian (BIC) 4489.232 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.111 ## 90 Percent confidence interval - lower 0.084 ## 90 Percent confidence interval - upper 0.140 ## P-value RMSEA &lt;= 0.05 0.000 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.181 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## f_t1 =~ ## y1t1 (lmb1) 0.595 0.027 22.333 0.000 ## y2t1 (lmb2) 0.371 0.021 18.042 0.000 ## y3t1 (lmb3) 0.513 0.025 20.776 0.000 ## f_t2 =~ ## y1t2 (lmb1) 0.595 0.027 22.333 0.000 ## y2t2 (lmb2) 0.371 0.021 18.042 0.000 ## y3t2 (lmb3) 0.513 0.025 20.776 0.000 ## f_t3 =~ ## y1t3 (lmb1) 0.595 0.027 22.333 0.000 ## y2t3 (lmb2) 0.371 0.021 18.042 0.000 ## y3t3 (lmb3) 0.513 0.025 20.776 0.000 ## I =~ ## f_t1 1.000 ## f_t2 1.000 ## f_t3 1.000 ## S =~ ## f_t1 0.000 ## f_t2 1.000 ## f_t3 2.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y1t1 ~~ ## .y1t2 0.112 0.049 2.315 0.021 ## .y1t3 0.083 0.045 1.851 0.064 ## .y1t2 ~~ ## .y1t3 0.162 0.056 2.906 0.004 ## .y2t1 ~~ ## .y2t2 0.117 0.034 3.452 0.001 ## .y2t3 -0.009 0.034 -0.283 0.777 ## .y2t2 ~~ ## .y2t3 0.114 0.036 3.134 0.002 ## .y3t1 ~~ ## .y3t2 0.158 0.046 3.392 0.001 ## .y3t3 0.029 0.042 0.690 0.490 ## .y3t2 ~~ ## .y3t3 0.095 0.047 2.029 0.042 ## I ~~ ## S -0.079 0.119 -0.662 0.508 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .y2t1 -0.264 0.050 -5.288 0.000 ## .y3t1 0.540 0.058 9.269 0.000 ## .y2t2 -0.266 0.057 -4.689 0.000 ## .y3t2 0.291 0.068 4.287 0.000 ## .y2t3 -0.228 0.064 -3.584 0.000 ## .y3t3 0.534 0.071 7.542 0.000 ## .y1t1 0.000 ## .y1t2 0.000 ## .y1t3 0.000 ## .f_t1 -0.207 0.068 -3.036 0.002 ## .f_t2 0.119 0.080 1.484 0.138 ## .f_t3 0.262 0.050 5.201 0.000 ## I 0.174 0.065 2.683 0.007 ## S 0.643 0.071 9.078 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y1t1 0.320 0.062 5.191 0.000 ## .y2t1 0.376 0.045 8.418 0.000 ## .y3t1 0.440 0.060 7.277 0.000 ## .y1t2 0.457 0.077 5.946 0.000 ## .y2t2 0.394 0.048 8.224 0.000 ## .y3t2 0.474 0.068 7.006 0.000 ## .y1t3 0.333 0.077 4.305 0.000 ## .y2t3 0.433 0.052 8.397 0.000 ## .y3t3 0.381 0.065 5.895 0.000 ## .f_t1 1.000 ## .f_t2 1.000 ## .f_t3 1.000 ## I 1.000 ## S 1.000 semPaths(exLinearGrowthFit, what=&#39;est&#39;, fade= F) 19.3.2 Exercise Q: Could you fit the latent basis model to the same dataset and compare the fit of the two models? "],["lavaan-lab-17-second-order-and-bifactor-models.html", "Chapter 20 Lavaan Lab 17: Second-order and Bifactor Models 20.1 PART I: Unidimensional model 20.2 PART II: Correlated factors model 20.3 PART III: Second-order factor Model 20.4 PART IV: Bifactor Model 20.5 PART V: Model Comparison 20.6 Exercise: Mental Ability Scale", " Chapter 20 Lavaan Lab 17: Second-order and Bifactor Models In this lab, we will evaluate the dimensionality of ISMI-29 by fitting and comparing the following four models: Unidimensional model (one-factor CFA) Correlated factors model (multi-factor CFA) Second-order factor model Bifactor model Load up the lavaan and semPlot libraries: library(lavaan) library(semPlot) In this lab, we will work with the ISMI-29 data that are collected using Internalized Stigma of Mental Illness Scale 758 participants and 29 items Let’s read in the dataset: ISMI29 = read.csv(&#39;ISMI-29 n758 (Hammer 16).csv&#39;, header = F) Take a look at the dataset: head(ISMI29) ## V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 V21 V22 V23 V24 V25 V26 V27 V28 V29 ## 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 2 1 2 1 1 2 1 1 2 2 2 ## 2 4 4 4 3 4 4 2 2 1 1 2 2 3 2 2 3 3 2 3 3 2 2 4 2 2 4 4 1 2 ## 3 4 4 3 1 1 1 2 1 2 1 3 2 1 3 3 3 3 3 3 3 3 1 3 2 2 4 3 1 2 ## 4 1 1 2 1 1 3 2 1 1 1 1 1 1 3 2 1 1 1 2 1 3 1 2 1 1 1 1 1 2 ## 5 3 3 4 1 1 3 3 2 2 1 1 1 1 2 1 1 1 1 1 2 1 1 1 1 2 3 1 2 2 ## 6 2 2 3 2 2 3 2 1 2 1 1 1 1 1 1 1 1 1 3 2 2 2 4 1 2 3 1 1 3 sample size: n &lt;- nrow(ISMI29) n #758 ## [1] 758 Factor structure: Item1-6: Alienation “Having a mental illness has spoiled my life.” Item7-13: Stereotype Endorsement “Mentally ill people tend to be violent” Item14-18: Discrimination Experience “People discriminate against me because I have a mental illness” Item19-24: Social Withdrawal “I don’t talk about myself as much because I don’t want to burden others with my mental illness” Item25-29: *Stigma Resistance (*reverse-coded) “I can have a good, fulfilling life, despite my mental illness” 20.1 PART I: Unidimensional model Write out syntax for a one-factor CFA model: uni.model = &#39; ISMI =~ V1+V2+V3+V4+V5+V6+V7+V8+V9+V10+V11+V12+V13+V14+V15+V16+V17+V18+V19+ V20+V21+V22+V23+V24+V25+V26+V27+V28+V29 &#39; Fit the model: It is recommended to fix the variances of all first- and second-order factors to be 1 (lavaan: std.lv = TRUE) and request standardized solutions; uni.model.fit = lavaan::sem(uni.model, data=ISMI29, ordered = colnames(ISMI29), std.lv = TRUE, fixed.x = F) summary(uni.model.fit, standardized = TRUE, fit.measures = TRUE) ## lavaan 0.6-12 ended normally after 15 iterations ## ## Estimator DWLS ## Optimization method NLMINB ## Number of model parameters 116 ## ## Number of observations 758 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 2140.966 2606.734 ## Degrees of freedom 377 377 ## P-value (Chi-square) 0.000 0.000 ## Scaling correction factor 0.869 ## Shift parameter 143.934 ## simple second-order correction ## ## Model Test Baseline Model: ## ## Test statistic 76016.285 21485.170 ## Degrees of freedom 406 406 ## P-value 0.000 0.000 ## Scaling correction factor 3.587 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.977 0.894 ## Tucker-Lewis Index (TLI) 0.975 0.886 ## ## Robust Comparative Fit Index (CFI) NA ## Robust Tucker-Lewis Index (TLI) NA ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.079 0.088 ## 90 Percent confidence interval - lower 0.075 0.085 ## 90 Percent confidence interval - upper 0.082 0.092 ## P-value RMSEA &lt;= 0.05 0.000 0.000 ## ## Robust RMSEA NA ## 90 Percent confidence interval - lower NA ## 90 Percent confidence interval - upper NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.073 0.073 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Unstructured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## ISMI =~ ## V1 0.788 0.016 50.143 0.000 0.788 0.788 ## V2 0.730 0.018 40.532 0.000 0.730 0.730 ## V3 0.523 0.027 19.276 0.000 0.523 0.523 ## V4 0.699 0.019 36.908 0.000 0.699 0.699 ## V5 0.693 0.020 33.790 0.000 0.693 0.693 ## V6 0.726 0.018 40.699 0.000 0.726 0.726 ## V7 0.556 0.027 20.637 0.000 0.556 0.556 ## V8 0.586 0.032 18.105 0.000 0.586 0.586 ## V9 0.306 0.038 7.953 0.000 0.306 0.306 ## V10 0.678 0.026 26.009 0.000 0.678 0.678 ## V11 0.661 0.025 26.609 0.000 0.661 0.661 ## V12 0.572 0.034 16.904 0.000 0.572 0.572 ## V13 0.758 0.023 32.495 0.000 0.758 0.758 ## V14 0.660 0.022 29.922 0.000 0.660 0.660 ## V15 0.654 0.023 28.747 0.000 0.654 0.654 ## V16 0.738 0.018 40.911 0.000 0.738 0.738 ## V17 0.707 0.020 35.246 0.000 0.707 0.707 ## V18 0.764 0.017 44.135 0.000 0.764 0.764 ## V19 0.583 0.025 23.420 0.000 0.583 0.583 ## V20 0.768 0.016 46.920 0.000 0.768 0.768 ## V21 0.773 0.017 46.637 0.000 0.773 0.773 ## V22 0.744 0.018 40.995 0.000 0.744 0.744 ## V23 0.710 0.020 35.129 0.000 0.710 0.710 ## V24 0.739 0.019 39.098 0.000 0.739 0.739 ## V25 0.071 0.038 1.893 0.058 0.071 0.071 ## V26 0.728 0.019 37.856 0.000 0.728 0.728 ## V27 0.665 0.024 27.941 0.000 0.665 0.665 ## V28 0.359 0.038 9.485 0.000 0.359 0.359 ## V29 0.213 0.035 5.994 0.000 0.213 0.213 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .V1 0.000 0.000 0.000 ## .V2 0.000 0.000 0.000 ## .V3 0.000 0.000 0.000 ## .V4 0.000 0.000 0.000 ## .V5 0.000 0.000 0.000 ## .V6 0.000 0.000 0.000 ## .V7 0.000 0.000 0.000 ## .V8 0.000 0.000 0.000 ## .V9 0.000 0.000 0.000 ## .V10 0.000 0.000 0.000 ## .V11 0.000 0.000 0.000 ## .V12 0.000 0.000 0.000 ## .V13 0.000 0.000 0.000 ## .V14 0.000 0.000 0.000 ## .V15 0.000 0.000 0.000 ## .V16 0.000 0.000 0.000 ## .V17 0.000 0.000 0.000 ## .V18 0.000 0.000 0.000 ## .V19 0.000 0.000 0.000 ## .V20 0.000 0.000 0.000 ## .V21 0.000 0.000 0.000 ## .V22 0.000 0.000 0.000 ## .V23 0.000 0.000 0.000 ## .V24 0.000 0.000 0.000 ## .V25 0.000 0.000 0.000 ## .V26 0.000 0.000 0.000 ## .V27 0.000 0.000 0.000 ## .V28 0.000 0.000 0.000 ## .V29 0.000 0.000 0.000 ## ISMI 0.000 0.000 0.000 ## ## Thresholds: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## V1|t1 -0.980 0.054 -17.991 0.000 -0.980 -0.980 ## V1|t2 -0.099 0.046 -2.178 0.029 -0.099 -0.099 ## V1|t3 1.046 0.056 18.722 0.000 1.046 1.046 ## V2|t1 -0.903 0.053 -17.036 0.000 -0.903 -0.903 ## V2|t2 0.213 0.046 4.643 0.000 0.213 0.213 ## V2|t3 1.202 0.060 20.103 0.000 1.202 1.202 ## V3|t1 -1.149 0.058 -19.687 0.000 -1.149 -1.149 ## V3|t2 -0.070 0.046 -1.524 0.127 -0.070 -0.070 ## V3|t3 1.175 0.059 19.899 0.000 1.175 1.175 ## V4|t1 -0.985 0.055 -18.053 0.000 -0.985 -0.985 ## V4|t2 -0.066 0.046 -1.452 0.147 -0.066 -0.066 ## V4|t3 1.058 0.056 18.840 0.000 1.058 1.058 ## V5|t1 -0.785 0.051 -15.374 0.000 -0.785 -0.785 ## V5|t2 -0.046 0.046 -1.016 0.309 -0.046 -0.046 ## V5|t3 1.029 0.056 18.542 0.000 1.029 1.029 ## V6|t1 -0.758 0.051 -14.965 0.000 -0.758 -0.758 ## V6|t2 0.060 0.046 1.307 0.191 0.060 0.060 ## V6|t3 1.202 0.060 20.103 0.000 1.202 1.202 ## V7|t1 -0.685 0.050 -13.793 0.000 -0.685 -0.685 ## V7|t2 0.537 0.048 11.189 0.000 0.537 0.537 ## V7|t3 1.711 0.080 21.292 0.000 1.711 1.711 ## V8|t1 0.404 0.047 8.614 0.000 0.404 0.404 ## V8|t2 1.606 0.075 21.452 0.000 1.606 1.606 ## V8|t3 2.183 0.118 18.488 0.000 2.183 2.183 ## V9|t1 0.264 0.046 5.729 0.000 0.264 0.264 ## V9|t2 1.711 0.080 21.292 0.000 1.711 1.711 ## V9|t3 2.656 0.194 13.656 0.000 2.656 2.656 ## V10|t1 0.261 0.046 5.656 0.000 0.261 0.261 ## V10|t2 1.429 0.067 21.257 0.000 1.429 1.429 ## V10|t3 2.149 0.114 18.786 0.000 2.149 2.149 ## V11|t1 0.073 0.046 1.597 0.110 0.073 0.073 ## V11|t2 1.319 0.063 20.826 0.000 1.319 1.319 ## V11|t3 2.261 0.127 17.776 0.000 2.261 2.261 ## V12|t1 0.507 0.048 10.619 0.000 0.507 0.507 ## V12|t2 1.839 0.088 20.845 0.000 1.839 1.839 ## V12|t3 2.413 0.148 16.268 0.000 2.413 2.413 ## V13|t1 0.288 0.046 6.235 0.000 0.288 0.288 ## V13|t2 1.438 0.068 21.282 0.000 1.438 1.438 ## V13|t3 2.031 0.103 19.706 0.000 2.031 2.031 ## V14|t1 -0.632 0.049 -12.885 0.000 -0.632 -0.632 ## V14|t2 0.496 0.048 10.405 0.000 0.496 0.496 ## V14|t3 1.476 0.069 21.369 0.000 1.476 1.476 ## V15|t1 -0.401 0.047 -8.542 0.000 -0.401 -0.401 ## V15|t2 0.771 0.051 15.170 0.000 0.771 0.771 ## V15|t3 1.726 0.081 21.255 0.000 1.726 1.726 ## V16|t1 -0.557 0.048 -11.544 0.000 -0.557 -0.557 ## V16|t2 0.534 0.048 11.118 0.000 0.534 0.534 ## V16|t3 1.549 0.072 21.455 0.000 1.549 1.549 ## V17|t1 -0.285 0.046 -6.162 0.000 -0.285 -0.285 ## V17|t2 0.803 0.051 15.644 0.000 0.803 0.803 ## V17|t3 1.771 0.084 21.114 0.000 1.771 1.771 ## V18|t1 -0.437 0.047 -9.260 0.000 -0.437 -0.437 ## V18|t2 0.632 0.049 12.885 0.000 0.632 0.632 ## V18|t3 1.726 0.081 21.255 0.000 1.726 1.726 ## V19|t1 -1.243 0.061 -20.393 0.000 -1.243 -1.243 ## V19|t2 -0.210 0.046 -4.570 0.000 -0.210 -0.210 ## V19|t3 0.888 0.053 16.840 0.000 0.888 0.888 ## V20|t1 -0.664 0.049 -13.445 0.000 -0.664 -0.664 ## V20|t2 0.278 0.046 6.018 0.000 0.278 0.278 ## V20|t3 1.251 0.061 20.439 0.000 1.251 1.251 ## V21|t1 -0.727 0.050 -14.485 0.000 -0.727 -0.727 ## V21|t2 0.433 0.047 9.189 0.000 0.433 0.433 ## V21|t3 1.517 0.071 21.429 0.000 1.517 1.517 ## V22|t1 -0.261 0.046 -5.656 0.000 -0.261 -0.261 ## V22|t2 0.758 0.051 14.965 0.000 0.758 0.758 ## V22|t3 1.656 0.077 21.400 0.000 1.656 1.656 ## V23|t1 -0.572 0.048 -11.827 0.000 -0.572 -0.572 ## V23|t2 0.440 0.047 9.332 0.000 0.440 0.440 ## V23|t3 1.447 0.068 21.306 0.000 1.447 1.447 ## V24|t1 -0.323 0.046 -6.957 0.000 -0.323 -0.323 ## V24|t2 0.789 0.051 15.441 0.000 0.789 0.789 ## V24|t3 1.571 0.073 21.461 0.000 1.571 1.571 ## V25|t1 -0.873 0.052 -16.644 0.000 -0.873 -0.873 ## V25|t2 0.437 0.047 9.260 0.000 0.437 0.437 ## V25|t3 1.420 0.067 21.231 0.000 1.420 1.420 ## V26|t1 -1.001 0.055 -18.238 0.000 -1.001 -1.001 ## V26|t2 0.306 0.046 6.596 0.000 0.306 0.306 ## V26|t3 1.236 0.061 20.346 0.000 1.236 1.236 ## V27|t1 -0.481 0.048 -10.120 0.000 -0.481 -0.481 ## V27|t2 0.854 0.052 16.380 0.000 0.854 0.854 ## V27|t3 1.683 0.079 21.354 0.000 1.683 1.683 ## V28|t1 0.254 0.046 5.512 0.000 0.254 0.254 ## V28|t2 1.656 0.077 21.400 0.000 1.656 1.656 ## V28|t3 1.982 0.099 20.046 0.000 1.982 1.982 ## V29|t1 -0.698 0.050 -14.001 0.000 -0.698 -0.698 ## V29|t2 0.619 0.049 12.674 0.000 0.619 0.619 ## V29|t3 1.756 0.083 21.166 0.000 1.756 1.756 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .V1 0.379 0.379 0.379 ## .V2 0.467 0.467 0.467 ## .V3 0.727 0.727 0.727 ## .V4 0.511 0.511 0.511 ## .V5 0.520 0.520 0.520 ## .V6 0.473 0.473 0.473 ## .V7 0.691 0.691 0.691 ## .V8 0.656 0.656 0.656 ## .V9 0.907 0.907 0.907 ## .V10 0.541 0.541 0.541 ## .V11 0.563 0.563 0.563 ## .V12 0.672 0.672 0.672 ## .V13 0.425 0.425 0.425 ## .V14 0.564 0.564 0.564 ## .V15 0.573 0.573 0.573 ## .V16 0.455 0.455 0.455 ## .V17 0.500 0.500 0.500 ## .V18 0.416 0.416 0.416 ## .V19 0.660 0.660 0.660 ## .V20 0.410 0.410 0.410 ## .V21 0.402 0.402 0.402 ## .V22 0.447 0.447 0.447 ## .V23 0.495 0.495 0.495 ## .V24 0.453 0.453 0.453 ## .V25 0.995 0.995 0.995 ## .V26 0.470 0.470 0.470 ## .V27 0.558 0.558 0.558 ## .V28 0.871 0.871 0.871 ## .V29 0.955 0.955 0.955 ## ISMI 1.000 1.000 1.000 ## ## Scales y*: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## V1 1.000 1.000 1.000 ## V2 1.000 1.000 1.000 ## V3 1.000 1.000 1.000 ## V4 1.000 1.000 1.000 ## V5 1.000 1.000 1.000 ## V6 1.000 1.000 1.000 ## V7 1.000 1.000 1.000 ## V8 1.000 1.000 1.000 ## V9 1.000 1.000 1.000 ## V10 1.000 1.000 1.000 ## V11 1.000 1.000 1.000 ## V12 1.000 1.000 1.000 ## V13 1.000 1.000 1.000 ## V14 1.000 1.000 1.000 ## V15 1.000 1.000 1.000 ## V16 1.000 1.000 1.000 ## V17 1.000 1.000 1.000 ## V18 1.000 1.000 1.000 ## V19 1.000 1.000 1.000 ## V20 1.000 1.000 1.000 ## V21 1.000 1.000 1.000 ## V22 1.000 1.000 1.000 ## V23 1.000 1.000 1.000 ## V24 1.000 1.000 1.000 ## V25 1.000 1.000 1.000 ## V26 1.000 1.000 1.000 ## V27 1.000 1.000 1.000 ## V28 1.000 1.000 1.000 ## V29 1.000 1.000 1.000 Plot the path diagram: semPaths(uni.model.fit, what = &#39;std&#39;, fade = F) 20.2 PART II: Correlated factors model Write out syntax for a five-factor CFA model: cor.fac.model = &#39; Alienation =~ V1+V2+V3+V4+V5+V6 Stereotype =~ V7+V8+V9+V10+V11+V12+V13 Discrimination =~ V14+V15+V16+V17+V18 Withdrawal =~ V19+V20+V21+V22+V23+V24 Stigma =~ V25+V26+V27+V28+V29 &#39; cor.fac.model.fit = lavaan::sem(cor.fac.model, data=ISMI29, ordered = colnames(ISMI29), std.lv = TRUE, fixed.x = F) summary(cor.fac.model.fit, standardized = TRUE, fit.measures = TRUE) ## lavaan 0.6-12 ended normally after 43 iterations ## ## Estimator DWLS ## Optimization method NLMINB ## Number of model parameters 126 ## ## Number of observations 758 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 1340.188 1751.387 ## Degrees of freedom 367 367 ## P-value (Chi-square) 0.000 0.000 ## Scaling correction factor 0.829 ## Shift parameter 133.939 ## simple second-order correction ## ## Model Test Baseline Model: ## ## Test statistic 76016.285 21485.170 ## Degrees of freedom 406 406 ## P-value 0.000 0.000 ## Scaling correction factor 3.587 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.987 0.934 ## Tucker-Lewis Index (TLI) 0.986 0.927 ## ## Robust Comparative Fit Index (CFI) NA ## Robust Tucker-Lewis Index (TLI) NA ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.059 0.071 ## 90 Percent confidence interval - lower 0.056 0.067 ## 90 Percent confidence interval - upper 0.063 0.074 ## P-value RMSEA &lt;= 0.05 0.000 0.000 ## ## Robust RMSEA NA ## 90 Percent confidence interval - lower NA ## 90 Percent confidence interval - upper NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.059 0.059 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Unstructured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Alienation =~ ## V1 0.839 0.016 52.254 0.000 0.839 0.839 ## V2 0.776 0.018 42.466 0.000 0.776 0.776 ## V3 0.556 0.028 19.804 0.000 0.556 0.556 ## V4 0.743 0.019 39.776 0.000 0.743 0.743 ## V5 0.735 0.020 36.553 0.000 0.735 0.735 ## V6 0.773 0.018 43.805 0.000 0.773 0.773 ## Stereotype =~ ## V7 0.614 0.028 21.657 0.000 0.614 0.614 ## V8 0.643 0.034 19.107 0.000 0.643 0.643 ## V9 0.342 0.041 8.341 0.000 0.342 0.342 ## V10 0.741 0.026 28.172 0.000 0.741 0.741 ## V11 0.724 0.025 29.166 0.000 0.724 0.724 ## V12 0.627 0.035 18.158 0.000 0.627 0.627 ## V13 0.830 0.023 36.031 0.000 0.830 0.830 ## Discrimination =~ ## V14 0.727 0.021 34.282 0.000 0.727 0.727 ## V15 0.720 0.022 32.614 0.000 0.720 0.720 ## V16 0.809 0.016 50.107 0.000 0.809 0.809 ## V17 0.776 0.019 41.474 0.000 0.776 0.776 ## V18 0.853 0.019 45.026 0.000 0.853 0.853 ## Withdrawal =~ ## V19 0.609 0.026 23.873 0.000 0.609 0.609 ## V20 0.801 0.016 50.440 0.000 0.801 0.801 ## V21 0.808 0.017 48.511 0.000 0.808 0.808 ## V22 0.776 0.018 43.680 0.000 0.776 0.776 ## V23 0.743 0.021 36.183 0.000 0.743 0.743 ## V24 0.771 0.019 40.946 0.000 0.771 0.771 ## Stigma =~ ## V25 0.090 0.042 2.159 0.031 0.090 0.090 ## V26 0.814 0.020 40.736 0.000 0.814 0.814 ## V27 0.739 0.024 30.998 0.000 0.739 0.739 ## V28 0.406 0.040 10.062 0.000 0.406 0.406 ## V29 0.249 0.038 6.479 0.000 0.249 0.249 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Alienation ~~ ## Stereotype 0.798 0.021 37.898 0.000 0.798 0.798 ## Discrimination 0.767 0.019 41.097 0.000 0.767 0.767 ## Withdrawal 0.876 0.014 60.655 0.000 0.876 0.876 ## Stigma 0.868 0.019 46.044 0.000 0.868 0.868 ## Stereotype ~~ ## Discrimination 0.731 0.023 31.608 0.000 0.731 0.731 ## Withdrawal 0.825 0.019 44.132 0.000 0.825 0.825 ## Stigma 0.937 0.017 53.800 0.000 0.937 0.937 ## Discrimination ~~ ## Withdrawal 0.868 0.014 62.437 0.000 0.868 0.868 ## Stigma 0.688 0.028 24.773 0.000 0.688 0.688 ## Withdrawal ~~ ## Stigma 0.755 0.024 31.564 0.000 0.755 0.755 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .V1 0.000 0.000 0.000 ## .V2 0.000 0.000 0.000 ## .V3 0.000 0.000 0.000 ## .V4 0.000 0.000 0.000 ## .V5 0.000 0.000 0.000 ## .V6 0.000 0.000 0.000 ## .V7 0.000 0.000 0.000 ## .V8 0.000 0.000 0.000 ## .V9 0.000 0.000 0.000 ## .V10 0.000 0.000 0.000 ## .V11 0.000 0.000 0.000 ## .V12 0.000 0.000 0.000 ## .V13 0.000 0.000 0.000 ## .V14 0.000 0.000 0.000 ## .V15 0.000 0.000 0.000 ## .V16 0.000 0.000 0.000 ## .V17 0.000 0.000 0.000 ## .V18 0.000 0.000 0.000 ## .V19 0.000 0.000 0.000 ## .V20 0.000 0.000 0.000 ## .V21 0.000 0.000 0.000 ## .V22 0.000 0.000 0.000 ## .V23 0.000 0.000 0.000 ## .V24 0.000 0.000 0.000 ## .V25 0.000 0.000 0.000 ## .V26 0.000 0.000 0.000 ## .V27 0.000 0.000 0.000 ## .V28 0.000 0.000 0.000 ## .V29 0.000 0.000 0.000 ## Alienation 0.000 0.000 0.000 ## Stereotype 0.000 0.000 0.000 ## Discrimination 0.000 0.000 0.000 ## Withdrawal 0.000 0.000 0.000 ## Stigma 0.000 0.000 0.000 ## ## Thresholds: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## V1|t1 -0.980 0.054 -17.991 0.000 -0.980 -0.980 ## V1|t2 -0.099 0.046 -2.178 0.029 -0.099 -0.099 ## V1|t3 1.046 0.056 18.722 0.000 1.046 1.046 ## V2|t1 -0.903 0.053 -17.036 0.000 -0.903 -0.903 ## V2|t2 0.213 0.046 4.643 0.000 0.213 0.213 ## V2|t3 1.202 0.060 20.103 0.000 1.202 1.202 ## V3|t1 -1.149 0.058 -19.687 0.000 -1.149 -1.149 ## V3|t2 -0.070 0.046 -1.524 0.127 -0.070 -0.070 ## V3|t3 1.175 0.059 19.899 0.000 1.175 1.175 ## V4|t1 -0.985 0.055 -18.053 0.000 -0.985 -0.985 ## V4|t2 -0.066 0.046 -1.452 0.147 -0.066 -0.066 ## V4|t3 1.058 0.056 18.840 0.000 1.058 1.058 ## V5|t1 -0.785 0.051 -15.374 0.000 -0.785 -0.785 ## V5|t2 -0.046 0.046 -1.016 0.309 -0.046 -0.046 ## V5|t3 1.029 0.056 18.542 0.000 1.029 1.029 ## V6|t1 -0.758 0.051 -14.965 0.000 -0.758 -0.758 ## V6|t2 0.060 0.046 1.307 0.191 0.060 0.060 ## V6|t3 1.202 0.060 20.103 0.000 1.202 1.202 ## V7|t1 -0.685 0.050 -13.793 0.000 -0.685 -0.685 ## V7|t2 0.537 0.048 11.189 0.000 0.537 0.537 ## V7|t3 1.711 0.080 21.292 0.000 1.711 1.711 ## V8|t1 0.404 0.047 8.614 0.000 0.404 0.404 ## V8|t2 1.606 0.075 21.452 0.000 1.606 1.606 ## V8|t3 2.183 0.118 18.488 0.000 2.183 2.183 ## V9|t1 0.264 0.046 5.729 0.000 0.264 0.264 ## V9|t2 1.711 0.080 21.292 0.000 1.711 1.711 ## V9|t3 2.656 0.194 13.656 0.000 2.656 2.656 ## V10|t1 0.261 0.046 5.656 0.000 0.261 0.261 ## V10|t2 1.429 0.067 21.257 0.000 1.429 1.429 ## V10|t3 2.149 0.114 18.786 0.000 2.149 2.149 ## V11|t1 0.073 0.046 1.597 0.110 0.073 0.073 ## V11|t2 1.319 0.063 20.826 0.000 1.319 1.319 ## V11|t3 2.261 0.127 17.776 0.000 2.261 2.261 ## V12|t1 0.507 0.048 10.619 0.000 0.507 0.507 ## V12|t2 1.839 0.088 20.845 0.000 1.839 1.839 ## V12|t3 2.413 0.148 16.268 0.000 2.413 2.413 ## V13|t1 0.288 0.046 6.235 0.000 0.288 0.288 ## V13|t2 1.438 0.068 21.282 0.000 1.438 1.438 ## V13|t3 2.031 0.103 19.706 0.000 2.031 2.031 ## V14|t1 -0.632 0.049 -12.885 0.000 -0.632 -0.632 ## V14|t2 0.496 0.048 10.405 0.000 0.496 0.496 ## V14|t3 1.476 0.069 21.369 0.000 1.476 1.476 ## V15|t1 -0.401 0.047 -8.542 0.000 -0.401 -0.401 ## V15|t2 0.771 0.051 15.170 0.000 0.771 0.771 ## V15|t3 1.726 0.081 21.255 0.000 1.726 1.726 ## V16|t1 -0.557 0.048 -11.544 0.000 -0.557 -0.557 ## V16|t2 0.534 0.048 11.118 0.000 0.534 0.534 ## V16|t3 1.549 0.072 21.455 0.000 1.549 1.549 ## V17|t1 -0.285 0.046 -6.162 0.000 -0.285 -0.285 ## V17|t2 0.803 0.051 15.644 0.000 0.803 0.803 ## V17|t3 1.771 0.084 21.114 0.000 1.771 1.771 ## V18|t1 -0.437 0.047 -9.260 0.000 -0.437 -0.437 ## V18|t2 0.632 0.049 12.885 0.000 0.632 0.632 ## V18|t3 1.726 0.081 21.255 0.000 1.726 1.726 ## V19|t1 -1.243 0.061 -20.393 0.000 -1.243 -1.243 ## V19|t2 -0.210 0.046 -4.570 0.000 -0.210 -0.210 ## V19|t3 0.888 0.053 16.840 0.000 0.888 0.888 ## V20|t1 -0.664 0.049 -13.445 0.000 -0.664 -0.664 ## V20|t2 0.278 0.046 6.018 0.000 0.278 0.278 ## V20|t3 1.251 0.061 20.439 0.000 1.251 1.251 ## V21|t1 -0.727 0.050 -14.485 0.000 -0.727 -0.727 ## V21|t2 0.433 0.047 9.189 0.000 0.433 0.433 ## V21|t3 1.517 0.071 21.429 0.000 1.517 1.517 ## V22|t1 -0.261 0.046 -5.656 0.000 -0.261 -0.261 ## V22|t2 0.758 0.051 14.965 0.000 0.758 0.758 ## V22|t3 1.656 0.077 21.400 0.000 1.656 1.656 ## V23|t1 -0.572 0.048 -11.827 0.000 -0.572 -0.572 ## V23|t2 0.440 0.047 9.332 0.000 0.440 0.440 ## V23|t3 1.447 0.068 21.306 0.000 1.447 1.447 ## V24|t1 -0.323 0.046 -6.957 0.000 -0.323 -0.323 ## V24|t2 0.789 0.051 15.441 0.000 0.789 0.789 ## V24|t3 1.571 0.073 21.461 0.000 1.571 1.571 ## V25|t1 -0.873 0.052 -16.644 0.000 -0.873 -0.873 ## V25|t2 0.437 0.047 9.260 0.000 0.437 0.437 ## V25|t3 1.420 0.067 21.231 0.000 1.420 1.420 ## V26|t1 -1.001 0.055 -18.238 0.000 -1.001 -1.001 ## V26|t2 0.306 0.046 6.596 0.000 0.306 0.306 ## V26|t3 1.236 0.061 20.346 0.000 1.236 1.236 ## V27|t1 -0.481 0.048 -10.120 0.000 -0.481 -0.481 ## V27|t2 0.854 0.052 16.380 0.000 0.854 0.854 ## V27|t3 1.683 0.079 21.354 0.000 1.683 1.683 ## V28|t1 0.254 0.046 5.512 0.000 0.254 0.254 ## V28|t2 1.656 0.077 21.400 0.000 1.656 1.656 ## V28|t3 1.982 0.099 20.046 0.000 1.982 1.982 ## V29|t1 -0.698 0.050 -14.001 0.000 -0.698 -0.698 ## V29|t2 0.619 0.049 12.674 0.000 0.619 0.619 ## V29|t3 1.756 0.083 21.166 0.000 1.756 1.756 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .V1 0.297 0.297 0.297 ## .V2 0.398 0.398 0.398 ## .V3 0.690 0.690 0.690 ## .V4 0.449 0.449 0.449 ## .V5 0.460 0.460 0.460 ## .V6 0.403 0.403 0.403 ## .V7 0.623 0.623 0.623 ## .V8 0.586 0.586 0.586 ## .V9 0.883 0.883 0.883 ## .V10 0.451 0.451 0.451 ## .V11 0.476 0.476 0.476 ## .V12 0.607 0.607 0.607 ## .V13 0.311 0.311 0.311 ## .V14 0.471 0.471 0.471 ## .V15 0.481 0.481 0.481 ## .V16 0.346 0.346 0.346 ## .V17 0.398 0.398 0.398 ## .V18 0.272 0.272 0.272 ## .V19 0.629 0.629 0.629 ## .V20 0.359 0.359 0.359 ## .V21 0.347 0.347 0.347 ## .V22 0.398 0.398 0.398 ## .V23 0.448 0.448 0.448 ## .V24 0.405 0.405 0.405 ## .V25 0.992 0.992 0.992 ## .V26 0.337 0.337 0.337 ## .V27 0.454 0.454 0.454 ## .V28 0.835 0.835 0.835 ## .V29 0.938 0.938 0.938 ## Alienation 1.000 1.000 1.000 ## Stereotype 1.000 1.000 1.000 ## Discrimination 1.000 1.000 1.000 ## Withdrawal 1.000 1.000 1.000 ## Stigma 1.000 1.000 1.000 ## ## Scales y*: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## V1 1.000 1.000 1.000 ## V2 1.000 1.000 1.000 ## V3 1.000 1.000 1.000 ## V4 1.000 1.000 1.000 ## V5 1.000 1.000 1.000 ## V6 1.000 1.000 1.000 ## V7 1.000 1.000 1.000 ## V8 1.000 1.000 1.000 ## V9 1.000 1.000 1.000 ## V10 1.000 1.000 1.000 ## V11 1.000 1.000 1.000 ## V12 1.000 1.000 1.000 ## V13 1.000 1.000 1.000 ## V14 1.000 1.000 1.000 ## V15 1.000 1.000 1.000 ## V16 1.000 1.000 1.000 ## V17 1.000 1.000 1.000 ## V18 1.000 1.000 1.000 ## V19 1.000 1.000 1.000 ## V20 1.000 1.000 1.000 ## V21 1.000 1.000 1.000 ## V22 1.000 1.000 1.000 ## V23 1.000 1.000 1.000 ## V24 1.000 1.000 1.000 ## V25 1.000 1.000 1.000 ## V26 1.000 1.000 1.000 ## V27 1.000 1.000 1.000 ## V28 1.000 1.000 1.000 ## V29 1.000 1.000 1.000 semPaths(cor.fac.model.fit, what = &#39;std&#39;, fade = F) 20.3 PART III: Second-order factor Model Write out syntax for a five-factor second-order CFA model: secondfac.model = &#39; Alienation =~ V1+V2+V3+V4+V5+V6 Stereotype =~ V7+V8+V9+V10+V11+V12+V13 Discrimination =~ V14+V15+V16+V17+V18 Withdrawal =~ V19+V20+V21+V22+V23+V24 Stigma =~ V25+V26+V27+V28+V29 # Second-order factor ISMI ISMI =~ Alienation + Stereotype + Discrimination + Withdrawal + Stigma &#39; secondfac.model.fit = lavaan::sem(secondfac.model, data=ISMI29, ordered = colnames(ISMI29), std.lv = TRUE, fixed.x = F) summary(secondfac.model.fit, standardized = TRUE, fit.measures = TRUE) ## lavaan 0.6-12 ended normally after 150 iterations ## ## Estimator DWLS ## Optimization method NLMINB ## Number of model parameters 121 ## ## Number of observations 758 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 1550.644 1966.260 ## Degrees of freedom 372 372 ## P-value (Chi-square) 0.000 0.000 ## Scaling correction factor 0.848 ## Shift parameter 138.291 ## simple second-order correction ## ## Model Test Baseline Model: ## ## Test statistic 76016.285 21485.170 ## Degrees of freedom 406 406 ## P-value 0.000 0.000 ## Scaling correction factor 3.587 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.984 0.924 ## Tucker-Lewis Index (TLI) 0.983 0.917 ## ## Robust Comparative Fit Index (CFI) NA ## Robust Tucker-Lewis Index (TLI) NA ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.065 0.075 ## 90 Percent confidence interval - lower 0.061 0.072 ## 90 Percent confidence interval - upper 0.068 0.079 ## P-value RMSEA &lt;= 0.05 0.000 0.000 ## ## Robust RMSEA NA ## 90 Percent confidence interval - lower NA ## 90 Percent confidence interval - upper NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.064 0.064 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Unstructured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Alienation =~ ## V1 0.337 0.022 15.037 0.000 0.839 0.839 ## V2 0.312 0.021 15.081 0.000 0.776 0.776 ## V3 0.223 0.017 13.259 0.000 0.556 0.556 ## V4 0.298 0.019 15.597 0.000 0.742 0.742 ## V5 0.295 0.019 15.793 0.000 0.735 0.735 ## V6 0.310 0.020 15.515 0.000 0.772 0.772 ## Stereotype =~ ## V7 0.279 0.020 13.715 0.000 0.613 0.613 ## V8 0.292 0.022 13.523 0.000 0.643 0.643 ## V9 0.155 0.020 7.585 0.000 0.340 0.340 ## V10 0.336 0.022 15.118 0.000 0.739 0.739 ## V11 0.330 0.022 15.229 0.000 0.725 0.725 ## V12 0.284 0.021 13.434 0.000 0.625 0.625 ## V13 0.379 0.024 15.594 0.000 0.833 0.833 ## Discrimination =~ ## V14 0.377 0.018 20.832 0.000 0.727 0.727 ## V15 0.373 0.018 20.422 0.000 0.720 0.720 ## V16 0.420 0.019 22.444 0.000 0.810 0.810 ## V17 0.402 0.018 21.813 0.000 0.776 0.776 ## V18 0.442 0.024 18.275 0.000 0.853 0.853 ## Withdrawal =~ ## V19 0.185 0.019 9.612 0.000 0.609 0.609 ## V20 0.243 0.023 10.420 0.000 0.801 0.801 ## V21 0.245 0.024 10.093 0.000 0.809 0.809 ## V22 0.235 0.023 10.395 0.000 0.775 0.775 ## V23 0.225 0.023 9.963 0.000 0.743 0.743 ## V24 0.234 0.023 10.186 0.000 0.772 0.772 ## Stigma =~ ## V25 0.036 0.018 1.967 0.049 0.082 0.082 ## V26 0.357 0.031 11.708 0.000 0.815 0.815 ## V27 0.325 0.027 11.913 0.000 0.742 0.742 ## V28 0.175 0.022 8.005 0.000 0.400 0.400 ## V29 0.105 0.018 5.830 0.000 0.241 0.241 ## ISMI =~ ## Alienation 2.281 0.168 13.578 0.000 0.916 0.916 ## Stereotype 1.958 0.147 13.316 0.000 0.891 0.891 ## Discrimination 1.651 0.104 15.930 0.000 0.855 0.855 ## Withdrawal 3.144 0.332 9.469 0.000 0.953 0.953 ## Stigma 2.050 0.195 10.535 0.000 0.899 0.899 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .V1 0.000 0.000 0.000 ## .V2 0.000 0.000 0.000 ## .V3 0.000 0.000 0.000 ## .V4 0.000 0.000 0.000 ## .V5 0.000 0.000 0.000 ## .V6 0.000 0.000 0.000 ## .V7 0.000 0.000 0.000 ## .V8 0.000 0.000 0.000 ## .V9 0.000 0.000 0.000 ## .V10 0.000 0.000 0.000 ## .V11 0.000 0.000 0.000 ## .V12 0.000 0.000 0.000 ## .V13 0.000 0.000 0.000 ## .V14 0.000 0.000 0.000 ## .V15 0.000 0.000 0.000 ## .V16 0.000 0.000 0.000 ## .V17 0.000 0.000 0.000 ## .V18 0.000 0.000 0.000 ## .V19 0.000 0.000 0.000 ## .V20 0.000 0.000 0.000 ## .V21 0.000 0.000 0.000 ## .V22 0.000 0.000 0.000 ## .V23 0.000 0.000 0.000 ## .V24 0.000 0.000 0.000 ## .V25 0.000 0.000 0.000 ## .V26 0.000 0.000 0.000 ## .V27 0.000 0.000 0.000 ## .V28 0.000 0.000 0.000 ## .V29 0.000 0.000 0.000 ## .Alienation 0.000 0.000 0.000 ## .Stereotype 0.000 0.000 0.000 ## .Discrimination 0.000 0.000 0.000 ## .Withdrawal 0.000 0.000 0.000 ## .Stigma 0.000 0.000 0.000 ## ISMI 0.000 0.000 0.000 ## ## Thresholds: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## V1|t1 -0.980 0.054 -17.991 0.000 -0.980 -0.980 ## V1|t2 -0.099 0.046 -2.178 0.029 -0.099 -0.099 ## V1|t3 1.046 0.056 18.722 0.000 1.046 1.046 ## V2|t1 -0.903 0.053 -17.036 0.000 -0.903 -0.903 ## V2|t2 0.213 0.046 4.643 0.000 0.213 0.213 ## V2|t3 1.202 0.060 20.103 0.000 1.202 1.202 ## V3|t1 -1.149 0.058 -19.687 0.000 -1.149 -1.149 ## V3|t2 -0.070 0.046 -1.524 0.127 -0.070 -0.070 ## V3|t3 1.175 0.059 19.899 0.000 1.175 1.175 ## V4|t1 -0.985 0.055 -18.053 0.000 -0.985 -0.985 ## V4|t2 -0.066 0.046 -1.452 0.147 -0.066 -0.066 ## V4|t3 1.058 0.056 18.840 0.000 1.058 1.058 ## V5|t1 -0.785 0.051 -15.374 0.000 -0.785 -0.785 ## V5|t2 -0.046 0.046 -1.016 0.309 -0.046 -0.046 ## V5|t3 1.029 0.056 18.542 0.000 1.029 1.029 ## V6|t1 -0.758 0.051 -14.965 0.000 -0.758 -0.758 ## V6|t2 0.060 0.046 1.307 0.191 0.060 0.060 ## V6|t3 1.202 0.060 20.103 0.000 1.202 1.202 ## V7|t1 -0.685 0.050 -13.793 0.000 -0.685 -0.685 ## V7|t2 0.537 0.048 11.189 0.000 0.537 0.537 ## V7|t3 1.711 0.080 21.292 0.000 1.711 1.711 ## V8|t1 0.404 0.047 8.614 0.000 0.404 0.404 ## V8|t2 1.606 0.075 21.452 0.000 1.606 1.606 ## V8|t3 2.183 0.118 18.488 0.000 2.183 2.183 ## V9|t1 0.264 0.046 5.729 0.000 0.264 0.264 ## V9|t2 1.711 0.080 21.292 0.000 1.711 1.711 ## V9|t3 2.656 0.194 13.656 0.000 2.656 2.656 ## V10|t1 0.261 0.046 5.656 0.000 0.261 0.261 ## V10|t2 1.429 0.067 21.257 0.000 1.429 1.429 ## V10|t3 2.149 0.114 18.786 0.000 2.149 2.149 ## V11|t1 0.073 0.046 1.597 0.110 0.073 0.073 ## V11|t2 1.319 0.063 20.826 0.000 1.319 1.319 ## V11|t3 2.261 0.127 17.776 0.000 2.261 2.261 ## V12|t1 0.507 0.048 10.619 0.000 0.507 0.507 ## V12|t2 1.839 0.088 20.845 0.000 1.839 1.839 ## V12|t3 2.413 0.148 16.268 0.000 2.413 2.413 ## V13|t1 0.288 0.046 6.235 0.000 0.288 0.288 ## V13|t2 1.438 0.068 21.282 0.000 1.438 1.438 ## V13|t3 2.031 0.103 19.706 0.000 2.031 2.031 ## V14|t1 -0.632 0.049 -12.885 0.000 -0.632 -0.632 ## V14|t2 0.496 0.048 10.405 0.000 0.496 0.496 ## V14|t3 1.476 0.069 21.369 0.000 1.476 1.476 ## V15|t1 -0.401 0.047 -8.542 0.000 -0.401 -0.401 ## V15|t2 0.771 0.051 15.170 0.000 0.771 0.771 ## V15|t3 1.726 0.081 21.255 0.000 1.726 1.726 ## V16|t1 -0.557 0.048 -11.544 0.000 -0.557 -0.557 ## V16|t2 0.534 0.048 11.118 0.000 0.534 0.534 ## V16|t3 1.549 0.072 21.455 0.000 1.549 1.549 ## V17|t1 -0.285 0.046 -6.162 0.000 -0.285 -0.285 ## V17|t2 0.803 0.051 15.644 0.000 0.803 0.803 ## V17|t3 1.771 0.084 21.114 0.000 1.771 1.771 ## V18|t1 -0.437 0.047 -9.260 0.000 -0.437 -0.437 ## V18|t2 0.632 0.049 12.885 0.000 0.632 0.632 ## V18|t3 1.726 0.081 21.255 0.000 1.726 1.726 ## V19|t1 -1.243 0.061 -20.393 0.000 -1.243 -1.243 ## V19|t2 -0.210 0.046 -4.570 0.000 -0.210 -0.210 ## V19|t3 0.888 0.053 16.840 0.000 0.888 0.888 ## V20|t1 -0.664 0.049 -13.445 0.000 -0.664 -0.664 ## V20|t2 0.278 0.046 6.018 0.000 0.278 0.278 ## V20|t3 1.251 0.061 20.439 0.000 1.251 1.251 ## V21|t1 -0.727 0.050 -14.485 0.000 -0.727 -0.727 ## V21|t2 0.433 0.047 9.189 0.000 0.433 0.433 ## V21|t3 1.517 0.071 21.429 0.000 1.517 1.517 ## V22|t1 -0.261 0.046 -5.656 0.000 -0.261 -0.261 ## V22|t2 0.758 0.051 14.965 0.000 0.758 0.758 ## V22|t3 1.656 0.077 21.400 0.000 1.656 1.656 ## V23|t1 -0.572 0.048 -11.827 0.000 -0.572 -0.572 ## V23|t2 0.440 0.047 9.332 0.000 0.440 0.440 ## V23|t3 1.447 0.068 21.306 0.000 1.447 1.447 ## V24|t1 -0.323 0.046 -6.957 0.000 -0.323 -0.323 ## V24|t2 0.789 0.051 15.441 0.000 0.789 0.789 ## V24|t3 1.571 0.073 21.461 0.000 1.571 1.571 ## V25|t1 -0.873 0.052 -16.644 0.000 -0.873 -0.873 ## V25|t2 0.437 0.047 9.260 0.000 0.437 0.437 ## V25|t3 1.420 0.067 21.231 0.000 1.420 1.420 ## V26|t1 -1.001 0.055 -18.238 0.000 -1.001 -1.001 ## V26|t2 0.306 0.046 6.596 0.000 0.306 0.306 ## V26|t3 1.236 0.061 20.346 0.000 1.236 1.236 ## V27|t1 -0.481 0.048 -10.120 0.000 -0.481 -0.481 ## V27|t2 0.854 0.052 16.380 0.000 0.854 0.854 ## V27|t3 1.683 0.079 21.354 0.000 1.683 1.683 ## V28|t1 0.254 0.046 5.512 0.000 0.254 0.254 ## V28|t2 1.656 0.077 21.400 0.000 1.656 1.656 ## V28|t3 1.982 0.099 20.046 0.000 1.982 1.982 ## V29|t1 -0.698 0.050 -14.001 0.000 -0.698 -0.698 ## V29|t2 0.619 0.049 12.674 0.000 0.619 0.619 ## V29|t3 1.756 0.083 21.166 0.000 1.756 1.756 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .V1 0.295 0.295 0.295 ## .V2 0.398 0.398 0.398 ## .V3 0.691 0.691 0.691 ## .V4 0.449 0.449 0.449 ## .V5 0.460 0.460 0.460 ## .V6 0.403 0.403 0.403 ## .V7 0.624 0.624 0.624 ## .V8 0.587 0.587 0.587 ## .V9 0.885 0.885 0.885 ## .V10 0.453 0.453 0.453 ## .V11 0.475 0.475 0.475 ## .V12 0.610 0.610 0.610 ## .V13 0.307 0.307 0.307 ## .V14 0.472 0.472 0.472 ## .V15 0.482 0.482 0.482 ## .V16 0.344 0.344 0.344 ## .V17 0.397 0.397 0.397 ## .V18 0.273 0.273 0.273 ## .V19 0.629 0.629 0.629 ## .V20 0.359 0.359 0.359 ## .V21 0.346 0.346 0.346 ## .V22 0.399 0.399 0.399 ## .V23 0.449 0.449 0.449 ## .V24 0.404 0.404 0.404 ## .V25 0.993 0.993 0.993 ## .V26 0.336 0.336 0.336 ## .V27 0.450 0.450 0.450 ## .V28 0.840 0.840 0.840 ## .V29 0.942 0.942 0.942 ## .Alienation 1.000 0.161 0.161 ## .Stereotype 1.000 0.207 0.207 ## .Discrimination 1.000 0.268 0.268 ## .Withdrawal 1.000 0.092 0.092 ## .Stigma 1.000 0.192 0.192 ## ISMI 1.000 1.000 1.000 ## ## Scales y*: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## V1 1.000 1.000 1.000 ## V2 1.000 1.000 1.000 ## V3 1.000 1.000 1.000 ## V4 1.000 1.000 1.000 ## V5 1.000 1.000 1.000 ## V6 1.000 1.000 1.000 ## V7 1.000 1.000 1.000 ## V8 1.000 1.000 1.000 ## V9 1.000 1.000 1.000 ## V10 1.000 1.000 1.000 ## V11 1.000 1.000 1.000 ## V12 1.000 1.000 1.000 ## V13 1.000 1.000 1.000 ## V14 1.000 1.000 1.000 ## V15 1.000 1.000 1.000 ## V16 1.000 1.000 1.000 ## V17 1.000 1.000 1.000 ## V18 1.000 1.000 1.000 ## V19 1.000 1.000 1.000 ## V20 1.000 1.000 1.000 ## V21 1.000 1.000 1.000 ## V22 1.000 1.000 1.000 ## V23 1.000 1.000 1.000 ## V24 1.000 1.000 1.000 ## V25 1.000 1.000 1.000 ## V26 1.000 1.000 1.000 ## V27 1.000 1.000 1.000 ## V28 1.000 1.000 1.000 ## V29 1.000 1.000 1.000 semPaths(secondfac.model.fit, what = &#39;std&#39;, fade = F) 20.4 PART IV: Bifactor Model bifac.model = &#39; # specific factors Alienation =~ V1+V2+V3+V4+V5+V6 Stereotype =~ V7+V8+V9+V10+V11+V12+V13 Discrimination =~ V14+V15+V16+V17+V18 Withdrawal =~ V19+V20+V21+V22+V23+V24 Stigma =~ V25+V26+V27+V28+V29 # general factor GEN GEN =~ V1+V2+V3+V4+V5+V6+V7+V8+V9+V10+V11+V12+V13+V14+V15+V16+V17+V18+V19+ V20+V21+V22+V23+V24+V25+V26+V27+V28+V29 &#39; When using sem() to fit a bifactor model, make sure to turn on orthogonal = TRUE to ensure that all specific factors and general factors are uncorrelated otherwise, you’ll get an error/warning saying that the model is not identified. bifac.model.fit = lavaan::sem(bifac.model, data=ISMI29, ordered = colnames(ISMI29), std.lv = TRUE, fixed.x = F, orthogonal = TRUE) summary(bifac.model.fit, standardized = TRUE, fit.measures = TRUE) ## lavaan 0.6-12 ended normally after 75 iterations ## ## Estimator DWLS ## Optimization method NLMINB ## Number of model parameters 145 ## ## Number of observations 758 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 1062.671 1472.549 ## Degrees of freedom 348 348 ## P-value (Chi-square) 0.000 0.000 ## Scaling correction factor 0.788 ## Shift parameter 123.999 ## simple second-order correction ## ## Model Test Baseline Model: ## ## Test statistic 76016.285 21485.170 ## Degrees of freedom 406 406 ## P-value 0.000 0.000 ## Scaling correction factor 3.587 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.991 0.947 ## Tucker-Lewis Index (TLI) 0.989 0.938 ## ## Robust Comparative Fit Index (CFI) NA ## Robust Tucker-Lewis Index (TLI) NA ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.052 0.065 ## 90 Percent confidence interval - lower 0.049 0.062 ## 90 Percent confidence interval - upper 0.056 0.069 ## P-value RMSEA &lt;= 0.05 0.165 0.000 ## ## Robust RMSEA NA ## 90 Percent confidence interval - lower NA ## 90 Percent confidence interval - upper NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.054 0.054 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Unstructured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Alienation =~ ## V1 0.085 0.035 2.439 0.015 0.085 0.085 ## V2 0.147 0.036 4.126 0.000 0.147 0.147 ## V3 0.040 0.045 0.898 0.369 0.040 0.040 ## V4 0.474 0.038 12.601 0.000 0.474 0.474 ## V5 0.527 0.039 13.557 0.000 0.527 0.527 ## V6 0.393 0.035 11.194 0.000 0.393 0.393 ## Stereotype =~ ## V7 0.116 0.047 2.479 0.013 0.116 0.116 ## V8 0.210 0.049 4.272 0.000 0.210 0.210 ## V9 0.438 0.057 7.744 0.000 0.438 0.438 ## V10 0.274 0.042 6.544 0.000 0.274 0.274 ## V11 0.398 0.037 10.611 0.000 0.398 0.398 ## V12 0.563 0.056 10.012 0.000 0.563 0.563 ## V13 0.271 0.038 7.086 0.000 0.271 0.271 ## Discrimination =~ ## V14 0.495 0.032 15.564 0.000 0.495 0.495 ## V15 0.477 0.033 14.494 0.000 0.477 0.477 ## V16 0.554 0.032 17.388 0.000 0.554 0.554 ## V17 0.482 0.033 14.625 0.000 0.482 0.482 ## V18 -0.032 0.032 -1.008 0.313 -0.032 -0.032 ## Withdrawal =~ ## V19 0.172 0.048 3.606 0.000 0.172 0.172 ## V20 0.465 0.058 8.050 0.000 0.465 0.465 ## V21 0.082 0.040 2.054 0.040 0.082 0.082 ## V22 0.358 0.052 6.888 0.000 0.358 0.358 ## V23 0.107 0.046 2.330 0.020 0.107 0.107 ## V24 0.218 0.042 5.185 0.000 0.218 0.218 ## Stigma =~ ## V25 0.404 0.066 6.138 0.000 0.404 0.404 ## V26 0.056 0.040 1.414 0.157 0.056 0.056 ## V27 0.291 0.048 6.086 0.000 0.291 0.291 ## V28 0.483 0.067 7.201 0.000 0.483 0.483 ## V29 0.349 0.061 5.752 0.000 0.349 0.349 ## GEN =~ ## V1 0.797 0.016 49.723 0.000 0.797 0.797 ## V2 0.730 0.019 39.263 0.000 0.730 0.730 ## V3 0.532 0.028 19.069 0.000 0.532 0.532 ## V4 0.657 0.022 30.128 0.000 0.657 0.657 ## V5 0.646 0.023 27.532 0.000 0.646 0.646 ## V6 0.697 0.020 34.749 0.000 0.697 0.697 ## V7 0.561 0.028 20.047 0.000 0.561 0.561 ## V8 0.581 0.034 17.084 0.000 0.581 0.581 ## V9 0.267 0.041 6.586 0.000 0.267 0.267 ## V10 0.666 0.028 23.951 0.000 0.666 0.666 ## V11 0.638 0.026 24.148 0.000 0.638 0.638 ## V12 0.521 0.038 13.851 0.000 0.521 0.521 ## V13 0.750 0.025 30.069 0.000 0.750 0.750 ## V14 0.600 0.026 23.101 0.000 0.600 0.600 ## V15 0.595 0.027 22.367 0.000 0.595 0.595 ## V16 0.667 0.023 28.805 0.000 0.667 0.667 ## V17 0.647 0.024 26.442 0.000 0.647 0.647 ## V18 0.787 0.017 45.401 0.000 0.787 0.787 ## V19 0.581 0.026 22.230 0.000 0.581 0.581 ## V20 0.743 0.019 39.129 0.000 0.743 0.743 ## V21 0.785 0.018 44.861 0.000 0.785 0.785 ## V22 0.723 0.021 34.234 0.000 0.723 0.723 ## V23 0.718 0.021 33.856 0.000 0.718 0.718 ## V24 0.735 0.020 36.129 0.000 0.735 0.735 ## V25 0.056 0.039 1.451 0.147 0.056 0.056 ## V26 0.742 0.020 37.899 0.000 0.742 0.742 ## V27 0.673 0.024 27.596 0.000 0.673 0.673 ## V28 0.351 0.039 8.996 0.000 0.351 0.351 ## V29 0.206 0.037 5.619 0.000 0.206 0.206 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Alienation ~~ ## Stereotype 0.000 0.000 0.000 ## Discrimination 0.000 0.000 0.000 ## Withdrawal 0.000 0.000 0.000 ## Stigma 0.000 0.000 0.000 ## GEN 0.000 0.000 0.000 ## Stereotype ~~ ## Discrimination 0.000 0.000 0.000 ## Withdrawal 0.000 0.000 0.000 ## Stigma 0.000 0.000 0.000 ## GEN 0.000 0.000 0.000 ## Discrimination ~~ ## Withdrawal 0.000 0.000 0.000 ## Stigma 0.000 0.000 0.000 ## GEN 0.000 0.000 0.000 ## Withdrawal ~~ ## Stigma 0.000 0.000 0.000 ## GEN 0.000 0.000 0.000 ## Stigma ~~ ## GEN 0.000 0.000 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .V1 0.000 0.000 0.000 ## .V2 0.000 0.000 0.000 ## .V3 0.000 0.000 0.000 ## .V4 0.000 0.000 0.000 ## .V5 0.000 0.000 0.000 ## .V6 0.000 0.000 0.000 ## .V7 0.000 0.000 0.000 ## .V8 0.000 0.000 0.000 ## .V9 0.000 0.000 0.000 ## .V10 0.000 0.000 0.000 ## .V11 0.000 0.000 0.000 ## .V12 0.000 0.000 0.000 ## .V13 0.000 0.000 0.000 ## .V14 0.000 0.000 0.000 ## .V15 0.000 0.000 0.000 ## .V16 0.000 0.000 0.000 ## .V17 0.000 0.000 0.000 ## .V18 0.000 0.000 0.000 ## .V19 0.000 0.000 0.000 ## .V20 0.000 0.000 0.000 ## .V21 0.000 0.000 0.000 ## .V22 0.000 0.000 0.000 ## .V23 0.000 0.000 0.000 ## .V24 0.000 0.000 0.000 ## .V25 0.000 0.000 0.000 ## .V26 0.000 0.000 0.000 ## .V27 0.000 0.000 0.000 ## .V28 0.000 0.000 0.000 ## .V29 0.000 0.000 0.000 ## Alienation 0.000 0.000 0.000 ## Stereotype 0.000 0.000 0.000 ## Discrimination 0.000 0.000 0.000 ## Withdrawal 0.000 0.000 0.000 ## Stigma 0.000 0.000 0.000 ## GEN 0.000 0.000 0.000 ## ## Thresholds: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## V1|t1 -0.980 0.054 -17.991 0.000 -0.980 -0.980 ## V1|t2 -0.099 0.046 -2.178 0.029 -0.099 -0.099 ## V1|t3 1.046 0.056 18.722 0.000 1.046 1.046 ## V2|t1 -0.903 0.053 -17.036 0.000 -0.903 -0.903 ## V2|t2 0.213 0.046 4.643 0.000 0.213 0.213 ## V2|t3 1.202 0.060 20.103 0.000 1.202 1.202 ## V3|t1 -1.149 0.058 -19.687 0.000 -1.149 -1.149 ## V3|t2 -0.070 0.046 -1.524 0.127 -0.070 -0.070 ## V3|t3 1.175 0.059 19.899 0.000 1.175 1.175 ## V4|t1 -0.985 0.055 -18.053 0.000 -0.985 -0.985 ## V4|t2 -0.066 0.046 -1.452 0.147 -0.066 -0.066 ## V4|t3 1.058 0.056 18.840 0.000 1.058 1.058 ## V5|t1 -0.785 0.051 -15.374 0.000 -0.785 -0.785 ## V5|t2 -0.046 0.046 -1.016 0.309 -0.046 -0.046 ## V5|t3 1.029 0.056 18.542 0.000 1.029 1.029 ## V6|t1 -0.758 0.051 -14.965 0.000 -0.758 -0.758 ## V6|t2 0.060 0.046 1.307 0.191 0.060 0.060 ## V6|t3 1.202 0.060 20.103 0.000 1.202 1.202 ## V7|t1 -0.685 0.050 -13.793 0.000 -0.685 -0.685 ## V7|t2 0.537 0.048 11.189 0.000 0.537 0.537 ## V7|t3 1.711 0.080 21.292 0.000 1.711 1.711 ## V8|t1 0.404 0.047 8.614 0.000 0.404 0.404 ## V8|t2 1.606 0.075 21.452 0.000 1.606 1.606 ## V8|t3 2.183 0.118 18.488 0.000 2.183 2.183 ## V9|t1 0.264 0.046 5.729 0.000 0.264 0.264 ## V9|t2 1.711 0.080 21.292 0.000 1.711 1.711 ## V9|t3 2.656 0.194 13.656 0.000 2.656 2.656 ## V10|t1 0.261 0.046 5.656 0.000 0.261 0.261 ## V10|t2 1.429 0.067 21.257 0.000 1.429 1.429 ## V10|t3 2.149 0.114 18.786 0.000 2.149 2.149 ## V11|t1 0.073 0.046 1.597 0.110 0.073 0.073 ## V11|t2 1.319 0.063 20.826 0.000 1.319 1.319 ## V11|t3 2.261 0.127 17.776 0.000 2.261 2.261 ## V12|t1 0.507 0.048 10.619 0.000 0.507 0.507 ## V12|t2 1.839 0.088 20.845 0.000 1.839 1.839 ## V12|t3 2.413 0.148 16.268 0.000 2.413 2.413 ## V13|t1 0.288 0.046 6.235 0.000 0.288 0.288 ## V13|t2 1.438 0.068 21.282 0.000 1.438 1.438 ## V13|t3 2.031 0.103 19.706 0.000 2.031 2.031 ## V14|t1 -0.632 0.049 -12.885 0.000 -0.632 -0.632 ## V14|t2 0.496 0.048 10.405 0.000 0.496 0.496 ## V14|t3 1.476 0.069 21.369 0.000 1.476 1.476 ## V15|t1 -0.401 0.047 -8.542 0.000 -0.401 -0.401 ## V15|t2 0.771 0.051 15.170 0.000 0.771 0.771 ## V15|t3 1.726 0.081 21.255 0.000 1.726 1.726 ## V16|t1 -0.557 0.048 -11.544 0.000 -0.557 -0.557 ## V16|t2 0.534 0.048 11.118 0.000 0.534 0.534 ## V16|t3 1.549 0.072 21.455 0.000 1.549 1.549 ## V17|t1 -0.285 0.046 -6.162 0.000 -0.285 -0.285 ## V17|t2 0.803 0.051 15.644 0.000 0.803 0.803 ## V17|t3 1.771 0.084 21.114 0.000 1.771 1.771 ## V18|t1 -0.437 0.047 -9.260 0.000 -0.437 -0.437 ## V18|t2 0.632 0.049 12.885 0.000 0.632 0.632 ## V18|t3 1.726 0.081 21.255 0.000 1.726 1.726 ## V19|t1 -1.243 0.061 -20.393 0.000 -1.243 -1.243 ## V19|t2 -0.210 0.046 -4.570 0.000 -0.210 -0.210 ## V19|t3 0.888 0.053 16.840 0.000 0.888 0.888 ## V20|t1 -0.664 0.049 -13.445 0.000 -0.664 -0.664 ## V20|t2 0.278 0.046 6.018 0.000 0.278 0.278 ## V20|t3 1.251 0.061 20.439 0.000 1.251 1.251 ## V21|t1 -0.727 0.050 -14.485 0.000 -0.727 -0.727 ## V21|t2 0.433 0.047 9.189 0.000 0.433 0.433 ## V21|t3 1.517 0.071 21.429 0.000 1.517 1.517 ## V22|t1 -0.261 0.046 -5.656 0.000 -0.261 -0.261 ## V22|t2 0.758 0.051 14.965 0.000 0.758 0.758 ## V22|t3 1.656 0.077 21.400 0.000 1.656 1.656 ## V23|t1 -0.572 0.048 -11.827 0.000 -0.572 -0.572 ## V23|t2 0.440 0.047 9.332 0.000 0.440 0.440 ## V23|t3 1.447 0.068 21.306 0.000 1.447 1.447 ## V24|t1 -0.323 0.046 -6.957 0.000 -0.323 -0.323 ## V24|t2 0.789 0.051 15.441 0.000 0.789 0.789 ## V24|t3 1.571 0.073 21.461 0.000 1.571 1.571 ## V25|t1 -0.873 0.052 -16.644 0.000 -0.873 -0.873 ## V25|t2 0.437 0.047 9.260 0.000 0.437 0.437 ## V25|t3 1.420 0.067 21.231 0.000 1.420 1.420 ## V26|t1 -1.001 0.055 -18.238 0.000 -1.001 -1.001 ## V26|t2 0.306 0.046 6.596 0.000 0.306 0.306 ## V26|t3 1.236 0.061 20.346 0.000 1.236 1.236 ## V27|t1 -0.481 0.048 -10.120 0.000 -0.481 -0.481 ## V27|t2 0.854 0.052 16.380 0.000 0.854 0.854 ## V27|t3 1.683 0.079 21.354 0.000 1.683 1.683 ## V28|t1 0.254 0.046 5.512 0.000 0.254 0.254 ## V28|t2 1.656 0.077 21.400 0.000 1.656 1.656 ## V28|t3 1.982 0.099 20.046 0.000 1.982 1.982 ## V29|t1 -0.698 0.050 -14.001 0.000 -0.698 -0.698 ## V29|t2 0.619 0.049 12.674 0.000 0.619 0.619 ## V29|t3 1.756 0.083 21.166 0.000 1.756 1.756 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .V1 0.358 0.358 0.358 ## .V2 0.445 0.445 0.445 ## .V3 0.715 0.715 0.715 ## .V4 0.343 0.343 0.343 ## .V5 0.305 0.305 0.305 ## .V6 0.360 0.360 0.360 ## .V7 0.672 0.672 0.672 ## .V8 0.618 0.618 0.618 ## .V9 0.736 0.736 0.736 ## .V10 0.482 0.482 0.482 ## .V11 0.435 0.435 0.435 ## .V12 0.411 0.411 0.411 ## .V13 0.364 0.364 0.364 ## .V14 0.395 0.395 0.395 ## .V15 0.419 0.419 0.419 ## .V16 0.248 0.248 0.248 ## .V17 0.348 0.348 0.348 ## .V18 0.379 0.379 0.379 ## .V19 0.633 0.633 0.633 ## .V20 0.232 0.232 0.232 ## .V21 0.377 0.377 0.377 ## .V22 0.349 0.349 0.349 ## .V23 0.473 0.473 0.473 ## .V24 0.413 0.413 0.413 ## .V25 0.833 0.833 0.833 ## .V26 0.446 0.446 0.446 ## .V27 0.462 0.462 0.462 ## .V28 0.643 0.643 0.643 ## .V29 0.836 0.836 0.836 ## Alienation 1.000 1.000 1.000 ## Stereotype 1.000 1.000 1.000 ## Discrimination 1.000 1.000 1.000 ## Withdrawal 1.000 1.000 1.000 ## Stigma 1.000 1.000 1.000 ## GEN 1.000 1.000 1.000 ## ## Scales y*: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## V1 1.000 1.000 1.000 ## V2 1.000 1.000 1.000 ## V3 1.000 1.000 1.000 ## V4 1.000 1.000 1.000 ## V5 1.000 1.000 1.000 ## V6 1.000 1.000 1.000 ## V7 1.000 1.000 1.000 ## V8 1.000 1.000 1.000 ## V9 1.000 1.000 1.000 ## V10 1.000 1.000 1.000 ## V11 1.000 1.000 1.000 ## V12 1.000 1.000 1.000 ## V13 1.000 1.000 1.000 ## V14 1.000 1.000 1.000 ## V15 1.000 1.000 1.000 ## V16 1.000 1.000 1.000 ## V17 1.000 1.000 1.000 ## V18 1.000 1.000 1.000 ## V19 1.000 1.000 1.000 ## V20 1.000 1.000 1.000 ## V21 1.000 1.000 1.000 ## V22 1.000 1.000 1.000 ## V23 1.000 1.000 1.000 ## V24 1.000 1.000 1.000 ## V25 1.000 1.000 1.000 ## V26 1.000 1.000 1.000 ## V27 1.000 1.000 1.000 ## V28 1.000 1.000 1.000 ## V29 1.000 1.000 1.000 semPaths(bifac.model.fit, what = &#39;std&#39;, fade = F) 20.5 PART V: Model Comparison UniFactor = fitMeasures(uni.model.fit, fit.measures = c(&quot;chisq.scaled&quot;, &quot;df.scaled&quot;, &quot;pvalue.scaled&quot;, &quot;rmsea.scaled&quot;, &quot;rmsea.ci.lower.scaled&quot;, &quot;rmsea.ci.upper.scaled&quot;, &quot;cfi.scaled&quot;, &quot;tli.scaled&quot;, &quot;srmr_bentler&quot;)) FiveFactor = fitMeasures(cor.fac.model.fit, fit.measures = c(&quot;chisq.scaled&quot;, &quot;df.scaled&quot;, &quot;pvalue.scaled&quot;, &quot;rmsea.scaled&quot;, &quot;rmsea.ci.lower.scaled&quot;, &quot;rmsea.ci.upper.scaled&quot;, &quot;cfi.scaled&quot;, &quot;tli.scaled&quot;, &quot;srmr_bentler&quot;)) SecondOrder = fitMeasures(secondfac.model.fit, fit.measures = c(&quot;chisq.scaled&quot;, &quot;df.scaled&quot;, &quot;pvalue.scaled&quot;, &quot;rmsea.scaled&quot;, &quot;rmsea.ci.lower.scaled&quot;, &quot;rmsea.ci.upper.scaled&quot;, &quot;cfi.scaled&quot;, &quot;tli.scaled&quot;, &quot;srmr_bentler&quot;)) Bifactor = fitMeasures(bifac.model.fit, fit.measures = c(&quot;chisq.scaled&quot;, &quot;df.scaled&quot;, &quot;pvalue.scaled&quot;, &quot;rmsea.scaled&quot;, &quot;rmsea.ci.lower.scaled&quot;, &quot;rmsea.ci.upper.scaled&quot;, &quot;cfi.scaled&quot;, &quot;tli.scaled&quot;, &quot;srmr_bentler&quot;)) round(cbind(UniFactor, FiveFactor, SecondOrder, Bifactor), 3) ## UniFactor FiveFactor SecondOrder Bifactor ## chisq.scaled 2606.734 1751.387 1966.260 1472.549 ## df.scaled 377.000 367.000 372.000 348.000 ## pvalue.scaled 0.000 0.000 0.000 0.000 ## rmsea.scaled 0.088 0.071 0.075 0.065 ## rmsea.ci.lower.scaled 0.085 0.067 0.072 0.062 ## rmsea.ci.upper.scaled 0.092 0.074 0.079 0.069 ## cfi.scaled 0.894 0.934 0.924 0.947 ## tli.scaled 0.886 0.927 0.917 0.938 Bifactor model wins! Bifactor indices: library(BifactorIndicesCalculator) bifactorIndices(bifac.model.fit) ## $ModelLevelIndices ## ECV.GEN PUC Omega.GEN OmegaH.GEN ## 0.7629284 0.8251232 0.9475706 0.9021135 ## ## $FactorLevelIndices ## ECV_SS ECV_SG ECV_GS Omega OmegaH H FD ## Alienation 0.1978116 0.04500342 0.8021884 0.8576852 0.12689280 0.4702717 0.7798457 ## Stereotype 0.2660867 0.05719306 0.7339133 0.8185651 0.20058412 0.5260601 0.7721520 ## Discrimination 0.3156019 0.06635217 0.6843981 0.8568937 0.21680402 0.5773888 0.8395134 ## Withdrawal 0.1247961 0.02879844 0.8752039 0.8607013 0.08233921 0.3428283 0.7144528 ## Stigma 0.3408425 0.03972453 0.6591575 0.6197695 0.24248454 0.4234151 0.6776788 ## GEN 0.7629284 0.76292837 0.7629284 0.9475706 0.90211349 0.9584643 0.9703199 ## ## $ItemLevelIndices ## IECV ## V1 0.9888314 ## V2 0.9610230 ## V3 0.9942462 ## V4 0.6576392 ## V5 0.6005025 ## V6 0.7590656 ## V7 0.9593062 ## V8 0.8840932 ## V9 0.2706826 ## V10 0.8554197 ## V11 0.7203853 ## V12 0.4616681 ## V13 0.8845161 ## V14 0.5944395 ## V15 0.6092544 ## V16 0.5915492 ## V17 0.6430917 ## V18 0.9983599 ## V19 0.9195231 ## V20 0.7188428 ## V21 0.9891905 ## V22 0.8024897 ## V23 0.9780720 ## V24 0.9194145 ## V25 0.0190688 ## V26 0.9942829 ## V27 0.8426852 ## V28 0.3454092 ## V29 0.2580677 20.6 Exercise: Mental Ability Scale Let’s bring our Holzinger and Swineford Dataset back: head(HolzingerSwineford1939) ## id sex ageyr agemo school grade x1 x2 x3 x4 x5 x6 x7 x8 x9 ## 1 1 1 13 1 Pasteur 7 3.333333 7.75 0.375 2.333333 5.75 1.2857143 3.391304 5.75 6.361111 ## 2 2 2 13 7 Pasteur 7 5.333333 5.25 2.125 1.666667 3.00 1.2857143 3.782609 6.25 7.916667 ## 3 3 2 13 1 Pasteur 7 4.500000 5.25 1.875 1.000000 1.75 0.4285714 3.260870 3.90 4.416667 ## 4 4 1 13 2 Pasteur 7 5.333333 7.75 3.000 2.666667 4.50 2.4285714 3.000000 5.30 4.861111 ## 5 5 2 12 2 Pasteur 7 4.833333 4.75 0.875 2.666667 4.00 2.5714286 3.695652 6.30 5.916667 ## 6 6 2 14 1 Pasteur 7 5.333333 5.00 2.250 1.000000 3.00 0.8571429 4.347826 6.65 7.500000 ?HolzingerSwineford1939 This dataset has 301 cases with 9 mental ability items. Assignment: Could you use the four models above to examine the dimensionality of this ODD Subscale? Here is a factor structure that you may need: cor.fac.HS.model = &#39; visual =~ x1 + x2 + x3 textual =~ x4 + x5 + x6 speed =~ x7 + x8 + x9 &#39; Good luck! bifac.model = &#39; # specific factors visual =~ x2 + x3 # remove x1 because of heywood case textual =~ x4 + x5 + x6 speed =~ x7 + x8 + x9 # general factor GEN &lt;- G =~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 &#39; bifac.model.fit = lavaan::sem(bifac.model, data=HolzingerSwineford1939, #ordered = colnames(odd), std.lv = TRUE, fixed.x = F, orthogonal = TRUE, estimator = &#39;MLR&#39;) lavaan:::summary(bifac.model.fit, standardized = TRUE, fit.measures = TRUE) ## lavaan 0.6-12 ended normally after 24 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 26 ## ## Number of observations 301 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 36.711 36.204 ## Degrees of freedom 19 19 ## P-value (Chi-square) 0.009 0.010 ## Scaling correction factor 1.014 ## Yuan-Bentler correction (Mplus variant) ## ## Model Test Baseline Model: ## ## Test statistic 918.852 880.082 ## Degrees of freedom 36 36 ## P-value 0.000 0.000 ## Scaling correction factor 1.044 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.980 0.980 ## Tucker-Lewis Index (TLI) 0.962 0.961 ## ## Robust Comparative Fit Index (CFI) 0.980 ## Robust Tucker-Lewis Index (TLI) 0.962 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -3713.448 -3713.448 ## Scaling correction factor 1.078 ## for the MLR correction ## Loglikelihood unrestricted model (H1) -3695.092 -3695.092 ## Scaling correction factor 1.051 ## for the MLR correction ## ## Akaike (AIC) 7478.895 7478.895 ## Bayesian (BIC) 7575.280 7575.280 ## Sample-size adjusted Bayesian (BIC) 7492.823 7492.823 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.056 0.055 ## 90 Percent confidence interval - lower 0.027 0.027 ## 90 Percent confidence interval - upper 0.082 0.082 ## P-value RMSEA &lt;= 0.05 0.335 0.352 ## ## Robust RMSEA 0.055 ## 90 Percent confidence interval - lower 0.027 ## 90 Percent confidence interval - upper 0.082 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.040 0.040 ## ## Parameter Estimates: ## ## Standard errors Sandwich ## Information bread Observed ## Observed information based on Hessian ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## visual =~ ## x2 0.751 0.366 2.050 0.040 0.751 0.639 ## x3 0.223 0.008 27.825 0.000 0.223 0.198 ## textual =~ ## x4 0.854 0.067 12.802 0.000 0.854 0.735 ## x5 1.035 0.063 16.494 0.000 1.035 0.804 ## x6 0.782 0.057 13.746 0.000 0.782 0.715 ## speed =~ ## x7 0.717 0.084 8.510 0.000 0.717 0.659 ## x8 0.701 0.081 8.710 0.000 0.701 0.694 ## x9 0.441 0.057 7.688 0.000 0.441 0.438 ## G =~ ## x1 0.936 0.094 9.920 0.000 0.936 0.803 ## x2 0.455 0.084 5.422 0.000 0.455 0.387 ## x3 0.623 0.073 8.544 0.000 0.623 0.552 ## x4 0.486 0.088 5.542 0.000 0.486 0.418 ## x5 0.439 0.090 4.857 0.000 0.439 0.340 ## x6 0.468 0.085 5.490 0.000 0.468 0.428 ## x7 0.112 0.082 1.374 0.169 0.112 0.103 ## x8 0.291 0.074 3.936 0.000 0.291 0.288 ## x9 0.509 0.076 6.716 0.000 0.509 0.505 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## visual ~~ ## textual 0.000 0.000 0.000 ## speed 0.000 0.000 0.000 ## G 0.000 0.000 0.000 ## textual ~~ ## speed 0.000 0.000 0.000 ## G 0.000 0.000 0.000 ## speed ~~ ## G 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .x2 0.612 0.513 1.192 0.233 0.612 0.443 ## .x3 0.837 0.083 10.068 0.000 0.837 0.656 ## .x4 0.384 0.051 7.598 0.000 0.384 0.284 ## .x5 0.396 0.065 6.135 0.000 0.396 0.238 ## .x6 0.366 0.046 7.933 0.000 0.366 0.306 ## .x7 0.657 0.109 6.021 0.000 0.657 0.555 ## .x8 0.446 0.095 4.687 0.000 0.446 0.436 ## .x9 0.561 0.064 8.741 0.000 0.561 0.553 ## .x1 0.482 0.152 3.178 0.001 0.482 0.355 ## visual 1.000 1.000 1.000 ## textual 1.000 1.000 1.000 ## speed 1.000 1.000 1.000 ## G 1.000 1.000 1.000 "],["lavaan-lab-18-cfa-of-mtmm-matrix.html", "Chapter 21 Lavaan Lab 18: CFA of MTMM Matrix 21.1 PART I: Correlated methods specification 21.2 PART II: Correlated uniqueness specification", " Chapter 21 Lavaan Lab 18: CFA of MTMM Matrix In this lab, we will: run CFA on MTMM Matrix to investigate convergent and discrimative validity Load up the lavaan and semPlot libraries: library(lavaan) library(semPlot) Let’s read in a simulated MTMM matrix: load(&quot;MTMM.RData&quot;) Take a look at the matrix: dim(MTMM) ## [1] 9 9 head(MTMM) ## PARI SZTI SZDI PARC SZTC SZDC PARO SZTO SZDO ## PARI 13.032100 3.831654 4.821083 6.230066 2.198598 2.242893 4.463909 1.712295 1.612804 ## SZTI 3.831654 13.395600 6.280633 2.560975 6.498623 2.931111 1.852546 5.704037 1.799402 ## SZDI 4.821083 6.280633 12.888100 2.205911 1.370590 6.967652 1.554111 1.268419 4.950754 ## PARC 6.230066 2.560975 2.205911 8.643600 1.897447 1.633905 4.333795 1.856963 1.739304 ## SZTC 2.198598 6.498623 1.370590 1.897447 9.180900 0.829008 1.627837 4.700197 1.038442 ## SZDC 2.242893 2.931111 6.967652 1.633905 0.829008 8.122500 1.467864 1.710456 4.354686 This is a covariance matrix. You could also convert it to a correlation matrix: cov2cor(MTMM) ## PARI SZTI SZDI PARC SZTC SZDC PARO SZTO SZDO ## PARI 1.000 0.290 0.372 0.587 0.201 0.218 0.557 0.196 0.219 ## SZTI 0.290 1.000 0.478 0.238 0.586 0.281 0.228 0.644 0.241 ## SZDI 0.372 0.478 1.000 0.209 0.126 0.681 0.195 0.146 0.676 ## PARC 0.587 0.238 0.209 1.000 0.213 0.195 0.664 0.261 0.290 ## SZTC 0.201 0.586 0.126 0.213 1.000 0.096 0.242 0.641 0.168 ## SZDC 0.218 0.281 0.681 0.195 0.096 1.000 0.232 0.248 0.749 ## PARO 0.557 0.228 0.195 0.664 0.242 0.232 1.000 0.383 0.361 ## SZTO 0.196 0.644 0.146 0.261 0.641 0.248 0.383 1.000 0.342 ## SZDO 0.219 0.241 0.676 0.290 0.168 0.749 0.361 0.342 1.000 21.1 PART I: Correlated methods specification This model specifies both traits and methods factors: MTMM.model.spec1.wrong &lt;- &#39; # trait factors paranoid =~ PARI + PARC + PARO schizotypal =~ SZTI + SZTC + SZTO schizoid =~ SZDI + SZDC + SZDO # method factors inventory =~ SZTI + PARI + SZDI clininter =~ PARC + SZTC + SZDC obsrating =~ PARO + SZTO + SZDO &#39; Fit the model: Since MTMM is a covariance matrix, we supply the sample size 500; fit1 &lt;- lavaan::sem(MTMM.model.spec1.wrong, sample.cov=MTMM, sample.nobs=500, fixed.x = F) ## Warning in lavaan::lavaan(model = MTMM.model.spec1.wrong, sample.cov = MTMM, : lavaan WARNING: ## the optimizer (NLMINB) claimed the model converged, but not all ## elements of the gradient are (near) zero; the optimizer may not ## have found a local solution use check.gradient = FALSE to skip ## this check. You might get the following warning message: Warning message: In lavaan::lavaan(model = MTMM.model.spec1.wrong, sample.cov = MTMM, : lavaan WARNING: the optimizer (NLMINB) claimed the model converged, but not all elements of the gradient are (near) zero; the optimizer may not have found a local solution use check.gradient = FALSE to skip this check. The problem is by default lavaan correlates all traits and methods factors; To get the model to fit, we need to manually uncorrelate traits and methods factors; MTMM.model.spec1 &lt;- &#39; # trait factors paranoid =~ PARI + PARC + PARO schizotypal =~ SZTI + SZTC + SZTO schizoid =~ SZDI + SZDC + SZDO # method factors inventory =~ SZTI + PARI + SZDI clininter =~ PARC + SZTC + SZDC obsrating =~ PARO + SZTO + SZDO # uncorrelated trait and method paranoid ~~ 0*inventory paranoid ~~ 0*clininter paranoid ~~ 0*obsrating schizotypal ~~ 0*inventory schizotypal ~~ 0*clininter schizotypal ~~ 0*obsrating schizoid ~~ 0*inventory schizoid ~~ 0*clininter schizoid ~~ 0*obsrating &#39; Model fit: fit2 &lt;- lavaan::sem(MTMM.model.spec1, sample.cov=MTMM, sample.nobs=500, fixed.x = F) ## Warning in lav_object_post_check(object): lavaan WARNING: some estimated ov variances are negative ## Warning in lav_object_post_check(object): lavaan WARNING: some estimated lv variances are negative summary(fit2, standardized = T, fit.measures = T) ## lavaan 0.6-12 ended normally after 246 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 33 ## ## Number of observations 500 ## ## Model Test User Model: ## ## Test statistic 8.904 ## Degrees of freedom 12 ## P-value (Chi-square) 0.711 ## ## Model Test Baseline Model: ## ## Test statistic 2503.656 ## Degrees of freedom 36 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.004 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -9877.263 ## Loglikelihood unrestricted model (H1) -9872.811 ## ## Akaike (AIC) 19820.525 ## Bayesian (BIC) 19959.607 ## Sample-size adjusted Bayesian (BIC) 19854.863 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.035 ## P-value RMSEA &lt;= 0.05 0.994 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.016 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## paranoid =~ ## PARI 1.000 2.509 0.697 ## PARC 0.969 0.065 15.016 0.000 2.432 0.828 ## PARO 0.703 0.046 15.363 0.000 1.764 0.796 ## schizotypal =~ ## SZTI 1.000 2.815 0.766 ## SZTC 0.807 0.049 16.469 0.000 2.270 0.749 ## SZTO 0.716 0.039 18.235 0.000 2.015 0.836 ## schizoid =~ ## SZDI 1.000 2.345 0.662 ## SZDC 0.843 0.045 18.857 0.000 1.977 0.699 ## SZDO 0.863 0.119 7.243 0.000 2.024 0.994 ## inventory =~ ## SZTI 1.000 1.474 0.401 ## PARI 0.670 0.083 8.036 0.000 0.987 0.274 ## SZDI 1.999 0.335 5.966 0.000 2.946 0.832 ## clininter =~ ## PARC 1.000 NA NA ## SZTC 2.674 3.163 0.845 0.398 NA NA ## SZDC 13.997 20.660 0.677 0.498 NA NA ## obsrating =~ ## PARO 1.000 0.694 0.313 ## SZTO 1.522 0.516 2.948 0.003 1.057 0.438 ## SZDO 0.807 0.231 3.492 0.000 0.560 0.275 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## paranoid ~~ ## inventory 0.000 0.000 0.000 ## clininter 0.000 0.000 0.000 ## obsrating 0.000 0.000 0.000 ## schizotypal ~~ ## inventory 0.000 0.000 0.000 ## clininter 0.000 0.000 0.000 ## obsrating 0.000 0.000 0.000 ## schizoid ~~ ## inventory 0.000 0.000 0.000 ## clininter 0.000 0.000 0.000 ## obsrating 0.000 0.000 0.000 ## paranoid ~~ ## schizotypal 2.543 0.436 5.830 0.000 0.360 0.360 ## schizoid 1.986 0.474 4.190 0.000 0.338 0.338 ## schizotypal ~~ ## schizoid 1.736 0.479 3.627 0.000 0.263 0.263 ## inventory ~~ ## clininter 0.075 0.120 0.628 0.530 0.739 0.739 ## obsrating 0.037 0.135 0.277 0.782 0.036 0.036 ## clininter ~~ ## obsrating 0.026 0.044 0.589 0.556 0.535 0.535 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .PARI 5.677 0.469 12.096 0.000 5.677 0.439 ## .PARC 2.710 0.344 7.883 0.000 2.710 0.314 ## .PARO 1.321 0.192 6.881 0.000 1.321 0.269 ## .SZTI 3.425 0.460 7.447 0.000 3.425 0.253 ## .SZTC 4.073 0.357 11.396 0.000 4.073 0.443 ## .SZTO 0.634 0.330 1.920 0.055 0.634 0.109 ## .SZDI -1.636 1.181 -1.386 0.166 -1.636 -0.130 ## .SZDC 5.021 2.118 2.370 0.018 5.021 0.628 ## .SZDO -0.268 0.467 -0.574 0.566 -0.268 -0.065 ## paranoid 6.293 0.752 8.372 0.000 1.000 1.000 ## schizotypal 7.924 0.765 10.357 0.000 1.000 1.000 ## schizoid 5.500 1.045 5.261 0.000 1.000 1.000 ## inventory 2.172 0.521 4.171 0.000 1.000 1.000 ## clininter -0.005 0.014 -0.337 0.736 NA NA ## obsrating 0.482 0.210 2.300 0.021 1.000 1.000 Heywood case Warning messages: 1: In lav_object_post_check(object) : lavaan WARNING: some estimated ov variances are negative Plot the path diagram: semPaths(fit2, what=&#39;std&#39;, nCharNodes = 0, nCharEdges = 0, # don&#39;t limit variable name lengths curvePivot = TRUE, curve = 1.1, # pull covariances&#39; curves out a little fade=FALSE) ## Warning in qgraph::qgraph(Edgelist, labels = nLab, bidirectional = Bidir, : Non-finite weights are omitted 21.2 PART II: Correlated uniqueness specification In this specification: There is no method factor; Instead, the unique factors are correlated within method blocks; MTMM.model.spec2 &lt;- &#39; # trait factors paranoid =~ PARI + PARC + PARO schizotypal =~ SZTI + SZTC + SZTO schizoid =~ SZDI + SZDC + SZDO # no method factors # correlated residual covariances # Method 1 Block PARI ~~ SZTI + SZDI SZTI ~~ SZDI # Method 2 Block PARC ~~ SZTC + SZDC SZTC ~~ SZDC # Method 3 Block PARO ~~ SZTO + SZDO SZTO ~~ SZDO &#39; Model fit: fit3 &lt;- lavaan::sem(MTMM.model.spec2, sample.cov=MTMM, sample.nobs=500, fixed.x = F, std.lv = T) #results with standardized parameter estimates summary(fit3, standardized=TRUE, fit.measures=TRUE) ## lavaan 0.6-12 ended normally after 59 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 30 ## ## Number of observations 500 ## ## Model Test User Model: ## ## Test statistic 14.371 ## Degrees of freedom 15 ## P-value (Chi-square) 0.498 ## ## Model Test Baseline Model: ## ## Test statistic 2503.656 ## Degrees of freedom 36 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.001 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -9879.996 ## Loglikelihood unrestricted model (H1) -9872.811 ## ## Akaike (AIC) 19819.992 ## Bayesian (BIC) 19946.430 ## Sample-size adjusted Bayesian (BIC) 19851.209 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.041 ## P-value RMSEA &lt;= 0.05 0.989 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.025 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## paranoid =~ ## PARI 2.588 0.145 17.833 0.000 2.588 0.712 ## PARC 2.472 0.121 20.350 0.000 2.472 0.841 ## PARO 1.747 0.088 19.946 0.000 1.747 0.788 ## schizotypal =~ ## SZTI 2.950 0.132 22.367 0.000 2.950 0.788 ## SZTC 2.348 0.123 19.047 0.000 2.348 0.768 ## SZTO 2.047 0.089 22.905 0.000 2.047 0.843 ## schizoid =~ ## SZDI 2.713 0.120 22.526 0.000 2.713 0.769 ## SZDC 2.438 0.107 22.826 0.000 2.438 0.860 ## SZDO 1.782 0.073 24.323 0.000 1.782 0.872 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .PARI ~~ ## .SZTI 1.274 0.338 3.774 0.000 1.274 0.217 ## .SZDI 2.537 0.329 7.703 0.000 2.537 0.441 ## .SZTI ~~ ## .SZDI 3.872 0.342 11.329 0.000 3.872 0.746 ## .PARC ~~ ## .SZTC -0.335 0.210 -1.597 0.110 -0.335 -0.107 ## .SZDC -0.608 0.176 -3.461 0.001 -0.608 -0.265 ## .SZTC ~~ ## .SZDC -0.933 0.188 -4.967 0.000 -0.933 -0.330 ## .PARO ~~ ## .SZTO 0.737 0.118 6.240 0.000 0.737 0.413 ## .SZDO 0.505 0.096 5.274 0.000 0.505 0.368 ## .SZTO ~~ ## .SZDO 0.625 0.102 6.158 0.000 0.625 0.478 ## paranoid ~~ ## schizotypal 0.381 0.046 8.341 0.000 0.381 0.381 ## schizoid 0.359 0.046 7.856 0.000 0.359 0.359 ## schizotypal ~~ ## schizoid 0.310 0.047 6.666 0.000 0.310 0.310 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .PARI 6.514 0.513 12.695 0.000 6.514 0.493 ## .PARC 2.529 0.334 7.562 0.000 2.529 0.293 ## .PARO 1.867 0.179 10.434 0.000 1.867 0.380 ## .SZTI 5.309 0.460 11.529 0.000 5.309 0.379 ## .SZTC 3.846 0.330 11.654 0.000 3.846 0.411 ## .SZTO 1.704 0.175 9.742 0.000 1.704 0.289 ## .SZDI 5.080 0.386 13.158 0.000 5.080 0.408 ## .SZDC 2.085 0.230 9.047 0.000 2.085 0.260 ## .SZDO 1.005 0.107 9.351 0.000 1.005 0.240 ## paranoid 1.000 1.000 1.000 ## schizotypal 1.000 1.000 1.000 ## schizoid 1.000 1.000 1.000 semPaths(fit3, what=&#39;std&#39;, nCharNodes = 0, nCharEdges = 0, # don&#39;t limit variable name lengths curvePivot = TRUE, curve = 1.1, # pull covariances&#39; curves out a little fade=FALSE) "],["lavaan-lab-19-multilevel-sem.html", "Chapter 22 Lavaan Lab 19: Multilevel SEM 22.1 PART I: Multilevel CFA 1: within-only construct 22.2 PART II: Multilevel CFA 2: Between-only construct 22.3 PART III: Multilevel CFA 3: Shared cross-level construct 22.4 PART IV: Multilevel CFA 4: Configural construct 22.5 PART V: Multilevel CFA 5: Shared + Configural construct 22.6 PART VI: Model Comparison 22.7 PART VII: Adding Covariates to Multilevel SEM 22.8 PART VII: Final Model", " Chapter 22 Lavaan Lab 19: Multilevel SEM In this lab, we will: build a multilevel CFA model add covariates at both the between and the within level Load up the lavaan library: library(lavaan) Let’s read in a Mplus example dataset from an online location Data &lt;- read.table(&quot;http://statmodel.com/usersguide/chap9/ex9.6.dat&quot;) names(Data) &lt;- c(&quot;y1&quot;, &quot;y2&quot;, &quot;y3&quot;, &quot;y4&quot;, &quot;x1&quot;, &quot;x2&quot;, &quot;w&quot;, &quot;clus&quot;) Take a look at the matrix: head(Data) ## y1 y2 y3 y4 x1 x2 w clus ## 1 2.203250 1.858861 1.738477 2.244863 1.142800 -0.796987 -0.149501 1 ## 2 1.934917 2.127876 0.083120 2.509436 1.949033 -0.122764 -0.149501 1 ## 3 0.321955 0.977231 -0.835405 0.558367 -0.716481 -0.767064 -0.149501 1 ## 4 0.073154 -1.743092 -2.310271 -1.514332 -2.649131 0.637570 -0.149501 1 ## 5 -1.214906 0.452618 0.372610 -1.790372 -0.262916 0.302564 -0.149501 1 ## 6 0.298330 -1.820272 0.561335 -2.090582 -0.944963 1.363045 0.319335 2 dim(Data) ## [1] 1000 8 length(unique(Data$clus)) ## [1] 110 there are 1000 individual observations in 110 clusters cluster sizes: 5, 10, 15 4 measures at the within level y1, y2, y3, y4 2 covariates at the within level: x1, x2 1 covariate at the between level: w 22.1 PART I: Multilevel CFA 1: within-only construct This model specifies the latent variable only at the within level: model1 &lt;- &#39; level: 1 fw =~ y1 + y2 + y3 + y4 level: 2 y1 ~~ y1 + y2 + y3 + y4 y2 ~~ y2 + y2 + y3 y3 ~~ y3 + y4 y4 ~~ y4 # all variances and covariances are freely estimated &#39; Fit the model: model1fit &lt;- lavaan::sem(model1, data = Data, cluster = &quot;clus&quot;, estimator = &#39;MLR&#39;, fixed.x = FALSE) lavaan::summary(model1fit, fit.measures = T, standardized = T) ## lavaan 0.6-12 ended normally after 57 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 21 ## ## Number of observations 1000 ## Number of clusters [clus] 110 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 52.360 64.738 ## Degrees of freedom 3 3 ## P-value (Chi-square) 0.000 0.000 ## Scaling correction factor 0.809 ## Yuan-Bentler correction (Mplus variant) ## ## Model Test Baseline Model: ## ## Test statistic 2619.397 2815.244 ## Degrees of freedom 12 12 ## P-value 0.000 0.000 ## Scaling correction factor 0.930 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.981 0.978 ## Tucker-Lewis Index (TLI) 0.924 0.912 ## ## Robust Comparative Fit Index (CFI) 0.981 ## Robust Tucker-Lewis Index (TLI) 0.923 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -6900.518 -6900.518 ## Scaling correction factor 1.050 ## for the MLR correction ## Loglikelihood unrestricted model (H1) -6874.338 -6874.338 ## Scaling correction factor 1.020 ## for the MLR correction ## ## Akaike (AIC) 13843.036 13843.036 ## Bayesian (BIC) 13946.099 13946.099 ## Sample-size adjusted Bayesian (BIC) 13879.402 13879.402 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.128 0.143 ## 90 Percent confidence interval - lower 0.099 0.111 ## 90 Percent confidence interval - upper 0.160 0.178 ## P-value RMSEA &lt;= 0.05 0.000 0.000 ## ## Robust RMSEA 0.129 ## 90 Percent confidence interval - lower 0.103 ## 90 Percent confidence interval - upper 0.157 ## ## Standardized Root Mean Square Residual (corr metric): ## ## SRMR (within covariance matrix) 0.022 0.022 ## SRMR (between covariance matrix) 0.334 0.334 ## ## Parameter Estimates: ## ## Standard errors Sandwich ## Information bread Observed ## Observed information based on Hessian ## ## ## Level 1 [within]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## fw =~ ## y1 1.000 1.568 0.843 ## y2 1.019 0.035 28.881 0.000 1.598 0.857 ## y3 0.995 0.041 24.267 0.000 1.560 0.832 ## y4 1.034 0.035 29.613 0.000 1.622 0.851 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 0.000 0.000 0.000 ## .y2 0.000 0.000 0.000 ## .y3 0.000 0.000 0.000 ## .y4 0.000 0.000 0.000 ## fw 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 0.997 0.070 14.220 0.000 0.997 0.289 ## .y2 0.919 0.061 14.990 0.000 0.919 0.265 ## .y3 1.082 0.057 18.977 0.000 1.082 0.308 ## .y4 0.997 0.069 14.422 0.000 0.997 0.275 ## fw 2.458 0.186 13.209 0.000 1.000 1.000 ## ## ## Level 2 [clus]: ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 ~~ ## .y2 0.069 0.027 2.558 0.011 0.069 0.935 ## .y3 0.080 0.027 2.916 0.004 0.080 1.141 ## .y4 0.052 0.024 2.157 0.031 0.052 0.764 ## .y2 ~~ ## .y3 0.055 0.023 2.399 0.016 0.055 0.935 ## .y3 ~~ ## .y4 0.040 0.026 1.537 0.124 0.040 0.727 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 -0.063 0.089 -0.716 0.474 -0.063 -0.215 ## .y2 -0.060 0.088 -0.674 0.500 -0.060 -0.239 ## .y3 -0.026 0.086 -0.301 0.763 -0.026 -0.109 ## .y4 -0.005 0.089 -0.053 0.958 -0.005 -0.020 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 0.088 0.033 2.679 0.007 0.088 1.000 ## .y2 0.062 0.037 1.687 0.092 0.062 1.000 ## .y3 0.056 0.035 1.608 0.108 0.056 1.000 ## .y4 0.054 0.038 1.423 0.155 0.054 1.000 22.2 PART II: Multilevel CFA 2: Between-only construct Example: construct reflects self-reported ‘school climate’ measured by a questionnaire filled in by the school principles We will only have one response for each school We may collect other variables from students/teachers in the schools though Note that the following model syntax: model2.wrong &lt;- &#39; level: 1 # perhaps other level-1 variables level: 2 fb =~ y1 + y2 + y3 + y4 &#39; model2fit &lt;- lavaan::sem(model2.wrong, data = Data, cluster = &quot;clus&quot;, estimator = &#39;MLR&#39;, fixed.x = FALSE) Error in lav_partable_vnames(tmp.lav, type = &quot;ov&quot;, level = tmp.level.values[l]) : lavaan ERROR: level column does not contain value `1&#39; won’t work because there is nothing at level 1. Instead, specify this model just like a regular CFA model: model2 &lt;- &#39; fb =~ y1 + y2 + y3 + y4 &#39; Fit the model: model2fit &lt;- lavaan::sem(model2, data = Data, cluster = &quot;clus&quot;, estimator = &#39;MLR&#39;, fixed.x = FALSE) lavaan::summary(model2fit, fit.measures = T, standardized = T) ## lavaan 0.6-12 ended normally after 20 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 12 ## ## Number of observations 1000 ## Number of clusters [clus] 110 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 0.108 0.107 ## Degrees of freedom 2 2 ## P-value (Chi-square) 0.947 0.948 ## Scaling correction factor 1.013 ## Yuan-Bentler correction (Mplus variant) ## ## Model Test Baseline Model: ## ## Test statistic 2729.902 5889.388 ## Degrees of freedom 6 6 ## P-value 0.000 0.000 ## Scaling correction factor 0.464 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 1.000 ## Tucker-Lewis Index (TLI) 1.002 1.001 ## ## Robust Comparative Fit Index (CFI) 1.000 ## Robust Tucker-Lewis Index (TLI) 1.002 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -6907.183 -6907.183 ## Scaling correction factor 1.205 ## for the MLR correction ## Loglikelihood unrestricted model (H1) -6907.129 -6907.129 ## Scaling correction factor 1.178 ## for the MLR correction ## ## Akaike (AIC) 13838.367 13838.367 ## Bayesian (BIC) 13897.260 13897.260 ## Sample-size adjusted Bayesian (BIC) 13859.147 13859.147 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 0.000 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.007 0.003 ## P-value RMSEA &lt;= 0.05 0.995 0.996 ## ## Robust RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.006 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.001 0.001 ## ## Parameter Estimates: ## ## Standard errors Robust.cluster ## Information Observed ## Observed information based on Hessian ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## fb =~ ## y1 1.000 1.638 0.856 ## y2 0.995 0.031 31.694 0.000 1.629 0.860 ## y3 0.984 0.037 26.944 0.000 1.612 0.842 ## y4 1.003 0.029 34.582 0.000 1.643 0.851 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 -0.064 0.090 -0.707 0.479 -0.064 -0.033 ## .y2 -0.058 0.089 -0.652 0.515 -0.058 -0.031 ## .y3 -0.026 0.087 -0.303 0.762 -0.026 -0.014 ## .y4 -0.011 0.091 -0.119 0.905 -0.011 -0.006 ## fb 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 0.983 0.065 15.151 0.000 0.983 0.268 ## .y2 0.937 0.060 15.657 0.000 0.937 0.261 ## .y3 1.065 0.058 18.477 0.000 1.065 0.291 ## .y4 1.030 0.064 16.008 0.000 1.030 0.276 ## fb 2.684 0.178 15.101 0.000 1.000 1.000 22.3 PART III: Multilevel CFA 3: Shared cross-level construct This model specifies the latent variable both at the within and the between level; However, the latent variable only makes sense at the between level so SEM model is only built at the between level; The indicators are correlated at the within level; model3 &lt;- &#39; level: 1 y1 ~~ y1 + y2 + y3 + y4 y2 ~~ y2 + y3 + y4 y3 ~~ y3 + y4 y4 ~~ y4 level: 2 fs =~ y1 + y2 + y3 + y4 # Fix Significant Heywood Cases y2 ~~ v2*y2 y4 ~~ v4*y4 v2 &gt; 0 v4 &gt; 0 &#39; Fit the model: model3fit &lt;- lavaan::sem(model3, data = Data, cluster = &quot;clus&quot;, estimator = &#39;MLR&#39;, fixed.x = FALSE) ## Warning in lav_object_post_check(object): lavaan WARNING: some estimated ov variances are negative lavaan::summary(model3fit, standardized = T, fit.measures = T) ## lavaan 0.6-11 ended normally after 94 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 21 ## Number of inequality constraints 2 ## ## Number of observations 1000 ## Number of clusters [clus] 110 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 630.445 342.869 ## Degrees of freedom 3 3 ## P-value (Chi-square) 0.000 0.000 ## Scaling correction factor 1.839 ## Yuan-Bentler correction (Mplus variant) ## ## Model Test Baseline Model: ## ## Test statistic 2619.397 2815.244 ## Degrees of freedom 12 12 ## P-value 0.000 0.000 ## Scaling correction factor 0.930 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.759 0.879 ## Tucker-Lewis Index (TLI) 0.037 0.515 ## ## Robust Comparative Fit Index (CFI) 0.760 ## Robust Tucker-Lewis Index (TLI) 0.042 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -7189.561 -7189.561 ## Scaling correction factor 0.903 ## for the MLR correction ## Loglikelihood unrestricted model (H1) -6874.338 -6874.338 ## Scaling correction factor 1.020 ## for the MLR correction ## ## Akaike (AIC) 14421.121 14421.121 ## Bayesian (BIC) 14524.184 14524.184 ## Sample-size adjusted Bayesian (BIC) 14457.487 14457.487 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.457 0.337 ## 90 Percent confidence interval - lower 0.428 0.315 ## 90 Percent confidence interval - upper 0.488 0.359 ## P-value RMSEA &lt;= 0.05 0.000 0.000 ## ## Robust RMSEA 0.456 ## 90 Percent confidence interval - lower 0.416 ## 90 Percent confidence interval - upper 0.498 ## ## Standardized Root Mean Square Residual (corr metric): ## ## SRMR (within covariance matrix) 0.266 0.266 ## SRMR (between covariance matrix) 0.010 0.010 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Observed ## Observed information based on Hessian ## ## ## Level 1 [within]: ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 ~~ ## .y2 1.289 0.106 12.112 0.000 1.289 0.480 ## .y3 1.417 0.076 18.696 0.000 1.417 0.580 ## .y4 1.209 0.106 11.409 0.000 1.209 0.440 ## .y2 ~~ ## .y3 1.245 0.107 11.614 0.000 1.245 0.458 ## .y3 ~~ ## .y4 1.228 0.108 11.332 0.000 1.228 0.442 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 0.000 0.000 0.000 ## .y2 0.000 0.000 0.000 ## .y3 0.000 0.000 0.000 ## .y4 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 2.417 0.095 25.385 0.000 2.417 1.000 ## .y2 2.983 0.137 21.836 0.000 2.983 1.000 ## .y3 2.474 0.098 25.227 0.000 2.474 1.000 ## .y4 3.121 0.143 21.864 0.000 3.121 1.000 ## ## ## Level 2 [clus]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## fs =~ ## y1 1.000 0.772 1.001 ## y2 1.044 0.079 13.219 0.000 0.807 1.000 ## y3 0.963 0.066 14.530 0.000 0.744 1.008 ## y4 1.044 0.083 12.581 0.000 0.807 1.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 -0.076 0.090 -0.853 0.394 -0.076 -0.099 ## .y2 -0.071 0.095 -0.747 0.455 -0.071 -0.088 ## .y3 -0.039 0.087 -0.445 0.656 -0.039 -0.053 ## .y4 -0.024 0.096 -0.248 0.804 -0.024 -0.030 ## fs 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y2 (v2) -0.000 NA -0.000 -0.000 ## .y4 (v4) -0.000 -0.000 -0.000 ## .y1 -0.002 0.020 -0.077 0.939 -0.002 -0.003 ## .y3 -0.009 0.019 -0.455 0.649 -0.009 -0.016 ## fs 0.597 0.120 4.960 0.000 1.000 1.000 ## ## Constraints: ## |Slack| ## v2 - 0 0.000 ## v4 - 0 0.000 22.4 PART IV: Multilevel CFA 4: Configural construct Model 4a specifies the latent variable both at the within and the between level; The CFA at each level should have the same factor structure, but not necessarily the same parameter estimates; model4a &lt;- &#39; level: 1 fw =~ y1 + y2 + y3 + y4 level: 2 fb =~ y1 + y2 + y3 + y4 &#39; model4afit &lt;- lavaan::sem(model4a, data = Data, cluster = &quot;clus&quot;, estimator = &#39;MLR&#39;, fixed.x = FALSE) ## Warning in lav_object_post_check(object): lavaan WARNING: some estimated ov variances are negative lavaan::summary(model4afit, fit.measures = T, standardized = T) ## lavaan 0.6-12 ended normally after 50 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 20 ## ## Number of observations 1000 ## Number of clusters [clus] 110 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 0.000 0.000 ## Degrees of freedom 4 4 ## P-value (Chi-square) 1.000 1.000 ## Scaling correction factor 0.977 ## Yuan-Bentler correction (Mplus variant) ## ## Model Test Baseline Model: ## ## Test statistic 2619.397 2815.244 ## Degrees of freedom 12 12 ## P-value 0.000 0.000 ## Scaling correction factor 0.930 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 1.000 ## Tucker-Lewis Index (TLI) 1.005 1.004 ## ## Robust Comparative Fit Index (CFI) 1.000 ## Robust Tucker-Lewis Index (TLI) 1.004 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -6874.157 -6874.157 ## Scaling correction factor 1.028 ## for the MLR correction ## Loglikelihood unrestricted model (H1) -6874.338 -6874.338 ## Scaling correction factor 1.020 ## for the MLR correction ## ## Akaike (AIC) 13788.314 13788.314 ## Bayesian (BIC) 13886.469 13886.469 ## Sample-size adjusted Bayesian (BIC) 13822.948 13822.948 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 0.000 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.000 0.000 ## P-value RMSEA &lt;= 0.05 1.000 1.000 ## ## Robust RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## ## Standardized Root Mean Square Residual (corr metric): ## ## SRMR (within covariance matrix) 0.001 0.001 ## SRMR (between covariance matrix) 0.003 0.003 ## ## Parameter Estimates: ## ## Standard errors Sandwich ## Information bread Observed ## Observed information based on Hessian ## ## ## Level 1 [within]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## fw =~ ## y1 1.000 1.486 0.831 ## y2 0.992 0.035 28.208 0.000 1.474 0.836 ## y3 0.992 0.041 24.110 0.000 1.474 0.818 ## y4 1.007 0.035 28.619 0.000 1.496 0.830 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 0.000 0.000 0.000 ## .y2 0.000 0.000 0.000 ## .y3 0.000 0.000 0.000 ## .y4 0.000 0.000 0.000 ## fw 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 0.989 0.071 13.980 0.000 0.989 0.309 ## .y2 0.934 0.062 15.187 0.000 0.934 0.301 ## .y3 1.076 0.057 18.812 0.000 1.076 0.331 ## .y4 1.014 0.070 14.478 0.000 1.014 0.312 ## fw 2.207 0.165 13.371 0.000 1.000 1.000 ## ## ## Level 2 [clus]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## fb =~ ## y1 1.000 0.700 1.006 ## y2 1.005 0.086 11.732 0.000 0.704 0.998 ## y3 0.947 0.094 10.117 0.000 0.663 1.013 ## y4 0.984 0.080 12.240 0.000 0.689 0.983 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 -0.072 0.089 -0.815 0.415 -0.072 -0.104 ## .y2 -0.067 0.088 -0.755 0.450 -0.067 -0.095 ## .y3 -0.035 0.086 -0.404 0.687 -0.035 -0.053 ## .y4 -0.018 0.089 -0.205 0.838 -0.018 -0.026 ## fb 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 -0.006 0.018 -0.339 0.734 -0.006 -0.012 ## .y2 0.002 0.020 0.091 0.928 0.002 0.004 ## .y3 -0.011 0.020 -0.541 0.589 -0.011 -0.025 ## .y4 0.016 0.021 0.786 0.432 0.016 0.033 ## fb 0.490 0.120 4.099 0.000 1.000 1.000 Model 4b specifies the same CFA at each level and requires the same factor loadings; model4b &lt;- &#39; level: 1 fw =~ a*y1 + b*y2 + c*y3 + d*y4 level: 2 fb =~ a*y1 + b*y2 + c*y3 + d*y4 &#39; model4bfit &lt;- lavaan::sem(model4b, data = Data, cluster = &quot;clus&quot;, estimator = &#39;MLR&#39;, fixed.x = FALSE) ## Warning in lav_object_post_check(object): lavaan WARNING: some estimated ov variances are negative lavaan::summary(model4bfit, fit.measures = T, standardized = T) ## lavaan 0.6-12 ended normally after 44 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 20 ## Number of equality constraints 3 ## ## Number of observations 1000 ## Number of clusters [clus] 110 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 0.057 0.059 ## Degrees of freedom 7 7 ## P-value (Chi-square) 1.000 1.000 ## Scaling correction factor 0.967 ## Yuan-Bentler correction (Mplus variant) ## ## Model Test Baseline Model: ## ## Test statistic 2619.397 2815.244 ## Degrees of freedom 12 12 ## P-value 0.000 0.000 ## Scaling correction factor 0.930 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 1.000 ## Tucker-Lewis Index (TLI) 1.005 1.004 ## ## Robust Comparative Fit Index (CFI) 1.000 ## Robust Tucker-Lewis Index (TLI) 1.004 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -6874.367 -6874.367 ## Scaling correction factor 0.885 ## for the MLR correction ## Loglikelihood unrestricted model (H1) -6874.338 -6874.338 ## Scaling correction factor 1.020 ## for the MLR correction ## ## Akaike (AIC) 13782.733 13782.733 ## Bayesian (BIC) 13866.165 13866.165 ## Sample-size adjusted Bayesian (BIC) 13812.172 13812.172 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 0.000 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.000 0.000 ## P-value RMSEA &lt;= 0.05 1.000 1.000 ## ## Robust RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## ## Standardized Root Mean Square Residual (corr metric): ## ## SRMR (within covariance matrix) 0.002 0.002 ## SRMR (between covariance matrix) 0.003 0.003 ## ## Parameter Estimates: ## ## Standard errors Sandwich ## Information bread Observed ## Observed information based on Hessian ## ## ## Level 1 [within]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## fw =~ ## y1 (a) 1.000 1.489 0.832 ## y2 (b) 0.995 0.031 31.649 0.000 1.482 0.838 ## y3 (c) 0.983 0.036 27.090 0.000 1.463 0.815 ## y4 (d) 1.003 0.029 34.668 0.000 1.494 0.829 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 0.000 0.000 0.000 ## .y2 0.000 0.000 0.000 ## .y3 0.000 0.000 0.000 ## .y4 0.000 0.000 0.000 ## fw 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 0.987 0.069 14.228 0.000 0.987 0.308 ## .y2 0.930 0.061 15.173 0.000 0.930 0.298 ## .y3 1.081 0.057 18.905 0.000 1.081 0.335 ## .y4 1.015 0.068 14.877 0.000 1.015 0.313 ## fw 2.218 0.158 14.021 0.000 1.000 1.000 ## ## ## Level 2 [clus]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## fb =~ ## y1 (a) 1.000 0.693 1.005 ## y2 (b) 0.995 0.031 31.649 0.000 0.690 0.997 ## y3 (c) 0.983 0.036 27.090 0.000 0.681 1.013 ## y4 (d) 1.003 0.029 34.668 0.000 0.695 0.984 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 -0.071 0.089 -0.804 0.421 -0.071 -0.103 ## .y2 -0.066 0.088 -0.746 0.456 -0.066 -0.095 ## .y3 -0.034 0.086 -0.396 0.692 -0.034 -0.051 ## .y4 -0.017 0.089 -0.196 0.845 -0.017 -0.025 ## fb 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 -0.005 0.016 -0.302 0.763 -0.005 -0.010 ## .y2 0.003 0.019 0.170 0.865 0.003 0.007 ## .y3 -0.012 0.020 -0.601 0.548 -0.012 -0.027 ## .y4 0.016 0.020 0.767 0.443 0.016 0.031 ## fb 0.480 0.119 4.027 0.000 1.000 1.000 22.5 PART V: Multilevel CFA 5: Shared + Configural construct This model specifies the latent variable both at the within and the between level; The CFA at each level should have the same factor structure, but not necessarily the same parameter estimates; model5 &lt;- &#39; level: 1 fw =~ a*y1 + b*y2 + c*y3 + d*y4 level: 2 fb =~ a*y1 + b*y2 + c*y3 + d*y4 # configural fs =~ y1 + y2 + y3 + y4 # shared # fb and fs must be orthogonal fs ~~ 0*fb &#39; model5fit &lt;- lavaan::sem(model5, data = Data, cluster = &quot;clus&quot;, estimator = &#39;MLR&#39;, fixed.x = FALSE) ## Warning in lav_object_post_check(object): lavaan WARNING: some estimated ov variances are negative lavaan::summary(model5fit, fit.measures = T, standardized = T) ## lavaan 0.6-12 ended normally after 56 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 24 ## Number of equality constraints 3 ## ## Number of observations 1000 ## Number of clusters [clus] 110 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 0.000 0.000 ## Degrees of freedom 3 3 ## P-value (Chi-square) 1.000 1.000 ## Scaling correction factor 0.920 ## Yuan-Bentler correction (Mplus variant) ## ## Model Test Baseline Model: ## ## Test statistic 2619.397 2815.244 ## Degrees of freedom 12 12 ## P-value 0.000 0.000 ## Scaling correction factor 0.930 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 1.000 ## Tucker-Lewis Index (TLI) 1.005 1.004 ## ## Robust Comparative Fit Index (CFI) 1.000 ## Robust Tucker-Lewis Index (TLI) 1.004 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -6874.156 -6874.156 ## Scaling correction factor 0.905 ## for the MLR correction ## Loglikelihood unrestricted model (H1) -6874.338 -6874.338 ## Scaling correction factor 1.020 ## for the MLR correction ## ## Akaike (AIC) 13790.312 13790.312 ## Bayesian (BIC) 13893.375 13893.375 ## Sample-size adjusted Bayesian (BIC) 13826.678 13826.678 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 0.000 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.000 0.000 ## P-value RMSEA &lt;= 0.05 1.000 1.000 ## ## Robust RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## ## Standardized Root Mean Square Residual (corr metric): ## ## SRMR (within covariance matrix) 0.001 0.001 ## SRMR (between covariance matrix) 0.003 0.003 ## ## Parameter Estimates: ## ## Standard errors Sandwich ## Information bread Observed ## Observed information based on Hessian ## ## ## Level 1 [within]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## fw =~ ## y1 (a) 1.000 1.486 0.831 ## y2 (b) 0.993 0.036 27.488 0.000 1.474 0.836 ## y3 (c) 0.992 0.042 23.900 0.000 1.473 0.818 ## y4 (d) 1.007 0.036 28.299 0.000 1.496 0.830 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 0.000 0.000 0.000 ## .y2 0.000 0.000 0.000 ## .y3 0.000 0.000 0.000 ## .y4 0.000 0.000 0.000 ## fw 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 0.989 0.071 13.979 0.000 0.989 0.309 ## .y2 0.934 0.061 15.212 0.000 0.934 0.301 ## .y3 1.076 0.057 18.809 0.000 1.076 0.331 ## .y4 1.014 0.070 14.414 0.000 1.014 0.312 ## fw 2.207 0.167 13.244 0.000 1.000 1.000 ## ## ## Level 2 [clus]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## fb =~ ## y1 (a) 1.000 0.492 0.706 ## y2 (b) 0.993 0.036 27.488 0.000 0.488 0.692 ## y3 (c) 0.992 0.042 23.900 0.000 0.488 0.745 ## y4 (d) 1.007 0.036 28.299 0.000 0.495 0.707 ## fs =~ ## y1 1.000 0.499 0.717 ## y2 1.018 0.288 3.533 0.000 0.508 0.720 ## y3 0.903 1.179 0.766 0.444 0.450 0.688 ## y4 0.960 0.690 1.391 0.164 0.479 0.684 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## fb ~~ ## fs 0.000 0.000 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 -0.073 0.089 -0.819 0.413 -0.073 -0.104 ## .y2 -0.067 0.088 -0.758 0.449 -0.067 -0.095 ## .y3 -0.035 0.086 -0.406 0.685 -0.035 -0.053 ## .y4 -0.018 0.089 -0.207 0.836 -0.018 -0.026 ## fb 0.000 0.000 0.000 ## fs 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 -0.006 0.018 -0.330 0.742 -0.006 -0.012 ## .y2 0.001 0.021 0.063 0.950 0.001 0.003 ## .y3 -0.012 0.027 -0.427 0.670 -0.012 -0.027 ## .y4 0.016 0.021 0.779 0.436 0.016 0.033 ## fb 0.242 3.053 0.079 0.937 1.000 1.000 ## fs 0.249 3.022 0.082 0.934 1.000 1.000 22.6 PART VI: Model Comparison m1 = fitMeasures(model1fit, fit.measures = c(&quot;chisq.scaled&quot;, &quot;df.scaled&quot;, &quot;pvalue.scaled&quot;, &quot;rmsea.robust&quot;, &quot;rmsea.ci.lower.robust&quot;, &quot;rmsea.ci.upper.robust&quot;, &quot;cfi.robust&quot;, &quot;tli.robust&quot;, &quot;srmr_within&quot;, &quot;srmr_between&quot;)) m2 = fitMeasures(model2fit, fit.measures = c(&quot;chisq.scaled&quot;, &quot;df.scaled&quot;, &quot;pvalue.scaled&quot;, &quot;rmsea.robust&quot;, &quot;rmsea.ci.lower.robust&quot;, &quot;rmsea.ci.upper.robust&quot;, &quot;cfi.robust&quot;, &quot;tli.robust&quot;, &quot;srmr_within&quot;, &quot;srmr_between&quot;)) m3 = fitMeasures(model3fit, fit.measures = c(&quot;chisq.scaled&quot;, &quot;df.scaled&quot;, &quot;pvalue.scaled&quot;, &quot;rmsea.robust&quot;, &quot;rmsea.ci.lower.robust&quot;, &quot;rmsea.ci.upper.robust&quot;, &quot;cfi.robust&quot;, &quot;tli.robust&quot;, &quot;srmr_within&quot;, &quot;srmr_between&quot;)) m4a = fitMeasures(model4afit, fit.measures = c(&quot;chisq.scaled&quot;, &quot;df.scaled&quot;, &quot;pvalue.scaled&quot;, &quot;rmsea.robust&quot;, &quot;rmsea.ci.lower.robust&quot;, &quot;rmsea.ci.upper.robust&quot;, &quot;cfi.robust&quot;, &quot;tli.robust&quot;, &quot;srmr_within&quot;, &quot;srmr_between&quot;)) m4b = fitMeasures(model4bfit, fit.measures = c(&quot;chisq.scaled&quot;, &quot;df.scaled&quot;, &quot;pvalue.scaled&quot;, &quot;rmsea.robust&quot;, &quot;rmsea.ci.lower.robust&quot;, &quot;rmsea.ci.upper.robust&quot;, &quot;cfi.robust&quot;, &quot;tli.robust&quot;, &quot;srmr_within&quot;, &quot;srmr_between&quot;)) m5 = fitMeasures(model5fit, fit.measures = c(&quot;chisq.scaled&quot;, &quot;df.scaled&quot;, &quot;pvalue.scaled&quot;, &quot;rmsea.robust&quot;, &quot;rmsea.ci.lower.robust&quot;, &quot;rmsea.ci.upper.robust&quot;, &quot;cfi.robust&quot;, &quot;tli.robust&quot;, &quot;srmr_within&quot;, &quot;srmr_between&quot;)) #install.packages(&#39;qpcR&#39;) library(qpcR) round(qpcR:::cbind.na(m1, m2, m3, m4a, m4b, m5), 3) ## m1 m2 m3 m4a m4b m5 ## chisq.scaled 64.738 0.107 342.869 0.000 0.059 0.000 ## df.scaled 3.000 2.000 3.000 4.000 7.000 3.000 ## pvalue.scaled 0.000 0.948 0.000 1.000 1.000 1.000 ## rmsea.robust 0.129 0.000 0.456 0.000 0.000 0.000 ## rmsea.ci.lower.robust 0.103 0.000 0.416 0.000 0.000 0.000 ## rmsea.ci.upper.robust 0.157 0.006 0.498 0.000 0.000 0.000 ## cfi.robust 0.981 1.000 0.760 1.000 1.000 1.000 ## tli.robust 0.923 1.002 0.042 1.004 1.004 1.004 ## srmr_within 0.022 NA 0.266 0.001 0.002 0.001 ## srmr_between 0.334 NA 0.010 0.003 0.003 0.003 The final model goes to (drumroll)…model4b! 22.7 PART VII: Adding Covariates to Multilevel SEM 22.7.1 Model A: Adding a within-only covariate model4wCovA &lt;- &#39; level: 1 fw =~ a*y1 + b*y2 + c*y3 + d*y4 fw ~ x1 level: 2 fb =~ a*y1 + b*y2 + c*y3 + d*y4 &#39; model4wCovAfit &lt;- lavaan::sem(model4wCovA, data = Data, cluster = &quot;clus&quot;, estimator = &#39;MLR&#39;, fixed.x = FALSE) ## Warning in lav_object_post_check(object): lavaan WARNING: some estimated ov variances are negative lavaan::summary(model4wCovAfit, fit.measures = T, standardized = T) ## lavaan 0.6-12 ended normally after 54 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 23 ## Number of equality constraints 3 ## ## Number of observations 1000 ## Number of clusters [clus] 110 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 1.793 1.874 ## Degrees of freedom 10 10 ## P-value (Chi-square) 0.998 0.997 ## Scaling correction factor 0.957 ## Yuan-Bentler correction (Mplus variant) ## ## Model Test Baseline Model: ## ## Test statistic 3085.335 3318.855 ## Degrees of freedom 16 16 ## P-value 0.000 0.000 ## Scaling correction factor 0.930 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 1.000 ## Tucker-Lewis Index (TLI) 1.004 1.004 ## ## Robust Comparative Fit Index (CFI) 1.000 ## Robust Tucker-Lewis Index (TLI) 1.004 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -8053.772 -8053.772 ## Scaling correction factor 0.901 ## for the MLR correction ## Loglikelihood unrestricted model (H1) -8052.875 -8052.875 ## Scaling correction factor 1.009 ## for the MLR correction ## ## Akaike (AIC) 16147.544 16147.544 ## Bayesian (BIC) 16245.699 16245.699 ## Sample-size adjusted Bayesian (BIC) 16182.178 16182.178 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 0.000 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.000 0.000 ## P-value RMSEA &lt;= 0.05 1.000 1.000 ## ## Robust RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## ## Standardized Root Mean Square Residual (corr metric): ## ## SRMR (within covariance matrix) 0.004 0.004 ## SRMR (between covariance matrix) 0.004 0.004 ## ## Parameter Estimates: ## ## Standard errors Sandwich ## Information bread Observed ## Observed information based on Hessian ## ## ## Level 1 [within]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## fw =~ ## y1 (a) 1.000 1.501 0.834 ## y2 (b) 0.994 0.031 32.270 0.000 1.491 0.838 ## y3 (c) 0.984 0.036 27.550 0.000 1.476 0.818 ## y4 (d) 1.006 0.029 34.896 0.000 1.510 0.834 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## fw ~ ## x1 1.002 0.050 20.044 0.000 0.667 0.663 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 0.000 0.000 0.000 ## .y2 0.000 0.000 0.000 ## .y3 0.000 0.000 0.000 ## .y4 0.000 0.000 0.000 ## x1 0.007 0.036 0.195 0.846 0.007 0.007 ## .fw 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 0.989 0.065 15.126 0.000 0.989 0.305 ## .y2 0.942 0.055 17.114 0.000 0.942 0.297 ## .y3 1.080 0.058 18.491 0.000 1.080 0.331 ## .y4 1.001 0.065 15.438 0.000 1.001 0.305 ## .fw 1.264 0.096 13.119 0.000 0.561 0.561 ## x1 0.985 0.042 23.406 0.000 0.985 1.000 ## ## ## Level 2 [clus]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## fb =~ ## y1 (a) 1.000 0.636 1.004 ## y2 (b) 0.994 0.031 32.270 0.000 0.632 0.998 ## y3 (c) 0.984 0.036 27.550 0.000 0.626 1.016 ## y4 (d) 1.006 0.029 34.896 0.000 0.640 0.982 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 -0.070 0.080 -0.873 0.383 -0.070 -0.110 ## .y2 -0.064 0.078 -0.823 0.410 -0.064 -0.101 ## .y3 -0.033 0.076 -0.430 0.667 -0.033 -0.053 ## .y4 -0.016 0.078 -0.203 0.839 -0.016 -0.024 ## fb 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 -0.003 0.017 -0.177 0.860 -0.003 -0.007 ## .y2 0.002 0.019 0.094 0.925 0.002 0.005 ## .y3 -0.012 0.020 -0.624 0.533 -0.012 -0.033 ## .y4 0.015 0.020 0.766 0.444 0.015 0.036 ## fb 0.405 0.085 4.751 0.000 1.000 1.000 22.7.2 Model B: Adding a between-only covariate model4wCovB &lt;- &#39; level: 1 fw =~ a*y1 + b*y2 + c*y3 + d*y4 level: 2 fb =~ a*y1 + b*y2 + c*y3 + d*y4 fb ~ w &#39; model4wCovBfit &lt;- lavaan::sem(model4wCovB, data = Data, cluster = &quot;clus&quot;, estimator = &#39;MLR&#39;, fixed.x = FALSE) ## Warning in lav_object_post_check(object): lavaan WARNING: some estimated ov variances are negative lavaan::summary(model4wCovBfit, fit.measures = T, standardized = T) ## lavaan 0.6-12 ended normally after 35 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 23 ## Number of equality constraints 3 ## ## Number of observations 1000 ## Number of clusters [clus] 110 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 2.509 2.568 ## Degrees of freedom 10 10 ## P-value (Chi-square) 0.991 0.990 ## Scaling correction factor 0.977 ## Yuan-Bentler correction (Mplus variant) ## ## Model Test Baseline Model: ## ## Test statistic 2638.435 2788.623 ## Degrees of freedom 16 16 ## P-value 0.000 0.000 ## Scaling correction factor 0.946 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 1.000 ## Tucker-Lewis Index (TLI) 1.005 1.004 ## ## Robust Comparative Fit Index (CFI) 1.000 ## Robust Tucker-Lewis Index (TLI) 1.004 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -7010.919 -7010.919 ## Scaling correction factor 0.897 ## for the MLR correction ## Loglikelihood unrestricted model (H1) -7009.664 -7009.664 ## Scaling correction factor 1.013 ## for the MLR correction ## ## Akaike (AIC) 14061.838 14061.838 ## Bayesian (BIC) 14159.993 14159.993 ## Sample-size adjusted Bayesian (BIC) 14096.472 14096.472 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 0.000 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.000 0.000 ## P-value RMSEA &lt;= 0.05 1.000 1.000 ## ## Robust RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## ## Standardized Root Mean Square Residual (corr metric): ## ## SRMR (within covariance matrix) 0.002 0.002 ## SRMR (between covariance matrix) 0.017 0.017 ## ## Parameter Estimates: ## ## Standard errors Sandwich ## Information bread Observed ## Observed information based on Hessian ## ## ## Level 1 [within]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## fw =~ ## y1 (a) 1.000 1.491 0.832 ## y2 (b) 0.994 0.031 31.642 0.000 1.482 0.838 ## y3 (c) 0.982 0.036 27.079 0.000 1.464 0.815 ## y4 (d) 1.002 0.029 34.699 0.000 1.494 0.829 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 0.000 0.000 0.000 ## .y2 0.000 0.000 0.000 ## .y3 0.000 0.000 0.000 ## .y4 0.000 0.000 0.000 ## fw 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 0.987 0.069 14.241 0.000 0.987 0.308 ## .y2 0.929 0.061 15.165 0.000 0.929 0.297 ## .y3 1.081 0.057 18.940 0.000 1.081 0.335 ## .y4 1.015 0.068 14.894 0.000 1.015 0.313 ## fw 2.223 0.159 14.004 0.000 1.000 1.000 ## ## ## Level 2 [clus]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## fb =~ ## y1 (a) 1.000 0.687 1.007 ## y2 (b) 0.994 0.031 31.642 0.000 0.683 0.994 ## y3 (c) 0.982 0.036 27.079 0.000 0.675 1.015 ## y4 (d) 1.002 0.029 34.699 0.000 0.688 0.983 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## fb ~ ## w 0.363 0.103 3.533 0.000 0.529 0.477 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 -0.073 0.081 -0.900 0.368 -0.073 -0.107 ## .y2 -0.068 0.084 -0.813 0.416 -0.068 -0.099 ## .y3 -0.036 0.079 -0.451 0.652 -0.036 -0.054 ## .y4 -0.019 0.083 -0.229 0.819 -0.019 -0.027 ## w 0.006 0.086 0.070 0.944 0.006 0.007 ## .fb 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 -0.007 0.016 -0.415 0.678 -0.007 -0.015 ## .y2 0.006 0.019 0.290 0.772 0.006 0.012 ## .y3 -0.013 0.020 -0.639 0.523 -0.013 -0.029 ## .y4 0.016 0.021 0.778 0.437 0.016 0.033 ## .fb 0.364 0.078 4.645 0.000 0.772 0.772 ## w 0.815 0.107 7.638 0.000 0.815 1.000 22.7.3 Model C: Adding a covariate at both levels model4wCovC &lt;- &#39; level: 1 fw =~ a*y1 + b*y2 + c*y3 + d*y4 fw ~ x1 level: 2 fb =~ a*y1 + b*y2 + c*y3 + d*y4 fb ~ x1 &#39; model4wCovCfit &lt;- lavaan::sem(model4wCovC, data = Data, cluster = &quot;clus&quot;, estimator = &#39;MLR&#39;, fixed.x = FALSE) ## Warning in lav_object_post_check(object): lavaan WARNING: some estimated ov variances are negative lavaan::summary(model4wCovCfit, fit.measures = T, standardized = T) ## lavaan 0.6-12 ended normally after 83 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 25 ## Number of equality constraints 3 ## ## Number of observations 1000 ## Number of clusters [clus] 110 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 2.676 2.757 ## Degrees of freedom 13 13 ## P-value (Chi-square) 0.999 0.999 ## Scaling correction factor 0.971 ## Yuan-Bentler correction (Mplus variant) ## ## Model Test Baseline Model: ## ## Test statistic 3086.903 3246.984 ## Degrees of freedom 20 20 ## P-value 0.000 0.000 ## Scaling correction factor 0.951 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 1.000 ## Tucker-Lewis Index (TLI) 1.005 1.005 ## ## Robust Comparative Fit Index (CFI) 1.000 ## Robust Tucker-Lewis Index (TLI) 1.005 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -8052.605 -8052.605 ## Scaling correction factor 0.898 ## for the MLR correction ## Loglikelihood unrestricted model (H1) -8051.267 -8051.267 ## Scaling correction factor 1.002 ## for the MLR correction ## ## Akaike (AIC) 16149.210 16149.210 ## Bayesian (BIC) 16257.180 16257.180 ## Sample-size adjusted Bayesian (BIC) 16187.307 16187.307 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 0.000 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.000 0.000 ## P-value RMSEA &lt;= 0.05 1.000 1.000 ## ## Robust RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## ## Standardized Root Mean Square Residual (corr metric): ## ## SRMR (within covariance matrix) 0.004 0.004 ## SRMR (between covariance matrix) 0.023 0.023 ## ## Parameter Estimates: ## ## Standard errors Sandwich ## Information bread Observed ## Observed information based on Hessian ## ## ## Level 1 [within]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## fw =~ ## y1 (a) 1.000 1.489 0.832 ## y2 (b) 0.994 0.031 32.214 0.000 1.480 0.836 ## y3 (c) 0.984 0.036 27.538 0.000 1.465 0.816 ## y4 (d) 1.006 0.029 34.847 0.000 1.499 0.832 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## fw ~ ## x1 0.995 0.052 18.985 0.000 0.668 0.656 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 0.000 0.000 0.000 ## .y2 0.000 0.000 0.000 ## .y3 0.000 0.000 0.000 ## .y4 0.000 0.000 0.000 ## x1 0.000 0.000 0.000 ## .fw 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 0.989 0.065 15.129 0.000 0.989 0.308 ## .y2 0.942 0.055 17.105 0.000 0.942 0.301 ## .y3 1.080 0.058 18.488 0.000 1.080 0.335 ## .y4 1.001 0.065 15.435 0.000 1.001 0.308 ## .fw 1.263 0.097 13.085 0.000 0.570 0.570 ## x1 0.965 0.044 21.724 0.000 0.965 1.000 ## ## ## Level 2 [clus]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## fb =~ ## y1 (a) 1.000 0.686 1.003 ## y2 (b) 0.994 0.031 32.214 0.000 0.682 0.998 ## y3 (c) 0.984 0.036 27.538 0.000 0.675 1.014 ## y4 (d) 1.006 0.029 34.847 0.000 0.690 0.984 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## fb ~ ## x1 2.069 1.515 1.366 0.172 3.017 0.432 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 -0.076 0.087 -0.865 0.387 -0.076 -0.110 ## .y2 -0.070 0.084 -0.839 0.402 -0.070 -0.103 ## .y3 -0.038 0.082 -0.470 0.638 -0.038 -0.058 ## .y4 -0.022 0.084 -0.259 0.795 -0.022 -0.031 ## x1 0.005 0.034 0.141 0.888 0.005 0.034 ## .fb 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 -0.003 0.017 -0.158 0.875 -0.003 -0.006 ## .y2 0.002 0.019 0.089 0.929 0.002 0.004 ## .y3 -0.012 0.020 -0.624 0.533 -0.012 -0.028 ## .y4 0.015 0.020 0.756 0.450 0.015 0.031 ## .fb 0.383 0.101 3.800 0.000 0.813 0.813 ## x1 0.021 0.016 1.267 0.205 0.021 1.000 22.8 PART VII: Final Model modelFinal &lt;- &#39; level: 1 fw =~ a*y1 + b*y2 + c*y3 + d*y4 fw ~ x1 + x2 level: 2 fb =~ a*y1 + b*y2 + c*y3 + d*y4 fb ~ w &#39; modelFinalfit &lt;- lavaan::sem(modelFinal, data = Data, cluster = &quot;clus&quot;, estimator = &#39;MLR&#39;, fixed.x = FALSE) ## Warning in lav_object_post_check(object): lavaan WARNING: some estimated ov variances are negative lavaan::summary(modelFinalfit, fit.measures = T, standardized = T) ## lavaan 0.6-12 ended normally after 48 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 30 ## Number of equality constraints 3 ## ## Number of observations 1000 ## Number of clusters [clus] 110 ## ## Model Test User Model: ## Standard Robust ## Test Statistic 3.741 3.913 ## Degrees of freedom 16 16 ## P-value (Chi-square) 0.999 0.999 ## Scaling correction factor 0.956 ## Yuan-Bentler correction (Mplus variant) ## ## Model Test Baseline Model: ## ## Test statistic 3283.563 3481.228 ## Degrees of freedom 24 24 ## P-value 0.000 0.000 ## Scaling correction factor 0.943 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 1.000 ## Tucker-Lewis Index (TLI) 1.006 1.005 ## ## Robust Comparative Fit Index (CFI) 1.000 ## Robust Tucker-Lewis Index (TLI) 1.005 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -9527.367 -9527.367 ## Scaling correction factor 0.941 ## for the MLR correction ## Loglikelihood unrestricted model (H1) -9525.497 -9525.497 ## Scaling correction factor 1.012 ## for the MLR correction ## ## Akaike (AIC) 19108.735 19108.735 ## Bayesian (BIC) 19241.244 19241.244 ## Sample-size adjusted Bayesian (BIC) 19155.491 19155.491 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 0.000 ## 90 Percent confidence interval - lower 0.000 0.000 ## 90 Percent confidence interval - upper 0.000 0.000 ## P-value RMSEA &lt;= 0.05 1.000 1.000 ## ## Robust RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## ## Standardized Root Mean Square Residual (corr metric): ## ## SRMR (within covariance matrix) 0.004 0.004 ## SRMR (between covariance matrix) 0.014 0.014 ## ## Parameter Estimates: ## ## Standard errors Sandwich ## Information bread Observed ## Observed information based on Hessian ## ## ## Level 1 [within]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## fw =~ ## y1 (a) 1.000 1.498 0.834 ## y2 (b) 0.992 0.031 31.996 0.000 1.485 0.837 ## y3 (c) 0.982 0.035 27.682 0.000 1.470 0.816 ## y4 (d) 1.005 0.029 34.760 0.000 1.505 0.833 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## fw ~ ## x1 0.980 0.047 20.863 0.000 0.654 0.649 ## x2 0.515 0.039 13.113 0.000 0.344 0.347 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## x1 ~~ ## x2 0.032 0.033 0.972 0.331 0.032 0.032 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 0.000 0.000 0.000 ## .y2 0.000 0.000 0.000 ## .y3 0.000 0.000 0.000 ## .y4 0.000 0.000 0.000 ## x1 0.007 0.036 0.194 0.846 0.007 0.007 ## x2 0.014 0.038 0.372 0.710 0.014 0.014 ## .fw 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 0.983 0.063 15.631 0.000 0.983 0.305 ## .y2 0.944 0.056 16.896 0.000 0.944 0.300 ## .y3 1.084 0.057 18.963 0.000 1.084 0.334 ## .y4 1.001 0.064 15.764 0.000 1.001 0.307 ## .fw 0.995 0.081 12.358 0.000 0.444 0.444 ## x1 0.985 0.042 23.406 0.000 0.985 1.000 ## x2 1.017 0.049 20.603 0.000 1.017 1.000 ## ## ## Level 2 [clus]: ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## fb =~ ## y1 (a) 1.000 0.653 1.005 ## y2 (b) 0.992 0.031 31.996 0.000 0.647 0.996 ## y3 (c) 0.982 0.035 27.682 0.000 0.641 1.017 ## y4 (d) 1.005 0.029 34.760 0.000 0.656 0.982 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## fb ~ ## w 0.333 0.076 4.389 0.000 0.510 0.461 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 -0.083 0.072 -1.141 0.254 -0.083 -0.127 ## .y2 -0.077 0.073 -1.064 0.287 -0.077 -0.119 ## .y3 -0.045 0.069 -0.654 0.513 -0.045 -0.072 ## .y4 -0.029 0.072 -0.400 0.689 -0.029 -0.043 ## w 0.006 0.086 0.070 0.944 0.006 0.007 ## .fb 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .y1 -0.004 0.016 -0.246 0.806 -0.004 -0.010 ## .y2 0.003 0.019 0.177 0.859 0.003 0.008 ## .y3 -0.013 0.020 -0.660 0.509 -0.013 -0.033 ## .y4 0.016 0.020 0.778 0.436 0.016 0.036 ## .fb 0.336 0.061 5.520 0.000 0.788 0.788 ## w 0.815 0.107 7.638 0.000 0.815 1.000 "]]
